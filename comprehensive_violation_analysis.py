"""
ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ëª¨ë“ˆ
ì¡°ë¡€ì•ˆê³¼ PKL ë°ì´í„°ë² ì´ìŠ¤ì˜ ìœ„ë²• íŒë¡€ë¥¼ ë§¤ì¹­í•˜ì—¬ ì¢…í•©ì ì¸ ìœ„ë²•ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import os
from typing import List, Dict, Any, Tuple
import streamlit as st

def load_vectorstore_safe(pkl_path: str) -> Dict[str, Any]:
    """ì•ˆì „í•œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
    try:
        if not os.path.exists(pkl_path):
            return None
        
        with open(pkl_path, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨ ({pkl_path}): {str(e)}")
        return None

def extract_ordinance_articles(pdf_text: str) -> List[Dict[str, str]]:
    """ì¡°ë¡€ì•ˆì—ì„œ ì¡°ë¬¸ ì¶”ì¶œ"""
    articles = []
    
    # ì¡°ë¬¸ íŒ¨í„´: "ì œNì¡°", "ì œNì¡°ì˜N" ë“±
    article_pattern = r'ì œ\s*(\d+(?:ì˜\d+)?)\s*ì¡°(?:\s*[(ï¼ˆ][^)ï¼‰]*[)ï¼‰])?\s*([^\n]*?)(?=ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°|$)'
    
    matches = re.finditer(article_pattern, pdf_text, re.DOTALL | re.MULTILINE)
    
    for match in matches:
        article_num = match.group(1)
        article_title = match.group(2).strip() if match.group(2) else ""
        
        # ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ (ë‹¤ìŒ ì¡°ë¬¸ê¹Œì§€)
        start_pos = match.start()
        end_pos = match.end()
        
        # ì¡°ë¬¸ ë‚´ìš©ì„ ë” ìì„¸íˆ ì¶”ì¶œ
        content_match = re.search(
            rf'ì œ\s*{re.escape(article_num)}\s*ì¡°.*?(?=ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°|$)',
            pdf_text[start_pos:start_pos+2000],
            re.DOTALL
        )
        
        if content_match:
            content = content_match.group(0).strip()
        else:
            content = pdf_text[start_pos:min(start_pos+500, len(pdf_text))].strip()
        
        if content and len(content) > 10:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš© ì œì™¸
            articles.append({
                'article_number': article_num,
                'article_title': article_title,
                'content': content,
                'start_position': start_pos
            })
    
    return articles

def calculate_text_similarity(text1: str, text2: str, model) -> float:
    """ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        embeddings = model.encode([text1, text2])
        similarity = np.dot(embeddings[0], embeddings[1]) / (
            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
        )
        return float(similarity)
    except Exception as e:
        st.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        return 0.0

def analyze_violation_risk(ordinance_content: str, case_content: str, case_info: Dict) -> Dict[str, Any]:
    """ê°œë³„ ìœ„ë²• ìœ„í—˜ ë¶„ì„"""
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ê´€ë ¨ì„± ì ìˆ˜
    violation_keywords = {
        'ê¸°ê´€ìœ„ì„ì‚¬ë¬´': ['ìœ„ì„', 'ì‚¬ë¬´', 'ê¶Œí•œ', 'ì²˜ë¦¬', 'ì—…ë¬´'],
        'ìƒìœ„ë²•ë ¹ ìœ„ë°°': ['ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ìœ„ë°°', 'ì¶©ëŒ', 'ëª¨ìˆœ'],
        'ë²•ë¥ ìœ ë³´ ìœ„ë°°': ['ê¸°ë³¸ê¶Œ', 'ì œí•œ', 'ì˜ë¬´', 'ë¶€ê³¼', 'ê¶Œë¦¬', 'ììœ '],
        'ê¶Œí•œë°°ë¶„ ìœ„ë°°': ['êµ­ê°€ì‚¬ë¬´', 'ì§€ë°©ì‚¬ë¬´', 'ìì¹˜ì‚¬ë¬´', 'ë°°ë¶„', 'êµ¬ë¶„']
    }
    
    relevance_score = 0.0
    violation_type = "ì¼ë°˜ ìœ„ë²•"
    
    ordinance_lower = ordinance_content.lower()
    case_lower = case_content.lower()
    
    # ìœ„ë²• ìœ í˜•ë³„ ê´€ë ¨ì„± ê³„ì‚°
    type_scores = {}
    for v_type, keywords in violation_keywords.items():
        score = 0
        for keyword in keywords:
            if keyword in ordinance_lower and keyword in case_lower:
                score += 1
        type_scores[v_type] = score
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ìœ„ë²• ìœ í˜• ì„ íƒ
    if type_scores:
        violation_type = max(type_scores, key=type_scores.get)
        relevance_score = type_scores[violation_type] / len(violation_keywords[violation_type])
    
    # ìœ„í—˜ë„ ê³„ì‚° (0.0 ~ 1.0)
    risk_score = min(relevance_score * 0.8 + 0.2, 1.0)  # ê¸°ë³¸ 0.2 + ê´€ë ¨ì„± ì ìˆ˜
    
    return {
        'violation_type': violation_type,
        'risk_score': risk_score,
        'relevance_score': relevance_score,
        'case_summary': case_content[:200] + "..." if len(case_content) > 200 else case_content,
        'legal_principle': case_info.get('legal_principle', 'í•´ë‹¹ì—†ìŒ'),
        'recommendation': f"{violation_type} ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ê´€ë ¨ ë²•ë ¹ ê²€í†  í•„ìš”",
        'case_source': case_info.get('source', 'íŒë¡€ì§‘')
    }

def search_comprehensive_violation_cases(ordinance_articles: List[Dict], pkl_paths: List[str], max_results: int = 5) -> List[Dict]:
    """ì¢…í•© ìœ„ë²•ì„± íŒë¡€ ê²€ìƒ‰"""
    if not ordinance_articles:
        return []
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        st.write(f"[DEBUG] ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        comprehensive_results = []
        all_violation_risks = []  # ëª¨ë“  ì¡°ë¡€ì˜ ìœ„í—˜ ì‚¬ë¡€ë¥¼ ìˆ˜ì§‘
        
        # 1ë‹¨ê³„: ëª¨ë“  ì¡°ë¡€ì— ëŒ€í•´ ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰
        st.write(f"[DEBUG] ì´ {len(ordinance_articles)}ê°œ ì¡°ë¡€ì— ëŒ€í•´ ìœ„ë²• ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
        
        for article in ordinance_articles:
            article_results = {
                'ordinance_article': f"ì œ{article['article_number']}ì¡°",
                'ordinance_title': article['article_title'],
                'ordinance_content': article['content'],
                'violation_risks': []
            }
            
            # ê° PKL íŒŒì¼ì—ì„œ ê²€ìƒ‰
            for pkl_path in pkl_paths:
                if not os.path.exists(pkl_path):
                    continue
                
                vectorstore = load_vectorstore_safe(pkl_path)
                if not vectorstore:
                    continue
                
                try:
                    # ì¡°ë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                    content_keywords = []
                    
                    # ì‚¬ë¬´ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
                    if any(word in article['content'] for word in ['í—ˆê°€', 'ìŠ¹ì¸', 'ì‹ ê³ ', 'ì¸í—ˆê°€', 'ì§€ì •']):
                        content_keywords.extend(['ê¸°ê´€ìœ„ì„ì‚¬ë¬´', 'í—ˆê°€ì‚¬ë¬´', 'ì¸í—ˆê°€'])
                    
                    # ê¶Œí•œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ  
                    if any(word in article['content'] for word in ['ê¶Œí•œ', 'ì§€ì‹œ', 'ëª…ë ¹', 'ì²˜ë¶„']):
                        content_keywords.extend(['ê¶Œí•œìœ„ì„', 'ì²˜ë¶„ê¶Œí•œ'])
                    
                    # ë²•ë ¹ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
                    if any(word in article['content'] for word in ['ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™']):
                        content_keywords.extend(['ìƒìœ„ë²•ë ¹ìœ„ë°˜', 'ë²•ë ¹ì¶©ëŒ'])
                    
                    # ì¡°ë¬¸ ì œëª©ì—ì„œ í•µì‹¬ ë¶„ì•¼ ì¶”ì¶œ
                    title_field = ""
                    if any(word in article['article_title'] for word in ['ê±´ì¶•', 'ê±´ì„¤', 'ê°œë°œ']):
                        title_field = "ê±´ì¶•"
                        content_keywords.extend(['ê±´ì¶•í—ˆê°€', 'ê°œë°œí–‰ìœ„í—ˆê°€'])
                    elif any(word in article['article_title'] for word in ['í™˜ê²½', 'ëŒ€ê¸°', 'ìˆ˜ì§ˆ']):
                        title_field = "í™˜ê²½"
                        content_keywords.extend(['í™˜ê²½ì˜í–¥í‰ê°€', 'í™˜ê²½í—ˆê°€'])
                    elif any(word in article['article_title'] for word in ['ë„ì‹œ', 'ê³„íš', 'ìš©ë„']):
                        title_field = "ë„ì‹œê³„íš"
                        content_keywords.extend(['ë„ì‹œê³„íš', 'ìš©ë„ì§€ì—­'])
                    
                    # ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                    search_queries = [
                        f"{title_field} ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ ìœ„ë²•" if title_field else "ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ ìœ„ë²•",
                        f"{article['article_title']} ìœ„ë²• íŒë¡€",
                        "ì¡°ë¡€ ì œì •ê¶Œí•œ í•œê³„ ìœ„ë°˜",
                        "ìƒìœ„ë²•ë ¹ ìœ„ë°˜ ì¡°ë¡€",
                    ]
                    
                    # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì¿¼ë¦¬ ìƒì„±
                    if content_keywords:
                        for keyword in content_keywords[:3]:  # ìƒìœ„ 3ê°œë§Œ
                            search_queries.append(f"{keyword} ì¡°ë¡€ ìœ„ë²•")
                    
                    all_similarities = []
                    embeddings = vectorstore.get('embeddings', np.array([]))
                    
                    if len(embeddings) == 0:
                        continue
                    
                    # ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ í†µí•©
                    for query in search_queries:
                        query_embedding = model.encode([query])
                        similarities = np.dot(query_embedding, embeddings.T).flatten()
                        all_similarities.extend([(i, sim) for i, sim in enumerate(similarities)])
                    
                    # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœê³  ì ìˆ˜ë¡œ ì •ë ¬
                    idx_scores = {}
                    for idx, score in all_similarities:
                        if idx not in idx_scores or score > idx_scores[idx]:
                            idx_scores[idx] = score
                    
                    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
                    top_items = sorted(idx_scores.items(), key=lambda x: x[1], reverse=True)[:max_results]
                    
                    # PKL íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° documents ì‚¬ìš©
                    chunks = vectorstore.get('chunks', [])
                    if len(chunks) == 0:
                        # chunksê°€ ì—†ìœ¼ë©´ documents ì‚¬ìš©
                        documents = vectorstore.get('documents', [])
                        if documents:
                            # documentsë¥¼ chunks í˜•íƒœë¡œ ë³€í™˜
                            chunks = [{'text': doc} for doc in documents]
                    
                    st.write(f"[DEBUG] {article['article_title']} - ê²€ìƒ‰ëœ ê²°ê³¼ ìˆ˜: {len(top_items)}, ìµœê³  ìœ ì‚¬ë„: {top_items[0][1] if top_items else 0}, chunks: {len(chunks)}ê°œ")
                    
                    for idx, similarity in top_items:
                        if similarity > 0.15:  # ì„ê³„ê°’ ë‹¤ì‹œ ë†’ì„ (ê´€ë ¨ì„± ì¤‘ì‹œ)
                            chunk = chunks[idx]
                            chunk_text = chunk.get('text', '')
                            
                            # ê´€ë ¨ì„± ê²€ì¦ - í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            relevance_keywords = ['ì¡°ë¡€', 'ìœ„ë²•', 'ê¸°ê´€ìœ„ì„', 'ìƒìœ„ë²•ë ¹', 'ê¶Œí•œ', 'ì‚¬ë¬´']
                            relevance_score = sum(1 for keyword in relevance_keywords if keyword in chunk_text)
                            
                            # ì¡°ë¡€ì™€ ê´€ë ¨ëœ ë‚´ìš©ì¸ì§€ ì¶”ê°€ í™•ì¸
                            ordinance_indicators = ['ì¡°ë¡€ì•ˆ', 'ì¡°ë¡€ ì œì •', 'ì§€ë°©ìì¹˜ë‹¨ì²´', 'ìì¹˜ì‚¬ë¬´', 'ìœ„ì„ì‚¬ë¬´']
                            has_ordinance_context = any(indicator in chunk_text for indicator in ordinance_indicators)
                            
                            # ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ì œì™¸
                            if relevance_score < 2 and not has_ordinance_context:
                                st.write(f"[DEBUG] ê´€ë ¨ì„± ë¶€ì¡±ìœ¼ë¡œ ì œì™¸: {chunk_text[:100]}...")
                                continue
                            
                            # ìœ„ë²• ìœ„í—˜ ë¶„ì„
                            risk_analysis = analyze_violation_risk(
                                article['content'],
                                chunk['text'],
                                {
                                    'source': chunk.get('source', ''),
                                    'legal_principle': 'ë²•ë ¹ ìœ„ë°˜ ê¸ˆì§€ ì›ì¹™'
                                }
                            )
                            
                            # ìœ ì‚¬ë„ ë°˜ì˜
                            risk_analysis['similarity'] = float(similarity)
                            risk_analysis['risk_score'] = min(
                                risk_analysis['risk_score'] * (1 + similarity), 
                                1.0
                            )
                            
                            # ì¡°ë¡€ ì •ë³´ ì¶”ê°€í•´ì„œ ì „ì²´ ì»¬ë ‰ì…˜ì— ì €ì¥
                            risk_analysis['article_number'] = article['article_number']
                            risk_analysis['article_title'] = article['article_title']
                            article_results['violation_risks'].append(risk_analysis)
                            all_violation_risks.append(risk_analysis)  # ì „ì²´ ì»¬ë ‰ì…˜ì—ë„ ì¶”ê°€
                
                except Exception as e:
                    st.error(f"PKL ê²€ìƒ‰ ì˜¤ë¥˜ ({pkl_path}): {str(e)}")
                    continue
            
            # ìœ„í—˜ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ë§Œ ìœ ì§€
            article_results['violation_risks'].sort(key=lambda x: x['risk_score'], reverse=True)
            article_results['violation_risks'] = article_results['violation_risks'][:max_results]
            
            if article_results['violation_risks']:  # ìœ„í—˜ì´ ë°œê²¬ëœ ê²½ìš°ë§Œ ì¶”ê°€
                comprehensive_results.append(article_results)
        
        # 2ë‹¨ê³„: ì „ì²´ ìœ„í—˜ ì‚¬ë¡€ ê´€ë ¨ì„± í•„í„°ë§ ë° ìµœì í™”
        st.write(f"[DEBUG] 1ë‹¨ê³„ ì™„ë£Œ: {len(all_violation_risks)}ê°œ ìœ„í—˜ ì‚¬ë¡€ ìˆ˜ì§‘")
        
        if all_violation_risks:
            # ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ì‚¬ë¡€ ì •ë ¬
            all_violation_risks.sort(key=lambda x: (x['risk_score'], x['similarity']), reverse=True)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚° ë° í•„í„°ë§
            total_text_length = 0
            filtered_risks = []
            max_text_limit = 50000  # ì•½ 50K ë¬¸ì ì œí•œ
            
            for risk in all_violation_risks:
                case_text_length = len(risk.get('case_summary', '')) + len(risk.get('violation_type', ''))
                
                if total_text_length + case_text_length <= max_text_limit:
                    filtered_risks.append(risk)
                    total_text_length += case_text_length
                else:
                    # ê´€ë ¨ì„±ì´ ë§¤ìš° ë†’ì€ ê²½ìš°(ìœ„í—˜ë„ 0.7 ì´ìƒ)ë§Œ ì˜ˆì™¸ì ìœ¼ë¡œ í¬í•¨
                    if risk['risk_score'] >= 0.7 and len(filtered_risks) < max_results * 2:
                        filtered_risks.append(risk)
                        total_text_length += case_text_length
            
            st.write(f"[DEBUG] 2ë‹¨ê³„ ì™„ë£Œ: {len(filtered_risks)}ê°œ ì‚¬ë¡€ë¡œ í•„í„°ë§ (ì´ {total_text_length:,}ì)")
            
            # ì¡°ë¡€ë³„ë¡œ ì¬ë¶„ë°°
            for result in comprehensive_results:
                article_num = int(result['ordinance_article'].replace('ì œ', '').replace('ì¡°', ''))
                result['violation_risks'] = [
                    risk for risk in filtered_risks 
                    if risk.get('article_number') == article_num
                ][:max_results]  # ì¡°ë¡€ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        
        st.write(f"[DEBUG] ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ì™„ë£Œ: {len(comprehensive_results)}ê°œ ì¡°ë¬¸ì—ì„œ ìœ„í—˜ ë°œê²¬")
        return comprehensive_results
        
    except Exception as e:
        st.error(f"ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return []

def search_theoretical_background(problem_keywords: List[str], pkl_paths: List[str], max_results: int = 8, context_analysis: Dict = None) -> List[Dict]:
    """ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•œ ì´ë¡ ì  ë°°ê²½ì„ PKLì—ì„œ ê²€ìƒ‰"""
    if not problem_keywords:
        return []
    
    try:
        # ì—¬ëŸ¬ ëª¨ë¸ì„ ì‹œë„í•´ë³´ê¸°
        models_to_try = [
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'sentence-transformers/all-MiniLM-L6-v2', 
            'sentence-transformers/all-mpnet-base-v2'
        ]
        
        model = None
        for model_name in models_to_try:
            try:
                model = SentenceTransformer(model_name)
                st.write(f"[DEBUG] ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                break
            except Exception as e:
                st.write(f"[DEBUG] ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                continue
        
        if model is None:
            st.error("ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        theoretical_results = []

        # ğŸ” ë¬¸ë§¥ ê¸°ë°˜ ë™ì  ì¿¼ë¦¬ ìƒì„±
        all_search_queries = []

        if context_analysis:
            st.write(f"[DEBUG] ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼ í™œìš©: {len(context_analysis.get('key_concepts', []))}ê°œ ê°œë…")

            # 1. ì¶”ì¶œëœ í•µì‹¬ ê°œë… ê¸°ë°˜ ì¿¼ë¦¬
            for concept_info in context_analysis.get('key_concepts', []):
                concept = concept_info['concept']
                context = concept_info['context']

                # ë¬¸ë§¥ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
                context_keywords = []
                if 'í—ˆê°€' in context: context_keywords.append('í—ˆê°€ê¶Œí•œ')
                if 'ìŠ¹ì¸' in context: context_keywords.append('ìŠ¹ì¸ê¶Œí•œ')
                if 'ì²˜ë¶„' in context: context_keywords.append('í–‰ì •ì²˜ë¶„')
                if 'ìœ„ì„' in context: context_keywords.append('ê¶Œí•œìœ„ì„')

                # ê°œë…ë³„ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±
                all_search_queries.extend([
                    concept,
                    f"{concept} ì¡°ë¡€ ìœ„ë²•",
                    f"{concept} íŒë¡€",
                    f"{concept} í•œê³„"
                ])
                all_search_queries.extend(context_keywords)

            # 2. ë²•ì  ê·¼ê±° ê¸°ë°˜ ì¿¼ë¦¬
            for legal_basis in context_analysis.get('legal_basis', []):
                all_search_queries.extend([
                    legal_basis,
                    f"{legal_basis} ìœ„ë°˜",
                    f"{legal_basis} ì¡°ë¡€"
                ])

            # 3. ë¬¸ì œì  ìƒì„¸ ë‚´ìš© ê¸°ë°˜ ì¿¼ë¦¬ (í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ)
            for problem in context_analysis.get('problem_details', []):
                # ë¬¸ì œì ì—ì„œ í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ
                problem_keywords_extracted = []
                import re
                # í•œê¸€ ëª…ì‚¬ íŒ¨í„´ ì¶”ì¶œ (ë§¤ìš° ë‹¨ìˆœí•œ ë°©ì‹)
                nouns = re.findall(r'[ê°€-í£]{2,}(?:ì‚¬ë¬´|ê¶Œí•œ|ë²•ë ¹|ì¡°ë¡€|ìœ„ë°˜|ìœ„ë²•|í—ˆê°€|ìŠ¹ì¸|ì²˜ë¶„)', problem)
                problem_keywords_extracted.extend(nouns[:3])  # ìƒìœ„ 3ê°œë§Œ

                all_search_queries.extend([f"{noun} íŒë¡€" for noun in problem_keywords_extracted])

        # ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ë„ í¬í•¨
        for keyword in problem_keywords:
            # í‚¤ì›Œë“œë³„ íŠ¹í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            if keyword == "ê¸°ê´€ìœ„ì„ì‚¬ë¬´":
                all_search_queries.extend([
                    "ê¸°ê´€ìœ„ì„ì‚¬ë¬´", "ì¡°ë¡€ ì œì • ê¸ˆì§€", "ì§€ë°©ìì¹˜ë²• ì œ22ì¡°",
                    "êµ­ê°€ì‚¬ë¬´ ìœ„ì„", "ì‹œì¥ êµ°ìˆ˜ êµ¬ì²­ì¥", "ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€", "ê¸°ê´€ìœ„ì„ ê¸ˆì§€"
                ])
            elif keyword == "ìƒìœ„ë²•ë ¹":
                all_search_queries.extend([
                    "ìƒìœ„ë²•ë ¹", "ë²•ë ¹ìš°ìœ„", "ì¡°ë¡€ ë¬´íš¨", "ë²•ë ¹ ì¶©ëŒ", "ìƒìœ„ë²• ìœ„ë°˜", "ì¡°ë¡€ ìœ„ë²•"
                ])
            elif keyword == "ê¶Œí•œ":
                all_search_queries.extend([
                    "ê¶Œí•œ ìœ„ì„", "ê¶Œí•œë°°ë¶„", "ë²•ë¥ ìœ ë³´", "ì¡°ë¡€ ê¶Œí•œ", "ì§€ë°©ìì¹˜ë‹¨ì²´ ê¶Œí•œ"
                ])
            elif keyword == "ìœ„ë²•":
                all_search_queries.extend([
                    "ì¡°ë¡€ ìœ„ë²•", "ë¬´íš¨ ì¡°ë¡€", "ë²•ë ¹ ìœ„ë°˜", "ì¡°ë¡€ ìœ„ë°˜"
                ])
            elif keyword == "í—Œë²•ìœ„ë°˜":
                all_search_queries.extend([
                    "í—Œë²•ìœ„ë°˜", "ê¸°ë³¸ê¶Œì¹¨í•´", "í—Œë²•ì¬íŒì†Œ", "ìœ„í—Œì¡°ë¡€", "í—Œë²•ì  í•œê³„"
                ])
            elif keyword == "ê¸°ë³¸ê¶Œ":
                all_search_queries.extend([
                    "ê¸°ë³¸ê¶Œì¹¨í•´", "ì¬ì‚°ê¶Œ", "ì˜ì—…ì˜ììœ ", "í‰ë“±ê¶Œ", "ê¸°ë³¸ê¶Œ ì œí•œ"
                ])
            elif keyword in ["í‰ë“±ì›ì¹™", "ë¹„ë¡€ì›ì¹™"]:
                all_search_queries.extend([
                    f"{keyword}", f"{keyword} ìœ„ë°˜", "í—Œë²•ì›ì¹™", "ì¡°ë¡€ í•œê³„"
                ])
            elif keyword in ["ì¡°ì„¸", "ë²Œê¸ˆ", "ê³¼íƒœë£Œ"]:
                all_search_queries.extend([
                    "ì¡°ì„¸ë²•ë¥ ì£¼ì˜", "ë²Œê¸ˆ ë¶€ê³¼", "ê³¼íƒœë£Œ", "ë²•ë¥ ìœ ë³´", "ì¡°ë¡€ ë²Œì¹™"
                ])
            else:
                all_search_queries.extend([keyword, f"{keyword} ì¡°ë¡€", f"{keyword} ìœ„ë²•", f"{keyword} íŒë¡€"])

        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_queries = list(dict.fromkeys(all_search_queries))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
        st.write(f"[DEBUG] ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {len(unique_queries)}ê°œ")
        st.write(f"[DEBUG] ìƒìœ„ 5ê°œ ì¿¼ë¦¬: {unique_queries[:5]}")

        # PKL íŒŒì¼ë³„ ê²€ìƒ‰ (ë™ì  ì¿¼ë¦¬ ì‚¬ìš©)
        for pkl_path in pkl_paths:
                if not os.path.exists(pkl_path):
                    continue
                
                vectorstore = load_vectorstore_safe(pkl_path)
                if not vectorstore:
                    continue
                
                try:
                    # PKL íŒŒì¼ êµ¬ì¡° ìƒì„¸ í™•ì¸
                    st.write(f"[DEBUG] {pkl_path} êµ¬ì¡° í™•ì¸:")
                    st.write(f"[DEBUG]   - keys: {list(vectorstore.keys())}")
                    
                    embeddings = vectorstore.get('embeddings', np.array([]))
                    # PKL íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° documents ì‚¬ìš©
                    chunks = vectorstore.get('chunks', [])
                    if len(chunks) == 0:
                        # chunksê°€ ì—†ìœ¼ë©´ documents ì‚¬ìš©
                        documents = vectorstore.get('documents', [])
                        if documents:
                            # documentsë¥¼ chunks í˜•íƒœë¡œ ë³€í™˜
                            chunks = [{'text': doc} for doc in documents]
                            st.write(f"[DEBUG] documentsë¥¼ chunksë¡œ ë³€í™˜: {len(chunks)}ê°œ")
                    
                    st.write(f"[DEBUG]   - embeddings type: {type(embeddings)}, shape: {embeddings.shape if hasattr(embeddings, 'shape') else 'N/A'}")
                    st.write(f"[DEBUG]   - chunks type: {type(chunks)}, length: {len(chunks)}")
                    
                    if len(embeddings) == 0 or len(chunks) == 0:
                        st.write(f"[DEBUG] {pkl_path} - embeddings: {len(embeddings)}, chunks: {len(chunks)} (ê±´ë„ˆëœ€)")
                        continue
                    
                    # ì²« ë²ˆì§¸ chunk ë‚´ìš© í™•ì¸
                    if len(chunks) > 0:
                        first_chunk = chunks[0]
                        st.write(f"[DEBUG]   - ì²« ë²ˆì§¸ chunk êµ¬ì¡°: {type(first_chunk)}")
                        if isinstance(first_chunk, dict):
                            st.write(f"[DEBUG]   - chunk keys: {list(first_chunk.keys())}")
                            text_content = first_chunk.get('text', first_chunk.get('content', ''))[:100]
                            st.write(f"[DEBUG]   - ì²« 100ì: {text_content}")
                        else:
                            st.write(f"[DEBUG]   - chunk ë‚´ìš©: {str(first_chunk)[:100]}")
                    
                    # embeddingsì™€ chunks ê¸¸ì´ ì¼ì¹˜ í™•ì¸
                    if len(embeddings) != len(chunks):
                        st.write(f"[DEBUG] {pkl_path} - embeddings({len(embeddings)})ì™€ chunks({len(chunks)}) ê¸¸ì´ ë¶ˆì¼ì¹˜")
                        min_length = min(len(embeddings), len(chunks))
                        embeddings = embeddings[:min_length]
                        chunks = chunks[:min_length]
                        st.write(f"[DEBUG] {pkl_path} - {min_length}ê°œë¡œ ì¡°ì •")
                    
                    all_similarities = []
                    keyword_matches = []  # í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ë„ ì €ì¥
                    
                    # 1ì°¨: ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ (ë™ì  ì¿¼ë¦¬ ì‚¬ìš©)
                    for query in unique_queries[:15]:  # ìƒìœ„ 15ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš© (ì„±ëŠ¥ ê³ ë ¤)
                        try:
                            query_embedding = model.encode([query])
                            similarities = np.dot(query_embedding, embeddings.T).flatten()
                            all_similarities.extend([(i, sim) for i, sim in enumerate(similarities)])
                        except Exception as e:
                            st.write(f"[DEBUG] ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {str(e)}")

                    # 2ì°¨: ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ (ë°±ì—…) - ë™ì  ì¿¼ë¦¬ ì‚¬ìš©
                    for i, chunk in enumerate(chunks):
                        chunk_text = ""
                        if isinstance(chunk, dict):
                            chunk_text = chunk.get('text', chunk.get('content', ''))
                        else:
                            chunk_text = str(chunk)

                        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (í•µì‹¬ ì¿¼ë¦¬ë“¤ë¡œë§Œ)
                        keyword_score = 0
                        matched_queries = []
                        for query in unique_queries[:10]:  # ìƒìœ„ 10ê°œë§Œ
                            if query.lower() in chunk_text.lower():
                                keyword_score += 1
                                matched_queries.append(query)

                        if keyword_score > 0:
                            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ìœ ì‚¬ë„ì²˜ëŸ¼ ì‚¬ìš© (0.5 + ë§¤ì¹­ìˆ˜ * 0.1)
                            match_similarity = 0.5 + keyword_score * 0.1
                            keyword_matches.append((i, match_similarity))
                            st.write(f"[DEBUG] í‚¤ì›Œë“œ ë§¤ì¹­ ë°œê²¬: {matched_queries} - {keyword_score}ê°œ ë§¤ì¹­, ì ìˆ˜ {match_similarity:.3f}")
                    
                    # ì„ë² ë”© ê²°ê³¼ì™€ í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ ê²°í•©
                    all_similarities.extend(keyword_matches)
                    
                    # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœê³  ì ìˆ˜ë¡œ ì •ë ¬
                    idx_scores = {}
                    for idx, score in all_similarities:
                        if idx not in idx_scores or score > idx_scores[idx]:
                            idx_scores[idx] = score
                    
                    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
                    top_items = sorted(idx_scores.items(), key=lambda x: x[1], reverse=True)[:3]
                    
                    st.write(f"[DEBUG] {pkl_path} - ê²€ìƒ‰ ê²°ê³¼: {len(top_items)}ê°œ, chunks ê¸¸ì´: {len(chunks)}")
                    
                    for idx, similarity in top_items:
                        # ì¸ë±ìŠ¤ ë²”ìœ„ ì•ˆì „ ê²€ì‚¬
                        if idx >= len(chunks):
                            st.write(f"[DEBUG] ì¸ë±ìŠ¤ {idx}ê°€ chunks ê¸¸ì´ {len(chunks)}ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                            continue
                        
                        if similarity > 0.1:  # ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë§ì€ ê²°ê³¼ í¬í•¨
                            chunk = chunks[idx]
                            chunk_text = chunk.get('text', '')
                            
                            # ğŸ” ë¬¸ë§¥ ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€ (ë™ì )
                            relevance_score = 0
                            matched_concepts = []

                            # 1. ê¸°ë³¸ ë²•ì  ì§€í‘œ
                            theory_indicators = ['ì›ì¹™', 'íŒë¡€', 'í—Œë²•ì¬íŒì†Œ', 'ëŒ€ë²•ì›', 'ì´ë¡ ', 'í•™ì„¤', 'ë²•ë¦¬', 'ì¡°ë¡€', 'ìœ„ë²•', 'ë¬´íš¨', 'ë²•ë ¹', 'ìœ„ë°˜']
                            base_score = sum(1 for indicator in theory_indicators if indicator in chunk_text)
                            relevance_score += base_score

                            # 2. Gemini ë¶„ì„ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ê°œë…ê³¼ì˜ ë§¤ì¹­
                            if context_analysis:
                                for concept_info in context_analysis.get('key_concepts', []):
                                    concept = concept_info['concept']
                                    if concept in chunk_text:
                                        relevance_score += 2  # í•µì‹¬ ê°œë… ë§¤ì¹­ ì‹œ ë†’ì€ ì ìˆ˜
                                        matched_concepts.append(concept)

                                # 3. ë²•ì  ê·¼ê±°ì™€ì˜ ë§¤ì¹­
                                for legal_basis in context_analysis.get('legal_basis', []):
                                    if legal_basis in chunk_text:
                                        relevance_score += 3  # ë²•ì  ê·¼ê±° ë§¤ì¹­ ì‹œ ë” ë†’ì€ ì ìˆ˜
                                        matched_concepts.append(legal_basis)

                                # 4. ë¬¸ì œì  í‚¤ì›Œë“œì™€ì˜ ë§¤ì¹­
                                for problem in context_analysis.get('problem_details', []):
                                    # ë¬¸ì œì ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œí•˜ì—¬ ë§¤ì¹­
                                    problem_words = problem.split()[:5]  # ì²« 5ê°œ ë‹¨ì–´
                                    for word in problem_words:
                                        if len(word) > 1 and word in chunk_text:
                                            relevance_score += 1

                            # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ (ë™ì ìœ¼ë¡œ ì¡°ì •)
                            min_relevance = 2 if context_analysis else 1

                            if relevance_score >= min_relevance:
                                theoretical_results.append({
                                    'topic': ', '.join(problem_keywords) if len(problem_keywords) > 1 else problem_keywords[0],
                                    'content': chunk_text,
                                    'relevance_score': float(similarity),
                                    'context_relevance': relevance_score,  # ë¬¸ë§¥ ê´€ë ¨ì„± ì ìˆ˜
                                    'matched_concepts': matched_concepts,   # ë§¤ì¹­ëœ ê°œë…ë“¤
                                    'source': pkl_path,
                                    'query_used': unique_queries[0] if unique_queries else "ê¸°ë³¸ê²€ìƒ‰"
                                })
                                st.write(f"[DEBUG] âœ… ë°œê²¬: ìœ ì‚¬ë„ {similarity:.3f}, ë¬¸ë§¥ê´€ë ¨ì„± {relevance_score}, ë§¤ì¹­ê°œë…: {matched_concepts}")
                            else:
                                st.write(f"[DEBUG] âŒ ì œì™¸: ìœ ì‚¬ë„ {similarity:.3f}, ë¬¸ë§¥ê´€ë ¨ì„± {relevance_score} (ê¸°ì¤€: {min_relevance})")
                
                except Exception as e:
                    st.error(f"ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ì˜¤ë¥˜ ({pkl_path}): {str(e)}")
                    continue
        
        # ë¬¸ë§¥ ê´€ë ¨ì„±ê³¼ ìœ ì‚¬ë„ë¥¼ ì¢…í•©í•˜ì—¬ ì •ë ¬ (ë¬¸ë§¥ ê´€ë ¨ì„±ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        theoretical_results.sort(key=lambda x: (
            x.get('context_relevance', 0) * 0.7 + x['relevance_score'] * 0.3
        ), reverse=True)
        
        # ì¤‘ë³µ ì œê±° (ìœ ì‚¬í•œ ë‚´ìš© í•„í„°ë§)
        filtered_results = []
        seen_content = set()
        
        for result in theoretical_results:
            content_hash = result['content'][:100]  # ì²« 100ìë¡œ ì¤‘ë³µ íŒë³„
            if content_hash not in seen_content:
                filtered_results.append(result)
                seen_content.add(content_hash)
                
                if len(filtered_results) >= max_results:
                    break
        
        st.write(f"[DEBUG] ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼")
        return filtered_results
        
    except Exception as e:
        st.error(f"ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def apply_violation_cases_to_ordinance(violation_cases: List[Dict], ordinance_text: str, pkl_paths: List[str]) -> List[Dict]:
    """ìœ„ë²• íŒë¡€ë¥¼ ì¡°ë¡€ì•ˆì— ì§ì ‘ ì ìš©"""
    if not violation_cases:
        return []
    
    try:
        st.write(f"[DEBUG] ìœ„ë²• íŒë¡€ ì ìš© ì‹œì‘: {len(violation_cases)}ê°œ íŒë¡€")
        
        # ì¡°ë¡€ ì¡°ë¬¸ ì¶”ì¶œ
        ordinance_articles = extract_ordinance_articles(ordinance_text)
        st.write(f"[DEBUG] ì¡°ë¡€ ì¡°ë¬¸ ì¶”ì¶œ: {len(ordinance_articles)}ê°œ")
        
        if not ordinance_articles:
            return []
        
        # ì¢…í•© ìœ„ë²•ì„± ê²€ìƒ‰ ìˆ˜í–‰
        comprehensive_results = search_comprehensive_violation_cases(ordinance_articles, pkl_paths)
        
        return comprehensive_results
        
    except Exception as e:
        st.error(f"ìœ„ë²• íŒë¡€ ì ìš© ì˜¤ë¥˜: {str(e)}")
        return []

def format_comprehensive_analysis_result(results: List[Dict]) -> str:
    """ì¢…í•© ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…"""
    if not results:
        return "ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ì—ì„œ íŠ¹ë³„í•œ ìœ„í—˜ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    formatted_result = "ğŸš¨ **ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼**\n\n"
    
    total_risks = sum(len(result['violation_risks']) for result in results)
    formatted_result += f"**ì´ {len(results)}ê°œ ì¡°ë¬¸ì—ì„œ {total_risks}ê°œì˜ ìœ„ë²• ìœ„í—˜ ë°œê²¬**\n\n"
    
    for result in results:
        formatted_result += f"### {result['ordinance_article']} {result.get('ordinance_title', '')}\n"
        formatted_result += f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:150]}...\n\n"
        
        for i, risk in enumerate(result['violation_risks'][:3], 1):
            formatted_result += f"**ìœ„í—˜ {i}: {risk['violation_type']}** (ìœ„í—˜ë„: {risk['risk_score']:.2f})\n"
            formatted_result += f"- ê´€ë ¨ íŒë¡€: {risk['case_summary']}\n"
            formatted_result += f"- ê°œì„  ê¶Œê³ : {risk['recommendation']}\n"
            formatted_result += f"- íŒë¡€ ì¶œì²˜: {risk['case_source']}\n\n"
        
        if len(result['violation_risks']) > 3:
            formatted_result += f"*(ì™¸ {len(result['violation_risks']) - 3}ê°œ ì¶”ê°€ ìœ„í—˜)*\n\n"
        
        formatted_result += "---\n\n"
    
    return formatted_result

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_comprehensive_analysis():
    """ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    sample_ordinance = """
    ì œ1ì¡°(ëª©ì ) ì´ ì¡°ë¡€ëŠ” ì£¼ì°¨ì¥ì˜ ì„¤ì¹˜ ë° ê´€ë¦¬ì— ê´€í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
    
    ì œ2ì¡°(ì •ì˜) ì´ ì¡°ë¡€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´ì˜ ëœ»ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. "ì£¼ì°¨ì¥"ì´ë€ ìë™ì°¨ë¥¼ ì£¼ì°¨ì‹œí‚¤ê¸° ìœ„í•œ ì‹œì„¤ì„ ë§í•œë‹¤.
    
    ì œ3ì¡°(ì£¼ì°¨ì¥ ì„¤ì¹˜ ê¸°ì¤€) ì£¼ì°¨ì¥ì˜ ì„¤ì¹˜ ê¸°ì¤€ì€ ì‹œì¥ì´ ì •í•œë‹¤.
    """
    
    pkl_paths = [
        'enhanced_vectorstore_20250914_101739.pkl'
    ]
    
    # ì¡°ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    articles = extract_ordinance_articles(sample_ordinance)
    print(f"ì¶”ì¶œëœ ì¡°ë¬¸ ìˆ˜: {len(articles)}")
    
    # ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸
    if articles:
        results = search_comprehensive_violation_cases(articles, pkl_paths)
        print(f"ë¶„ì„ ê²°ê³¼: {len(results)}ê°œ ì¡°ë¬¸ì—ì„œ ìœ„í—˜ ë°œê²¬")
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = format_comprehensive_analysis_result(results)
        print(formatted[:500] + "...")

if __name__ == "__main__":
    test_comprehensive_analysis()