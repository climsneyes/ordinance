"""
ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ëª¨ë“ˆ
ì¡°ë¡€ì•ˆê³¼ PKL ë°ì´í„°ë² ì´ìŠ¤ì˜ ìœ„ë²• íŒë¡€ë¥¼ ë§¤ì¹­í•˜ì—¬ ì¢…í•©ì ì¸ ìœ„ë²•ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ë²•ë ¹ëª… ì •ê·œí™”ë¥¼ í†µí•´ ì¤‘ë³µì„ ì œê±°í•˜ê³  Gemini API í˜¸ì¶œì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""

import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import os
from typing import List, Dict, Any, Tuple
import streamlit as st
from law_name_normalizer import LawNameNormalizer

def load_vectorstore_safe(pkl_path: str) -> Dict[str, Any]:
    """ì•ˆì „í•œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
    try:
        if not os.path.exists(pkl_path):
            return None
        
        with open(pkl_path, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨ ({pkl_path}): {str(e)}")
        return None

def extract_ordinance_articles(pdf_text: str) -> List[Dict[str, str]]:
    """ì¡°ë¡€ì•ˆì—ì„œ ì¡°ë¬¸ ì¶”ì¶œ"""
    articles = []
    
    # ì¡°ë¬¸ íŒ¨í„´: "ì œNì¡°", "ì œNì¡°ì˜N" ë“±
    article_pattern = r'ì œ\s*(\d+(?:ì˜\d+)?)\s*ì¡°(?:\s*[(ï¼ˆ][^)ï¼‰]*[)ï¼‰])?\s*([^\n]*?)(?=ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°|$)'
    
    matches = re.finditer(article_pattern, pdf_text, re.DOTALL | re.MULTILINE)
    
    for match in matches:
        article_num = match.group(1)
        article_title = match.group(2).strip() if match.group(2) else ""
        
        # ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ (ë‹¤ìŒ ì¡°ë¬¸ê¹Œì§€)
        start_pos = match.start()
        end_pos = match.end()
        
        # ì¡°ë¬¸ ë‚´ìš©ì„ ë” ìì„¸íˆ ì¶”ì¶œ
        content_match = re.search(
            rf'ì œ\s*{re.escape(article_num)}\s*ì¡°.*?(?=ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°|$)',
            pdf_text[start_pos:start_pos+2000],
            re.DOTALL
        )
        
        if content_match:
            content = content_match.group(0).strip()
        else:
            content = pdf_text[start_pos:min(start_pos+500, len(pdf_text))].strip()
        
        if content and len(content) > 10:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš© ì œì™¸
            articles.append({
                'article_number': article_num,
                'article_title': article_title,
                'content': content,
                'start_position': start_pos
            })
    
    return articles

def extract_law_names_from_text(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
    law_names = []

    # ë²•ë ¹ëª… íŒ¨í„´ë“¤
    patterns = [
        r'([^.\n]*?ë²•)[.\s]',  # ~ë²•
        r'([^.\n]*?ë ¹)[.\s]',  # ~ë ¹ (ì‹œí–‰ë ¹, ëŒ€í†µë ¹ë ¹ ë“±)
        r'([^.\n]*?ê·œì¹™)[.\s]',  # ~ê·œì¹™
        r'([^.\n]*?ì¡°ë¡€)[.\s]',  # ~ì¡°ë¡€
        r'([^.\n]*?ê·œì •)[.\s]',  # ~ê·œì •
        r'í—Œë²•\s*ì œ\s*\d+ì¡°',  # í—Œë²• ì¡°ë¬¸
        r'(ì§€ë°©ìì¹˜ë²•)',  # íŠ¹ì • ì¤‘ìš” ë²•ë ¹
        r'(ì§€ë°©êµë¶€ì„¸ë²•)',
        r'(êµ­ê°€ì¬ì •ë²•)',
        r'(ê³µê³µê¸°ê´€ì˜\s*ìš´ì˜ì—\s*ê´€í•œ\s*ë²•ë¥ )',
        r'(í–‰ì •ì ˆì°¨ë²•)',
        r'(í–‰ì •ê¸°ë³¸ë²•)',
    ]

    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            if isinstance(match, tuple):
                match = match[0]

            # ì •ë¦¬
            clean_name = re.sub(r'\s+', ' ', match).strip()
            if len(clean_name) >= 3 and clean_name not in law_names:
                law_names.append(clean_name)

    return law_names

def normalize_and_deduplicate_laws(law_names: List[str]) -> List[str]:
    """ë²•ë ¹ëª… ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°"""
    if not law_names:
        return []

    normalizer = LawNameNormalizer()

    try:
        # ë²•ë ¹ëª… ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
        normalized_laws = normalizer.deduplicate_laws(law_names, min_similarity=0.85)

        st.write(f"[DEBUG] ë²•ë ¹ëª… ì •ê·œí™”: {len(law_names)}ê°œ â†’ {len(normalized_laws)}ê°œ")

        # ì •ê·œí™” ê²°ê³¼ í‘œì‹œ
        if len(law_names) != len(normalized_laws):
            st.write(f"[INFO] ì¤‘ë³µ ì œê±°ëœ ë²•ë ¹:")
            for i, (original, normalized) in enumerate(zip(law_names, normalized_laws)):
                if original != normalized:
                    st.write(f"  - '{original}' â†’ '{normalized}'")

        return normalized_laws

    except Exception as e:
        st.error(f"ë²•ë ¹ëª… ì •ê·œí™” ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ì‹œ ê°„ë‹¨í•œ ì¤‘ë³µ ì œê±°ë§Œ ìˆ˜í–‰
        return list(set(law_names))

def calculate_text_similarity(text1: str, text2: str, model) -> float:
    """ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        embeddings = model.encode([text1, text2])
        similarity = np.dot(embeddings[0], embeddings[1]) / (
            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
        )
        return float(similarity)
    except Exception as e:
        st.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        return 0.0

def analyze_violation_risk(ordinance_content: str, case_content: str, case_info: Dict) -> Dict[str, Any]:
    """ê°œë³„ ìœ„ë²• ìœ„í—˜ ë¶„ì„"""
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ê´€ë ¨ì„± ì ìˆ˜
    violation_keywords = {
        'ê¸°ê´€ìœ„ì„ì‚¬ë¬´': ['ìœ„ì„', 'ì‚¬ë¬´', 'ê¶Œí•œ', 'ì²˜ë¦¬', 'ì—…ë¬´'],
        'ìƒìœ„ë²•ë ¹ ìœ„ë°°': ['ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ìœ„ë°°', 'ì¶©ëŒ', 'ëª¨ìˆœ'],
        'ë²•ë¥ ìœ ë³´ ìœ„ë°°': ['ê¸°ë³¸ê¶Œ', 'ì œí•œ', 'ì˜ë¬´', 'ë¶€ê³¼', 'ê¶Œë¦¬', 'ììœ '],
        'ê¶Œí•œë°°ë¶„ ìœ„ë°°': ['êµ­ê°€ì‚¬ë¬´', 'ì§€ë°©ì‚¬ë¬´', 'ìì¹˜ì‚¬ë¬´', 'ë°°ë¶„', 'êµ¬ë¶„']
    }
    
    relevance_score = 0.0
    violation_type = "ì¼ë°˜ ìœ„ë²•"
    
    ordinance_lower = ordinance_content.lower()
    case_lower = case_content.lower()
    
    # ìœ„ë²• ìœ í˜•ë³„ ê´€ë ¨ì„± ê³„ì‚°
    type_scores = {}
    for v_type, keywords in violation_keywords.items():
        score = 0
        for keyword in keywords:
            if keyword in ordinance_lower and keyword in case_lower:
                score += 1
        type_scores[v_type] = score
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ìœ„ë²• ìœ í˜• ì„ íƒ
    if type_scores:
        violation_type = max(type_scores, key=type_scores.get)
        relevance_score = type_scores[violation_type] / len(violation_keywords[violation_type])
    
    # ìœ„í—˜ë„ ê³„ì‚° (0.0 ~ 1.0)
    risk_score = min(relevance_score * 0.8 + 0.2, 1.0)  # ê¸°ë³¸ 0.2 + ê´€ë ¨ì„± ì ìˆ˜
    
    return {
        'violation_type': violation_type,
        'risk_score': risk_score,
        'relevance_score': relevance_score,
        'case_summary': case_content[:200] + "..." if len(case_content) > 200 else case_content,
        'legal_principle': case_info.get('legal_principle', 'í•´ë‹¹ì—†ìŒ'),
        'recommendation': f"{violation_type} ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ê´€ë ¨ ë²•ë ¹ ê²€í†  í•„ìš”",
        'case_source': case_info.get('source', 'íŒë¡€ì§‘')
    }

def search_comprehensive_violation_cases(ordinance_articles: List[Dict], pkl_paths: List[str], max_results: int = 5) -> List[Dict]:
    """ì¢…í•© ìœ„ë²•ì„± íŒë¡€ ê²€ìƒ‰"""
    if not ordinance_articles:
        return []
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        st.write(f"[DEBUG] ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        comprehensive_results = []
        all_violation_risks = []  # ëª¨ë“  ì¡°ë¡€ì˜ ìœ„í—˜ ì‚¬ë¡€ë¥¼ ìˆ˜ì§‘
        
        # 1ë‹¨ê³„: ëª¨ë“  ì¡°ë¡€ì— ëŒ€í•´ ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰
        st.write(f"[DEBUG] ì´ {len(ordinance_articles)}ê°œ ì¡°ë¡€ì— ëŒ€í•´ ìœ„ë²• ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
        
        for article in ordinance_articles:
            article_results = {
                'ordinance_article': f"ì œ{article['article_number']}ì¡°",
                'ordinance_title': article['article_title'],
                'ordinance_content': article['content'],
                'violation_risks': []
            }
            
            # ê° PKL íŒŒì¼ì—ì„œ ê²€ìƒ‰
            for pkl_path in pkl_paths:
                if not os.path.exists(pkl_path):
                    continue
                
                vectorstore = load_vectorstore_safe(pkl_path)
                if not vectorstore:
                    continue
                
                try:
                    # ì¡°ë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                    content_keywords = []
                    
                    # ì‚¬ë¬´ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
                    if any(word in article['content'] for word in ['í—ˆê°€', 'ìŠ¹ì¸', 'ì‹ ê³ ', 'ì¸í—ˆê°€', 'ì§€ì •']):
                        content_keywords.extend(['ê¸°ê´€ìœ„ì„ì‚¬ë¬´', 'í—ˆê°€ì‚¬ë¬´', 'ì¸í—ˆê°€'])
                    
                    # ê¶Œí•œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ  
                    if any(word in article['content'] for word in ['ê¶Œí•œ', 'ì§€ì‹œ', 'ëª…ë ¹', 'ì²˜ë¶„']):
                        content_keywords.extend(['ê¶Œí•œìœ„ì„', 'ì²˜ë¶„ê¶Œí•œ'])
                    
                    # ë²•ë ¹ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
                    if any(word in article['content'] for word in ['ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™']):
                        content_keywords.extend(['ìƒìœ„ë²•ë ¹ìœ„ë°˜', 'ë²•ë ¹ì¶©ëŒ'])
                    
                    # ì¡°ë¬¸ ì œëª©ì—ì„œ í•µì‹¬ ë¶„ì•¼ ì¶”ì¶œ
                    title_field = ""
                    if any(word in article['article_title'] for word in ['ê±´ì¶•', 'ê±´ì„¤', 'ê°œë°œ']):
                        title_field = "ê±´ì¶•"
                        content_keywords.extend(['ê±´ì¶•í—ˆê°€', 'ê°œë°œí–‰ìœ„í—ˆê°€'])
                    elif any(word in article['article_title'] for word in ['í™˜ê²½', 'ëŒ€ê¸°', 'ìˆ˜ì§ˆ']):
                        title_field = "í™˜ê²½"
                        content_keywords.extend(['í™˜ê²½ì˜í–¥í‰ê°€', 'í™˜ê²½í—ˆê°€'])
                    elif any(word in article['article_title'] for word in ['ë„ì‹œ', 'ê³„íš', 'ìš©ë„']):
                        title_field = "ë„ì‹œê³„íš"
                        content_keywords.extend(['ë„ì‹œê³„íš', 'ìš©ë„ì§€ì—­'])
                    
                    # ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                    search_queries = [
                        f"{title_field} ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ ìœ„ë²•" if title_field else "ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ ìœ„ë²•",
                        f"{article['article_title']} ìœ„ë²• íŒë¡€",
                        "ì¡°ë¡€ ì œì •ê¶Œí•œ í•œê³„ ìœ„ë°˜",
                        "ìƒìœ„ë²•ë ¹ ìœ„ë°˜ ì¡°ë¡€",
                    ]
                    
                    # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì¿¼ë¦¬ ìƒì„±
                    if content_keywords:
                        for keyword in content_keywords[:3]:  # ìƒìœ„ 3ê°œë§Œ
                            search_queries.append(f"{keyword} ì¡°ë¡€ ìœ„ë²•")
                    
                    all_similarities = []
                    embeddings = vectorstore.get('embeddings', np.array([]))
                    
                    if len(embeddings) == 0:
                        continue
                    
                    # ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ í†µí•©
                    for query in search_queries:
                        query_embedding = model.encode([query])
                        similarities = np.dot(query_embedding, embeddings.T).flatten()
                        all_similarities.extend([(i, sim) for i, sim in enumerate(similarities)])
                    
                    # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœê³  ì ìˆ˜ë¡œ ì •ë ¬
                    idx_scores = {}
                    for idx, score in all_similarities:
                        if idx not in idx_scores or score > idx_scores[idx]:
                            idx_scores[idx] = score
                    
                    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
                    top_items = sorted(idx_scores.items(), key=lambda x: x[1], reverse=True)[:max_results]
                    
                    # PKL íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° documents ì‚¬ìš©
                    chunks = vectorstore.get('chunks', [])
                    if len(chunks) == 0:
                        # chunksê°€ ì—†ìœ¼ë©´ documents ì‚¬ìš©
                        documents = vectorstore.get('documents', [])
                        if documents:
                            # documentsë¥¼ chunks í˜•íƒœë¡œ ë³€í™˜
                            chunks = [{'text': doc} for doc in documents]
                    
                    st.write(f"[DEBUG] {article['article_title']} - ê²€ìƒ‰ëœ ê²°ê³¼ ìˆ˜: {len(top_items)}, ìµœê³  ìœ ì‚¬ë„: {top_items[0][1] if top_items else 0}, chunks: {len(chunks)}ê°œ")
                    
                    for idx, similarity in top_items:
                        if similarity > 0.15:  # ì„ê³„ê°’ ë‹¤ì‹œ ë†’ì„ (ê´€ë ¨ì„± ì¤‘ì‹œ)
                            chunk = chunks[idx]
                            chunk_text = chunk.get('text', '')
                            
                            # ê´€ë ¨ì„± ê²€ì¦ - í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            relevance_keywords = ['ì¡°ë¡€', 'ìœ„ë²•', 'ê¸°ê´€ìœ„ì„', 'ìƒìœ„ë²•ë ¹', 'ê¶Œí•œ', 'ì‚¬ë¬´']
                            relevance_score = sum(1 for keyword in relevance_keywords if keyword in chunk_text)
                            
                            # ì¡°ë¡€ì™€ ê´€ë ¨ëœ ë‚´ìš©ì¸ì§€ ì¶”ê°€ í™•ì¸
                            ordinance_indicators = ['ì¡°ë¡€ì•ˆ', 'ì¡°ë¡€ ì œì •', 'ì§€ë°©ìì¹˜ë‹¨ì²´', 'ìì¹˜ì‚¬ë¬´', 'ìœ„ì„ì‚¬ë¬´']
                            has_ordinance_context = any(indicator in chunk_text for indicator in ordinance_indicators)
                            
                            # ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ì œì™¸
                            if relevance_score < 2 and not has_ordinance_context:
                                st.write(f"[DEBUG] ê´€ë ¨ì„± ë¶€ì¡±ìœ¼ë¡œ ì œì™¸: {chunk_text[:100]}...")
                                continue
                            
                            # ìœ„ë²• ìœ„í—˜ ë¶„ì„
                            risk_analysis = analyze_violation_risk(
                                article['content'],
                                chunk['text'],
                                {
                                    'source': chunk.get('source', ''),
                                    'legal_principle': 'ë²•ë ¹ ìœ„ë°˜ ê¸ˆì§€ ì›ì¹™'
                                }
                            )
                            
                            # ìœ ì‚¬ë„ ë°˜ì˜
                            risk_analysis['similarity'] = float(similarity)
                            risk_analysis['risk_score'] = min(
                                risk_analysis['risk_score'] * (1 + similarity), 
                                1.0
                            )
                            
                            # ì¡°ë¡€ ì •ë³´ ì¶”ê°€í•´ì„œ ì „ì²´ ì»¬ë ‰ì…˜ì— ì €ì¥
                            risk_analysis['article_number'] = article['article_number']
                            risk_analysis['article_title'] = article['article_title']
                            article_results['violation_risks'].append(risk_analysis)
                            all_violation_risks.append(risk_analysis)  # ì „ì²´ ì»¬ë ‰ì…˜ì—ë„ ì¶”ê°€
                
                except Exception as e:
                    st.error(f"PKL ê²€ìƒ‰ ì˜¤ë¥˜ ({pkl_path}): {str(e)}")
                    continue
            
            # ìœ„í—˜ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ë§Œ ìœ ì§€
            article_results['violation_risks'].sort(key=lambda x: x['risk_score'], reverse=True)
            article_results['violation_risks'] = article_results['violation_risks'][:max_results]
            
            if article_results['violation_risks']:  # ìœ„í—˜ì´ ë°œê²¬ëœ ê²½ìš°ë§Œ ì¶”ê°€
                comprehensive_results.append(article_results)
        
        # 2ë‹¨ê³„: ì „ì²´ ìœ„í—˜ ì‚¬ë¡€ ê´€ë ¨ì„± í•„í„°ë§ ë° ìµœì í™”
        st.write(f"[DEBUG] 1ë‹¨ê³„ ì™„ë£Œ: {len(all_violation_risks)}ê°œ ìœ„í—˜ ì‚¬ë¡€ ìˆ˜ì§‘")
        
        if all_violation_risks:
            # ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ì‚¬ë¡€ ì •ë ¬
            all_violation_risks.sort(key=lambda x: (x['risk_score'], x['similarity']), reverse=True)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚° ë° í•„í„°ë§
            total_text_length = 0
            filtered_risks = []
            max_text_limit = 50000  # ì•½ 50K ë¬¸ì ì œí•œ
            
            for risk in all_violation_risks:
                case_text_length = len(risk.get('case_summary', '')) + len(risk.get('violation_type', ''))
                
                if total_text_length + case_text_length <= max_text_limit:
                    filtered_risks.append(risk)
                    total_text_length += case_text_length
                else:
                    # ê´€ë ¨ì„±ì´ ë§¤ìš° ë†’ì€ ê²½ìš°(ìœ„í—˜ë„ 0.7 ì´ìƒ)ë§Œ ì˜ˆì™¸ì ìœ¼ë¡œ í¬í•¨
                    if risk['risk_score'] >= 0.7 and len(filtered_risks) < max_results * 2:
                        filtered_risks.append(risk)
                        total_text_length += case_text_length
            
            st.write(f"[DEBUG] 2ë‹¨ê³„ ì™„ë£Œ: {len(filtered_risks)}ê°œ ì‚¬ë¡€ë¡œ í•„í„°ë§ (ì´ {total_text_length:,}ì)")
            
            # ì¡°ë¡€ë³„ë¡œ ì¬ë¶„ë°°
            for result in comprehensive_results:
                article_num = int(result['ordinance_article'].replace('ì œ', '').replace('ì¡°', ''))
                result['violation_risks'] = [
                    risk for risk in filtered_risks 
                    if risk.get('article_number') == article_num
                ][:max_results]  # ì¡°ë¡€ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        
        st.write(f"[DEBUG] ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ì™„ë£Œ: {len(comprehensive_results)}ê°œ ì¡°ë¬¸ì—ì„œ ìœ„í—˜ ë°œê²¬")
        return comprehensive_results

    except Exception as e:
        st.error(f"ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return []

def extract_and_normalize_relevant_laws(comprehensive_results: List[Dict]) -> Dict[str, List[str]]:
    """ë¶„ì„ ê²°ê³¼ì—ì„œ ê´€ë ¨ ë²•ë ¹ëª…ì„ ì¶”ì¶œí•˜ê³  ì •ê·œí™”"""
    if not comprehensive_results:
        return {'normalized_laws': [], 'law_details': []}

    st.write("### ğŸ“‹ ê´€ë ¨ ë²•ë ¹ëª… ì¶”ì¶œ ë° ì •ê·œí™”")

    all_law_names = []
    law_sources = {}  # ë²•ë ¹ì´ ì–´ëŠ ì¡°ë¬¸ì—ì„œ ë‚˜ì™”ëŠ”ì§€ ì¶”ì 

    try:
        # 1. ëª¨ë“  ìœ„ë²• ì‚¬ë¡€ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ
        for result in comprehensive_results:
            article_num = result.get('ordinance_article', '')

            # ì¡°ë¡€ ë‚´ìš©ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ
            ordinance_content = result.get('ordinance_content', '')
            if ordinance_content:
                extracted_laws = extract_law_names_from_text(ordinance_content)
                for law in extracted_laws:
                    all_law_names.append(law)
                    if law not in law_sources:
                        law_sources[law] = []
                    law_sources[law].append(f"{article_num} (ì¡°ë¡€)")

            # ìœ„í—˜ ì‚¬ë¡€ ë‚´ìš©ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ
            for violation_risk in result.get('violation_risks', []):
                case_summary = violation_risk.get('case_summary', '')
                if case_summary:
                    extracted_laws = extract_law_names_from_text(case_summary)
                    for law in extracted_laws:
                        all_law_names.append(law)
                        if law not in law_sources:
                            law_sources[law] = []
                        law_sources[law].append(f"{article_num} (ì‚¬ë¡€)")

        st.write(f"[DEBUG] ì´ {len(all_law_names)}ê°œ ë²•ë ¹ëª… ì¶”ì¶œë¨")

        # 2. ë²•ë ¹ëª… ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
        if all_law_names:
            normalized_laws = normalize_and_deduplicate_laws(all_law_names)

            # 3. ì •ê·œí™”ëœ ë²•ë ¹ ì •ë³´ì™€ ì¶œì²˜ ë§¤í•‘
            law_details = []
            normalizer = LawNameNormalizer()

            for law in normalized_laws:
                # ì •ê·œí™”ëœ ë²•ë ¹ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
                law_info = normalizer.get_best_match_with_info(law)

                # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                related_sources = []
                for original_law, sources in law_sources.items():
                    # ì›ë³¸ ë²•ë ¹ê³¼ ì •ê·œí™”ëœ ë²•ë ¹ì´ ìœ ì‚¬í•œì§€ í™•ì¸
                    if normalizer._calculate_similarity(
                        original_law.lower(),
                        law.lower()
                    ) >= 0.8:
                        related_sources.extend(sources)

                law_details.append({
                    'law_name': law_info['title'],
                    'law_number': law_info.get('number', ''),
                    'law_type': law_info.get('type', ''),
                    'enforcement_date': law_info.get('enforcement_date', ''),
                    'similarity_score': law_info.get('similarity', 0.0),
                    'related_articles': list(set(related_sources)),  # ì¤‘ë³µ ì œê±°
                    'api_error': law_info.get('error', None)
                })

            # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
            st.write(f"**âœ… ì •ê·œí™” ì™„ë£Œ**: {len(normalized_laws)}ê°œ ë²•ë ¹")

            # ì¤‘ë³µ ì œê±° íš¨ê³¼ í‘œì‹œ
            if len(all_law_names) > len(normalized_laws):
                st.success(f"ğŸ¯ ì¤‘ë³µ ì œê±° íš¨ê³¼: {len(all_law_names)}ê°œ â†’ {len(normalized_laws)}ê°œ ({((len(all_law_names) - len(normalized_laws)) / len(all_law_names) * 100):.1f}% ê°ì†Œ)")

            # ìƒìœ„ ê´€ë ¨ ë²•ë ¹ í‘œì‹œ
            with st.expander("ğŸ” ì¶”ì¶œëœ ì£¼ìš” ë²•ë ¹ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                for i, detail in enumerate(law_details[:10], 1):  # ìƒìœ„ 10ê°œë§Œ
                    st.write(f"{i}. **{detail['law_name']}**")
                    if detail['law_number']:
                        st.write(f"   - ë²•ë ¹ë²ˆí˜¸: {detail['law_number']}")
                    if detail['related_articles']:
                        st.write(f"   - ê´€ë ¨ ì¡°ë¬¸: {', '.join(detail['related_articles'][:3])}")  # ìµœëŒ€ 3ê°œë§Œ
                    if detail['api_error']:
                        st.write(f"   âš ï¸ API ì˜¤ë¥˜: {detail['api_error']}")

            return {
                'normalized_laws': normalized_laws,
                'law_details': law_details,
                'original_count': len(all_law_names),
                'normalized_count': len(normalized_laws),
                'reduction_rate': ((len(all_law_names) - len(normalized_laws)) / len(all_law_names) * 100) if all_law_names else 0
            }

        else:
            st.warning("ì¶”ì¶œëœ ë²•ë ¹ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {'normalized_laws': [], 'law_details': [], 'original_count': 0, 'normalized_count': 0, 'reduction_rate': 0}

    except Exception as e:
        st.error(f"ë²•ë ¹ëª… ì¶”ì¶œ/ì •ê·œí™” ì˜¤ë¥˜: {e}")
        return {'normalized_laws': [], 'law_details': [], 'error': str(e)}

def search_theoretical_background(problem_keywords: List[str], pkl_paths: List[str], max_results: int = 8, context_analysis: Dict = None) -> List[Dict]:
    """ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•œ ì´ë¡ ì  ë°°ê²½ì„ PKLì—ì„œ ê²€ìƒ‰"""
    if not problem_keywords:
        return []
    
    try:
        # ì—¬ëŸ¬ ëª¨ë¸ì„ ì‹œë„í•´ë³´ê¸°
        models_to_try = [
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'sentence-transformers/all-MiniLM-L6-v2', 
            'sentence-transformers/all-mpnet-base-v2'
        ]
        
        model = None
        for model_name in models_to_try:
            try:
                model = SentenceTransformer(model_name)
                st.write(f"[DEBUG] ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                break
            except Exception as e:
                st.write(f"[DEBUG] ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                continue
        
        if model is None:
            st.error("ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        theoretical_results = []

        # ğŸ” ë¬¸ë§¥ ê¸°ë°˜ ë™ì  ì¿¼ë¦¬ ìƒì„±
        all_search_queries = []

        if context_analysis:
            st.write(f"[DEBUG] ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼ í™œìš©: {len(context_analysis.get('key_concepts', []))}ê°œ ê°œë…")

            # 1. ì¶”ì¶œëœ í•µì‹¬ ê°œë… ê¸°ë°˜ ì¿¼ë¦¬
            for concept_info in context_analysis.get('key_concepts', []):
                concept = concept_info['concept']
                context = concept_info['context']

                # ë¬¸ë§¥ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
                context_keywords = []
                if 'í—ˆê°€' in context: context_keywords.append('í—ˆê°€ê¶Œí•œ')
                if 'ìŠ¹ì¸' in context: context_keywords.append('ìŠ¹ì¸ê¶Œí•œ')
                if 'ì²˜ë¶„' in context: context_keywords.append('í–‰ì •ì²˜ë¶„')
                if 'ìœ„ì„' in context: context_keywords.append('ê¶Œí•œìœ„ì„')

                # ê°œë…ë³„ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±
                all_search_queries.extend([
                    concept,
                    f"{concept} ì¡°ë¡€ ìœ„ë²•",
                    f"{concept} íŒë¡€",
                    f"{concept} í•œê³„"
                ])
                all_search_queries.extend(context_keywords)

            # 2. ë²•ì  ê·¼ê±° ê¸°ë°˜ ì¿¼ë¦¬
            for legal_basis in context_analysis.get('legal_basis', []):
                all_search_queries.extend([
                    legal_basis,
                    f"{legal_basis} ìœ„ë°˜",
                    f"{legal_basis} ì¡°ë¡€"
                ])

            # 3. ë¬¸ì œì  ìƒì„¸ ë‚´ìš© ê¸°ë°˜ ì¿¼ë¦¬ (í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ)
            for problem in context_analysis.get('problem_details', []):
                # ë¬¸ì œì ì—ì„œ í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ
                problem_keywords_extracted = []
                import re
                # í•œê¸€ ëª…ì‚¬ íŒ¨í„´ ì¶”ì¶œ (ë§¤ìš° ë‹¨ìˆœí•œ ë°©ì‹)
                nouns = re.findall(r'[ê°€-í£]{2,}(?:ì‚¬ë¬´|ê¶Œí•œ|ë²•ë ¹|ì¡°ë¡€|ìœ„ë°˜|ìœ„ë²•|í—ˆê°€|ìŠ¹ì¸|ì²˜ë¶„)', problem)
                problem_keywords_extracted.extend(nouns[:3])  # ìƒìœ„ 3ê°œë§Œ

                all_search_queries.extend([f"{noun} íŒë¡€" for noun in problem_keywords_extracted])

        # ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ë„ í¬í•¨
        for keyword in problem_keywords:
            # í‚¤ì›Œë“œë³„ íŠ¹í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            if keyword == "ê¸°ê´€ìœ„ì„ì‚¬ë¬´":
                all_search_queries.extend([
                    "ê¸°ê´€ìœ„ì„ì‚¬ë¬´", "ì¡°ë¡€ ì œì • ê¸ˆì§€", "ì§€ë°©ìì¹˜ë²• ì œ22ì¡°",
                    "êµ­ê°€ì‚¬ë¬´ ìœ„ì„", "ì‹œì¥ êµ°ìˆ˜ êµ¬ì²­ì¥", "ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€", "ê¸°ê´€ìœ„ì„ ê¸ˆì§€"
                ])
            elif keyword == "ìƒìœ„ë²•ë ¹":
                all_search_queries.extend([
                    "ìƒìœ„ë²•ë ¹", "ë²•ë ¹ìš°ìœ„", "ì¡°ë¡€ ë¬´íš¨", "ë²•ë ¹ ì¶©ëŒ", "ìƒìœ„ë²• ìœ„ë°˜", "ì¡°ë¡€ ìœ„ë²•"
                ])
            elif keyword == "ê¶Œí•œ":
                all_search_queries.extend([
                    "ê¶Œí•œ ìœ„ì„", "ê¶Œí•œë°°ë¶„", "ë²•ë¥ ìœ ë³´", "ì¡°ë¡€ ê¶Œí•œ", "ì§€ë°©ìì¹˜ë‹¨ì²´ ê¶Œí•œ"
                ])
            elif keyword == "ìœ„ë²•":
                all_search_queries.extend([
                    "ì¡°ë¡€ ìœ„ë²•", "ë¬´íš¨ ì¡°ë¡€", "ë²•ë ¹ ìœ„ë°˜", "ì¡°ë¡€ ìœ„ë°˜"
                ])
            elif keyword == "í—Œë²•ìœ„ë°˜":
                all_search_queries.extend([
                    "í—Œë²•ìœ„ë°˜", "ê¸°ë³¸ê¶Œì¹¨í•´", "í—Œë²•ì¬íŒì†Œ", "ìœ„í—Œì¡°ë¡€", "í—Œë²•ì  í•œê³„"
                ])
            elif keyword == "ê¸°ë³¸ê¶Œ":
                all_search_queries.extend([
                    "ê¸°ë³¸ê¶Œì¹¨í•´", "ì¬ì‚°ê¶Œ", "ì˜ì—…ì˜ììœ ", "í‰ë“±ê¶Œ", "ê¸°ë³¸ê¶Œ ì œí•œ"
                ])
            elif keyword in ["í‰ë“±ì›ì¹™", "ë¹„ë¡€ì›ì¹™"]:
                all_search_queries.extend([
                    f"{keyword}", f"{keyword} ìœ„ë°˜", "í—Œë²•ì›ì¹™", "ì¡°ë¡€ í•œê³„"
                ])
            elif keyword in ["ì¡°ì„¸", "ë²Œê¸ˆ", "ê³¼íƒœë£Œ"]:
                all_search_queries.extend([
                    "ì¡°ì„¸ë²•ë¥ ì£¼ì˜", "ë²Œê¸ˆ ë¶€ê³¼", "ê³¼íƒœë£Œ", "ë²•ë¥ ìœ ë³´", "ì¡°ë¡€ ë²Œì¹™"
                ])
            else:
                all_search_queries.extend([keyword, f"{keyword} ì¡°ë¡€", f"{keyword} ìœ„ë²•", f"{keyword} íŒë¡€"])

        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_queries = list(dict.fromkeys(all_search_queries))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
        st.write(f"[DEBUG] ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {len(unique_queries)}ê°œ")
        st.write(f"[DEBUG] ìƒìœ„ 5ê°œ ì¿¼ë¦¬: {unique_queries[:5]}")

        # PKL íŒŒì¼ë³„ ê²€ìƒ‰ (ë™ì  ì¿¼ë¦¬ ì‚¬ìš©)
        for pkl_path in pkl_paths:
                if not os.path.exists(pkl_path):
                    continue
                
                vectorstore = load_vectorstore_safe(pkl_path)
                if not vectorstore:
                    continue
                
                try:
                    # PKL íŒŒì¼ êµ¬ì¡° ìƒì„¸ í™•ì¸
                    st.write(f"[DEBUG] {pkl_path} êµ¬ì¡° í™•ì¸:")
                    st.write(f"[DEBUG]   - keys: {list(vectorstore.keys())}")
                    
                    embeddings = vectorstore.get('embeddings', np.array([]))
                    # PKL íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° documents ì‚¬ìš©
                    chunks = vectorstore.get('chunks', [])
                    if len(chunks) == 0:
                        # chunksê°€ ì—†ìœ¼ë©´ documents ì‚¬ìš©
                        documents = vectorstore.get('documents', [])
                        if documents:
                            # documentsë¥¼ chunks í˜•íƒœë¡œ ë³€í™˜
                            chunks = [{'text': doc} for doc in documents]
                            st.write(f"[DEBUG] documentsë¥¼ chunksë¡œ ë³€í™˜: {len(chunks)}ê°œ")
                    
                    st.write(f"[DEBUG]   - embeddings type: {type(embeddings)}, shape: {embeddings.shape if hasattr(embeddings, 'shape') else 'N/A'}")
                    st.write(f"[DEBUG]   - chunks type: {type(chunks)}, length: {len(chunks)}")
                    
                    if len(embeddings) == 0 or len(chunks) == 0:
                        st.write(f"[DEBUG] {pkl_path} - embeddings: {len(embeddings)}, chunks: {len(chunks)} (ê±´ë„ˆëœ€)")
                        continue
                    
                    # ì²« ë²ˆì§¸ chunk ë‚´ìš© í™•ì¸
                    if len(chunks) > 0:
                        first_chunk = chunks[0]
                        st.write(f"[DEBUG]   - ì²« ë²ˆì§¸ chunk êµ¬ì¡°: {type(first_chunk)}")
                        if isinstance(first_chunk, dict):
                            st.write(f"[DEBUG]   - chunk keys: {list(first_chunk.keys())}")
                            text_content = first_chunk.get('text', first_chunk.get('content', ''))[:100]
                            st.write(f"[DEBUG]   - ì²« 100ì: {text_content}")
                        else:
                            st.write(f"[DEBUG]   - chunk ë‚´ìš©: {str(first_chunk)[:100]}")
                    
                    # embeddingsì™€ chunks ê¸¸ì´ ì¼ì¹˜ í™•ì¸
                    if len(embeddings) != len(chunks):
                        st.write(f"[DEBUG] {pkl_path} - embeddings({len(embeddings)})ì™€ chunks({len(chunks)}) ê¸¸ì´ ë¶ˆì¼ì¹˜")
                        min_length = min(len(embeddings), len(chunks))
                        embeddings = embeddings[:min_length]
                        chunks = chunks[:min_length]
                        st.write(f"[DEBUG] {pkl_path} - {min_length}ê°œë¡œ ì¡°ì •")
                    
                    all_similarities = []
                    keyword_matches = []  # í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ë„ ì €ì¥
                    
                    # 1ì°¨: ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ (ë™ì  ì¿¼ë¦¬ ì‚¬ìš©)
                    for query in unique_queries[:15]:  # ìƒìœ„ 15ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš© (ì„±ëŠ¥ ê³ ë ¤)
                        try:
                            query_embedding = model.encode([query])
                            similarities = np.dot(query_embedding, embeddings.T).flatten()
                            all_similarities.extend([(i, sim) for i, sim in enumerate(similarities)])
                        except Exception as e:
                            st.write(f"[DEBUG] ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {str(e)}")

                    # 2ì°¨: ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ (ë°±ì—…) - ë™ì  ì¿¼ë¦¬ ì‚¬ìš©
                    for i, chunk in enumerate(chunks):
                        chunk_text = ""
                        if isinstance(chunk, dict):
                            chunk_text = chunk.get('text', chunk.get('content', ''))
                        else:
                            chunk_text = str(chunk)

                        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (í•µì‹¬ ì¿¼ë¦¬ë“¤ë¡œë§Œ)
                        keyword_score = 0
                        matched_queries = []
                        for query in unique_queries[:10]:  # ìƒìœ„ 10ê°œë§Œ
                            if query.lower() in chunk_text.lower():
                                keyword_score += 1
                                matched_queries.append(query)

                        if keyword_score > 0:
                            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ìœ ì‚¬ë„ì²˜ëŸ¼ ì‚¬ìš© (0.5 + ë§¤ì¹­ìˆ˜ * 0.1)
                            match_similarity = 0.5 + keyword_score * 0.1
                            keyword_matches.append((i, match_similarity))
                            st.write(f"[DEBUG] í‚¤ì›Œë“œ ë§¤ì¹­ ë°œê²¬: {matched_queries} - {keyword_score}ê°œ ë§¤ì¹­, ì ìˆ˜ {match_similarity:.3f}")
                    
                    # ì„ë² ë”© ê²°ê³¼ì™€ í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ ê²°í•©
                    all_similarities.extend(keyword_matches)
                    
                    # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœê³  ì ìˆ˜ë¡œ ì •ë ¬
                    idx_scores = {}
                    for idx, score in all_similarities:
                        if idx not in idx_scores or score > idx_scores[idx]:
                            idx_scores[idx] = score
                    
                    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
                    top_items = sorted(idx_scores.items(), key=lambda x: x[1], reverse=True)[:3]
                    
                    st.write(f"[DEBUG] {pkl_path} - ê²€ìƒ‰ ê²°ê³¼: {len(top_items)}ê°œ, chunks ê¸¸ì´: {len(chunks)}")
                    
                    for idx, similarity in top_items:
                        # ì¸ë±ìŠ¤ ë²”ìœ„ ì•ˆì „ ê²€ì‚¬
                        if idx >= len(chunks):
                            st.write(f"[DEBUG] ì¸ë±ìŠ¤ {idx}ê°€ chunks ê¸¸ì´ {len(chunks)}ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                            continue
                        
                        if similarity > 0.1:  # ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë§ì€ ê²°ê³¼ í¬í•¨
                            chunk = chunks[idx]
                            chunk_text = chunk.get('text', '')
                            
                            # ğŸ” ë¬¸ë§¥ ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€ (ë™ì )
                            relevance_score = 0
                            matched_concepts = []

                            # 1. ê¸°ë³¸ ë²•ì  ì§€í‘œ
                            theory_indicators = ['ì›ì¹™', 'íŒë¡€', 'í—Œë²•ì¬íŒì†Œ', 'ëŒ€ë²•ì›', 'ì´ë¡ ', 'í•™ì„¤', 'ë²•ë¦¬', 'ì¡°ë¡€', 'ìœ„ë²•', 'ë¬´íš¨', 'ë²•ë ¹', 'ìœ„ë°˜']
                            base_score = sum(1 for indicator in theory_indicators if indicator in chunk_text)
                            relevance_score += base_score

                            # 2. Gemini ë¶„ì„ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ê°œë…ê³¼ì˜ ë§¤ì¹­
                            if context_analysis:
                                for concept_info in context_analysis.get('key_concepts', []):
                                    concept = concept_info['concept']
                                    if concept in chunk_text:
                                        relevance_score += 2  # í•µì‹¬ ê°œë… ë§¤ì¹­ ì‹œ ë†’ì€ ì ìˆ˜
                                        matched_concepts.append(concept)

                                # 3. ë²•ì  ê·¼ê±°ì™€ì˜ ë§¤ì¹­
                                for legal_basis in context_analysis.get('legal_basis', []):
                                    if legal_basis in chunk_text:
                                        relevance_score += 3  # ë²•ì  ê·¼ê±° ë§¤ì¹­ ì‹œ ë” ë†’ì€ ì ìˆ˜
                                        matched_concepts.append(legal_basis)

                                # 4. ë¬¸ì œì  í‚¤ì›Œë“œì™€ì˜ ë§¤ì¹­
                                for problem in context_analysis.get('problem_details', []):
                                    # ë¬¸ì œì ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œí•˜ì—¬ ë§¤ì¹­
                                    problem_words = problem.split()[:5]  # ì²« 5ê°œ ë‹¨ì–´
                                    for word in problem_words:
                                        if len(word) > 1 and word in chunk_text:
                                            relevance_score += 1

                            # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ (ë™ì ìœ¼ë¡œ ì¡°ì •)
                            min_relevance = 2 if context_analysis else 1

                            if relevance_score >= min_relevance:
                                theoretical_results.append({
                                    'topic': ', '.join(problem_keywords) if len(problem_keywords) > 1 else problem_keywords[0],
                                    'content': chunk_text,
                                    'relevance_score': float(similarity),
                                    'context_relevance': relevance_score,  # ë¬¸ë§¥ ê´€ë ¨ì„± ì ìˆ˜
                                    'matched_concepts': matched_concepts,   # ë§¤ì¹­ëœ ê°œë…ë“¤
                                    'source': pkl_path,
                                    'query_used': unique_queries[0] if unique_queries else "ê¸°ë³¸ê²€ìƒ‰"
                                })
                                st.write(f"[DEBUG] âœ… ë°œê²¬: ìœ ì‚¬ë„ {similarity:.3f}, ë¬¸ë§¥ê´€ë ¨ì„± {relevance_score}, ë§¤ì¹­ê°œë…: {matched_concepts}")
                            else:
                                st.write(f"[DEBUG] âŒ ì œì™¸: ìœ ì‚¬ë„ {similarity:.3f}, ë¬¸ë§¥ê´€ë ¨ì„± {relevance_score} (ê¸°ì¤€: {min_relevance})")
                
                except Exception as e:
                    st.error(f"ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ì˜¤ë¥˜ ({pkl_path}): {str(e)}")
                    continue
        
        # ë¬¸ë§¥ ê´€ë ¨ì„±ê³¼ ìœ ì‚¬ë„ë¥¼ ì¢…í•©í•˜ì—¬ ì •ë ¬ (ë¬¸ë§¥ ê´€ë ¨ì„±ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        theoretical_results.sort(key=lambda x: (
            x.get('context_relevance', 0) * 0.7 + x['relevance_score'] * 0.3
        ), reverse=True)
        
        # ì¤‘ë³µ ì œê±° (ìœ ì‚¬í•œ ë‚´ìš© í•„í„°ë§)
        filtered_results = []
        seen_content = set()
        
        for result in theoretical_results:
            content_hash = result['content'][:100]  # ì²« 100ìë¡œ ì¤‘ë³µ íŒë³„
            if content_hash not in seen_content:
                filtered_results.append(result)
                seen_content.add(content_hash)
                
                if len(filtered_results) >= max_results:
                    break
        
        st.write(f"[DEBUG] ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼")
        return filtered_results
        
    except Exception as e:
        st.error(f"ì´ë¡ ì  ë°°ê²½ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def apply_violation_cases_to_ordinance(violation_cases: List[Dict], ordinance_text: str, pkl_paths: List[str]) -> List[Dict]:
    """ìœ„ë²• íŒë¡€ë¥¼ ì¡°ë¡€ì•ˆì— ì§ì ‘ ì ìš©"""
    if not violation_cases:
        return []
    
    try:
        st.write(f"[DEBUG] ìœ„ë²• íŒë¡€ ì ìš© ì‹œì‘: {len(violation_cases)}ê°œ íŒë¡€")
        
        # ì¡°ë¡€ ì¡°ë¬¸ ì¶”ì¶œ
        ordinance_articles = extract_ordinance_articles(ordinance_text)
        st.write(f"[DEBUG] ì¡°ë¡€ ì¡°ë¬¸ ì¶”ì¶œ: {len(ordinance_articles)}ê°œ")
        
        if not ordinance_articles:
            return []
        
        # ì¢…í•© ìœ„ë²•ì„± ê²€ìƒ‰ ìˆ˜í–‰
        comprehensive_results = search_comprehensive_violation_cases(ordinance_articles, pkl_paths)
        
        return comprehensive_results
        
    except Exception as e:
        st.error(f"ìœ„ë²• íŒë¡€ ì ìš© ì˜¤ë¥˜: {str(e)}")
        return []

def format_comprehensive_analysis_result(results: List[Dict]) -> str:
    """ì¢…í•© ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…"""
    if not results:
        return "ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ì—ì„œ íŠ¹ë³„í•œ ìœ„í—˜ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    formatted_result = "ğŸš¨ **ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼**\n\n"
    
    total_risks = sum(len(result['violation_risks']) for result in results)
    formatted_result += f"**ì´ {len(results)}ê°œ ì¡°ë¬¸ì—ì„œ {total_risks}ê°œì˜ ìœ„ë²• ìœ„í—˜ ë°œê²¬**\n\n"
    
    for result in results:
        formatted_result += f"### {result['ordinance_article']} {result.get('ordinance_title', '')}\n"
        formatted_result += f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:150]}...\n\n"
        
        for i, risk in enumerate(result['violation_risks'][:3], 1):
            formatted_result += f"**ìœ„í—˜ {i}: {risk['violation_type']}** (ìœ„í—˜ë„: {risk['risk_score']:.2f})\n"
            formatted_result += f"- ê´€ë ¨ íŒë¡€: {risk['case_summary']}\n"
            formatted_result += f"- ê°œì„  ê¶Œê³ : {risk['recommendation']}\n"
            formatted_result += f"- íŒë¡€ ì¶œì²˜: {risk['case_source']}\n\n"
        
        if len(result['violation_risks']) > 3:
            formatted_result += f"*(ì™¸ {len(result['violation_risks']) - 3}ê°œ ì¶”ê°€ ìœ„í—˜)*\n\n"
        
        formatted_result += "---\n\n"
    
    return formatted_result

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_comprehensive_analysis():
    """ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    sample_ordinance = """
    ì œ1ì¡°(ëª©ì ) ì´ ì¡°ë¡€ëŠ” ì£¼ì°¨ì¥ì˜ ì„¤ì¹˜ ë° ê´€ë¦¬ì— ê´€í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
    
    ì œ2ì¡°(ì •ì˜) ì´ ì¡°ë¡€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´ì˜ ëœ»ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. "ì£¼ì°¨ì¥"ì´ë€ ìë™ì°¨ë¥¼ ì£¼ì°¨ì‹œí‚¤ê¸° ìœ„í•œ ì‹œì„¤ì„ ë§í•œë‹¤.
    
    ì œ3ì¡°(ì£¼ì°¨ì¥ ì„¤ì¹˜ ê¸°ì¤€) ì£¼ì°¨ì¥ì˜ ì„¤ì¹˜ ê¸°ì¤€ì€ ì‹œì¥ì´ ì •í•œë‹¤.
    """
    
    pkl_paths = [
        'enhanced_vectorstore_20250914_101739.pkl'
    ]
    
    # ì¡°ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    articles = extract_ordinance_articles(sample_ordinance)
    print(f"ì¶”ì¶œëœ ì¡°ë¬¸ ìˆ˜: {len(articles)}")
    
    # ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸
    if articles:
        results = search_comprehensive_violation_cases(articles, pkl_paths)
        print(f"ë¶„ì„ ê²°ê³¼: {len(results)}ê°œ ì¡°ë¬¸ì—ì„œ ìœ„í—˜ ë°œê²¬")
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = format_comprehensive_analysis_result(results)
        print(formatted[:500] + "...")

def create_optimized_analysis_payload(comprehensive_results: List[Dict], law_analysis: Dict, theoretical_background: List[Dict] = None) -> Dict[str, Any]:
    """Gemini API í˜¸ì¶œì„ ìœ„í•œ ìµœì í™”ëœ ë¶„ì„ í˜ì´ë¡œë“œ ìƒì„±"""

    if not comprehensive_results:
        return {'error': 'ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'}

    try:
        st.write("### ğŸš€ Gemini API í˜¸ì¶œ ìµœì í™”")

        # 1. í•µì‹¬ ìœ„í—˜ ì‚¬ë¡€ë§Œ ì„ ë³„ (ìƒìœ„ ìœ„í—˜ë„)
        high_risk_cases = []
        for result in comprehensive_results:
            for risk in result.get('violation_risks', []):
                if risk.get('risk_score', 0) >= 0.6:  # ìœ„í—˜ë„ 0.6 ì´ìƒë§Œ
                    high_risk_cases.append({
                        'article': result.get('ordinance_article', ''),
                        'risk_type': risk.get('violation_type', ''),
                        'risk_score': risk.get('risk_score', 0),
                        'case_summary': risk.get('case_summary', '')[:500],  # 500ìë¡œ ì œí•œ
                        'similarity': risk.get('similarity', 0)
                    })

        # ìœ„í—˜ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 10ê°œë§Œ ì„ íƒ
        high_risk_cases.sort(key=lambda x: x['risk_score'], reverse=True)
        selected_cases = high_risk_cases[:10]

        st.write(f"**âœ… ê³ ìœ„í—˜ ì‚¬ë¡€ ì„ ë³„**: {len(high_risk_cases)}ê°œ ì¤‘ {len(selected_cases)}ê°œ ì„ íƒ")

        # 2. ê´€ë ¨ ë²•ë ¹ ìš”ì•½ (ì •ê·œí™”ëœ ë²•ë ¹ë§Œ)
        relevant_laws_summary = []
        if law_analysis.get('law_details'):
            # ìƒìœ„ 5ê°œ ë²•ë ¹ë§Œ ì„ íƒ
            top_laws = sorted(
                law_analysis['law_details'],
                key=lambda x: x.get('similarity_score', 0),
                reverse=True
            )[:5]

            for law_detail in top_laws:
                relevant_laws_summary.append({
                    'law_name': law_detail['law_name'],
                    'law_type': law_detail.get('law_type', ''),
                    'related_articles_count': len(law_detail.get('related_articles', []))
                })

        st.write(f"**âœ… ê´€ë ¨ ë²•ë ¹ ìš”ì•½**: {len(relevant_laws_summary)}ê°œ ë²•ë ¹")

        # 3. ì´ë¡ ì  ë°°ê²½ ìš”ì•½ (ìˆëŠ” ê²½ìš°)
        theory_summary = []
        if theoretical_background:
            for theory in theoretical_background[:5]:  # ìƒìœ„ 5ê°œë§Œ
                theory_summary.append({
                    'principle': theory.get('legal_principle', '')[:200],  # 200ìë¡œ ì œí•œ
                    'relevance': theory.get('relevance_score', 0)
                })

        # 4. í†µí•© í˜ì´ë¡œë“œ ìƒì„±
        optimized_payload = {
            'analysis_summary': {
                'total_articles_analyzed': len(comprehensive_results),
                'high_risk_cases_count': len(selected_cases),
                'relevant_laws_count': len(relevant_laws_summary),
                'law_normalization_effect': {
                    'original_count': law_analysis.get('original_count', 0),
                    'normalized_count': law_analysis.get('normalized_count', 0),
                    'reduction_rate': law_analysis.get('reduction_rate', 0)
                }
            },
            'high_risk_violations': selected_cases,
            'relevant_laws': relevant_laws_summary,
            'theoretical_background': theory_summary,
            'optimization_metrics': {
                'total_text_length': sum(len(case.get('case_summary', '')) for case in selected_cases),
                'api_calls_saved': len(high_risk_cases) - len(selected_cases),
                'token_efficiency': f"ì•½ {(len(high_risk_cases) - len(selected_cases)) * 1000} í† í° ì ˆì•½"
            }
        }

        # 5. í˜ì´ë¡œë“œ í¬ê¸° ì²´í¬
        import json
        payload_size = len(json.dumps(optimized_payload, ensure_ascii=False))

        st.write(f"**ğŸ“Š ìµœì í™” íš¨ê³¼**:")
        st.write(f"  - ì„ ë³„ëœ ì‚¬ë¡€: {len(selected_cases)}ê°œ (ì „ì²´ {len(high_risk_cases)}ê°œ ì¤‘)")
        st.write(f"  - í˜ì´ë¡œë“œ í¬ê¸°: {payload_size:,} bytes")
        st.write(f"  - ì˜ˆìƒ API í˜¸ì¶œ ì ˆì•½: {len(high_risk_cases) - len(selected_cases)}íšŒ")

        if law_analysis.get('reduction_rate', 0) > 0:
            st.success(f"  - ë²•ë ¹ëª… ì¤‘ë³µ ì œê±°: {law_analysis['reduction_rate']:.1f}% ì ˆì•½")

        return optimized_payload

    except Exception as e:
        st.error(f"í˜ì´ë¡œë“œ ìµœì í™” ì˜¤ë¥˜: {e}")
        return {'error': str(e)}

def format_optimized_prompt_for_gemini(payload: Dict[str, Any]) -> str:
    """ìµœì í™”ëœ Gemini í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    if payload.get('error'):
        return f"ë¶„ì„ ì˜¤ë¥˜: {payload['error']}"

    try:
        prompt_parts = []

        # 1. ë¶„ì„ ê°œìš”
        summary = payload.get('analysis_summary', {})
        prompt_parts.append(f"""
## ì¡°ë¡€ ìœ„ë²•ì„± ì¢…í•© ë¶„ì„ ê²°ê³¼

**ë¶„ì„ ê°œìš”:**
- ë¶„ì„ ëŒ€ìƒ ì¡°ë¬¸: {summary.get('total_articles_analyzed', 0)}ê°œ
- ê³ ìœ„í—˜ ì‚¬ë¡€: {summary.get('high_risk_cases_count', 0)}ê°œ
- ê´€ë ¨ ë²•ë ¹: {summary.get('relevant_laws_count', 0)}ê°œ

**ìµœì í™” íš¨ê³¼:**
- ë²•ë ¹ëª… ì •ê·œí™”: {summary.get('law_normalization_effect', {}).get('original_count', 0)}ê°œ â†’ {summary.get('law_normalization_effect', {}).get('normalized_count', 0)}ê°œ
- ì¤‘ë³µ ì œê±°ìœ¨: {summary.get('law_normalization_effect', {}).get('reduction_rate', 0):.1f}%
""")

        # 2. ê³ ìœ„í—˜ ìœ„ë²• ì‚¬ë¡€
        high_risk_cases = payload.get('high_risk_violations', [])
        if high_risk_cases:
            prompt_parts.append("\n## ğŸš¨ ì£¼ìš” ìœ„ë²• ìœ„í—˜ ì‚¬ë¡€\n")
            for i, case in enumerate(high_risk_cases, 1):
                prompt_parts.append(f"""
**{i}. {case.get('article', '')} - {case.get('risk_type', '')}**
- ìœ„í—˜ë„: {case.get('risk_score', 0):.2f}
- ìœ ì‚¬ë„: {case.get('similarity', 0):.2f}
- ì‚¬ë¡€ ìš”ì•½: {case.get('case_summary', '')[:300]}...
""")

        # 3. ê´€ë ¨ ë²•ë ¹
        relevant_laws = payload.get('relevant_laws', [])
        if relevant_laws:
            prompt_parts.append("\n## ğŸ“‹ ê´€ë ¨ ì£¼ìš” ë²•ë ¹\n")
            for i, law in enumerate(relevant_laws, 1):
                prompt_parts.append(f"""
**{i}. {law.get('law_name', '')}**
- ë²•ë ¹ ìœ í˜•: {law.get('law_type', '')}
- ê´€ë ¨ ì¡°ë¬¸ ìˆ˜: {law.get('related_articles_count', 0)}ê°œ
""")

        # 4. ë¶„ì„ ìš”ì²­
        prompt_parts.append("""
## ğŸ“ ë¶„ì„ ìš”ì²­

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‚¬í•­ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ ì£¼ì„¸ìš”:

1. **ìœ„ë²•ì„± í‰ê°€**: ê° ì¡°ë¬¸ë³„ ìœ„ë²• ìœ„í—˜ë„ì™€ ê·¼ê±°
2. **ë²•ë ¹ ì¶©ëŒ ë¶„ì„**: ìƒìœ„ë²•ë ¹ê³¼ì˜ ì¶©ëŒ ê°€ëŠ¥ì„±
3. **ê°œì„  ë°©ì•ˆ**: êµ¬ì²´ì ì¸ ì¡°ë¡€ ê°œì„  ê¶Œê³ ì‚¬í•­
4. **ë²•ì  ê·¼ê±°**: ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€ ì¸ìš©
5. **ìš°ì„ ìˆœìœ„**: ìˆ˜ì •ì´ í•„ìš”í•œ ì¡°ë¬¸ì˜ ìš°ì„ ìˆœìœ„

**ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­:**
- ë²•ë ¹ ê°„ ìœ„ê³„ ê´€ê³„
- ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ìì¹˜ê¶Œ ë²”ìœ„
- ì‹¤ë¬´ì  ì ìš© ê°€ëŠ¥ì„±
""")

        final_prompt = "".join(prompt_parts)

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì²´í¬
        st.write(f"**ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸**: {len(final_prompt):,}ì")

        return final_prompt

    except Exception as e:
        return f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}"

def analyze_comprehensive_violations_optimized(ordinance_text: str, pkl_paths: List[str]) -> Dict[str, Any]:
    """ìµœì í™”ëœ ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ (ì „ì²´ í”„ë¡œì„¸ìŠ¤)"""

    st.write("# ğŸ” ìµœì í™”ëœ ì¢…í•© ìœ„ë²•ì„± ë¶„ì„")

    try:
        # 1. ì¡°ë¬¸ ì¶”ì¶œ
        st.write("## 1ë‹¨ê³„: ì¡°ë¬¸ ì¶”ì¶œ")
        articles = extract_ordinance_articles(ordinance_text)
        st.write(f"âœ… {len(articles)}ê°œ ì¡°ë¬¸ ì¶”ì¶œ")

        if not articles:
            return {'error': 'ì¡°ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}

        # 2. ìœ„ë²• ì‚¬ë¡€ ê²€ìƒ‰
        st.write("## 2ë‹¨ê³„: ìœ„ë²• ì‚¬ë¡€ ê²€ìƒ‰")
        comprehensive_results = search_comprehensive_violation_cases(articles, pkl_paths)
        st.write(f"âœ… {len(comprehensive_results)}ê°œ ì¡°ë¬¸ì—ì„œ ìœ„í—˜ ë°œê²¬")

        if not comprehensive_results:
            return {'error': 'ìœ„ë²• ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}

        # 3. ë²•ë ¹ëª… ì¶”ì¶œ ë° ì •ê·œí™”
        st.write("## 3ë‹¨ê³„: ë²•ë ¹ëª… ì •ê·œí™”")
        law_analysis = extract_and_normalize_relevant_laws(comprehensive_results)

        # 4. ìµœì í™”ëœ í˜ì´ë¡œë“œ ìƒì„±
        st.write("## 4ë‹¨ê³„: Gemini API ìµœì í™”")
        optimized_payload = create_optimized_analysis_payload(comprehensive_results, law_analysis)

        # 5. Gemini í”„ë¡¬í”„íŠ¸ ìƒì„±
        gemini_prompt = format_optimized_prompt_for_gemini(optimized_payload)

        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        return {
            'success': True,
            'articles_count': len(articles),
            'violations_found': len(comprehensive_results),
            'law_normalization': law_analysis,
            'optimized_payload': optimized_payload,
            'gemini_prompt': gemini_prompt,
            'optimization_summary': {
                'original_violations': sum(len(r.get('violation_risks', [])) for r in comprehensive_results),
                'selected_violations': len(optimized_payload.get('high_risk_violations', [])),
                'laws_normalized': law_analysis.get('normalized_count', 0),
                'laws_original': law_analysis.get('original_count', 0),
                'reduction_rate': law_analysis.get('reduction_rate', 0)
            }
        }

    except Exception as e:
        st.error(f"ìµœì í™”ëœ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {'error': str(e)}

if __name__ == "__main__":
    test_comprehensive_analysis()