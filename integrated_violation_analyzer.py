"""
í†µí•© ìœ„ë²•ì„± ë¶„ì„ê¸°
ë²•ë ¹ëª… ì¤‘ë³µ ì œê±° + Gemini API ìµœì í™”ê°€ í†µí•©ëœ ì™„ì „í•œ ì†”ë£¨ì…˜
"""

import streamlit as st
from law_name_deduplicator import SimpleLawNameDeduplicator
import re
from typing import List, Dict, Any
import json

class IntegratedViolationAnalyzer:
    def __init__(self):
        self.deduplicator = SimpleLawNameDeduplicator()

    def extract_laws_from_violations(self, violation_results: List[Dict]) -> List[str]:
        """ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        law_names = []

        for result in violation_results:
            # ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = self._extract_text_from_result(result)

            # ë²•ë ¹ëª… íŒ¨í„´ ì°¾ê¸°
            extracted_laws = self._find_law_patterns(text_content)
            law_names.extend(extracted_laws)

        return list(set(law_names))  # ì¤‘ë³µ ì œê±°

    def _extract_text_from_result(self, result: Dict) -> str:
        """ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_parts = []

        if isinstance(result, dict):
            for key, value in result.items():
                if isinstance(value, str):
                    text_parts.append(value)
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, str):
                            text_parts.append(item)
                        elif isinstance(item, dict):
                            text_parts.append(self._extract_text_from_result(item))

        return " ".join(text_parts)

    def _find_law_patterns(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë²•ë ¹ëª… íŒ¨í„´ ì°¾ê¸°"""
        patterns = [
            r'([ê°€-í£\s]*?ë²•ë¥ ?)[.\s,)]',
            r'([ê°€-í£\s]*?ì‹œí–‰ë ¹)[.\s,)]',
            r'([ê°€-í£\s]*?ì‹œí–‰ê·œì¹™)[.\s,)]',
            r'([ê°€-í£\s]*?ì¡°ë¡€)[.\s,)]',
            r'([ê°€-í£\s]*?ê·œì •)[.\s,)]',
            r'(ì§€ë°©ìì¹˜ë²•)',
            r'(ê³µê³µê¸°ê´€ì˜\s*ìš´ì˜ì—\s*ê´€í•œ\s*ë²•ë¥ ?)',
            r'(í–‰ì •ì ˆì°¨ë²•)',
            r'(êµ­ê°€ì¬ì •ë²•)',
            r'(í—Œë²•)',
        ]

        found_laws = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                clean_name = re.sub(r'\s+', ' ', match).strip()
                if len(clean_name) >= 3 and clean_name not in found_laws:
                    found_laws.append(clean_name)

        return found_laws

    def process_with_deduplication(self, violation_results: List[Dict]) -> Dict[str, Any]:
        """ìœ„ë²•ì„± ê²°ê³¼ë¥¼ ì¤‘ë³µ ì œê±°í•˜ì—¬ ì²˜ë¦¬"""

        st.write("### ğŸ”§ ë²•ë ¹ëª… ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ì¤‘...")

        # 1. ë²•ë ¹ëª… ì¶”ì¶œ
        extracted_laws = self.extract_laws_from_violations(violation_results)
        st.write(f"ğŸ“Š ì¶”ì¶œëœ ë²•ë ¹ëª…: {len(extracted_laws)}ê°œ")

        if not extracted_laws:
            st.warning("ì¶”ì¶œëœ ë²•ë ¹ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {'error': 'ë²•ë ¹ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}

        # 2. ì¤‘ë³µ ë¶„ì„ ë° ì œê±°
        analysis = self.deduplicator.analyze_duplications(extracted_laws)

        st.write(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {analysis['original_count']}ê°œ â†’ {analysis['deduplicated_count']}ê°œ")

        if analysis['reduction_count'] > 0:
            st.success(f"ğŸ¯ {analysis['reduction_count']}ê°œ ì¤‘ë³µ ë²•ë ¹ ì œê±° ({analysis['reduction_rate']:.1f}% ì ˆì•½)")

        # 3. ì¤‘ë³µ ê·¸ë£¹ ì •ë³´ í‘œì‹œ
        if analysis['duplicate_groups']:
            with st.expander("ğŸ” ì¤‘ë³µ ì œê±° ìƒì„¸ ì •ë³´", expanded=False):
                for i, group in enumerate(analysis['duplicate_groups'], 1):
                    best_name = self.deduplicator.select_best_name(group)
                    st.write(f"**ê·¸ë£¹ {i}**: {best_name}")
                    for law in group:
                        st.write(f"  - {law}")

        # 4. ë²•ë ¹ë³„ë¡œ ìœ„ë²•ì„± ê²°ê³¼ ì¬êµ¬ì„±
        restructured_results = self._restructure_by_laws(
            violation_results,
            analysis['duplicate_groups'],
            analysis['deduplicated_laws']
        )

        return {
            'success': True,
            'original_law_count': analysis['original_count'],
            'deduplicated_law_count': analysis['deduplicated_count'],
            'reduction_rate': analysis['reduction_rate'],
            'duplicate_groups': analysis['duplicate_groups'],
            'deduplicated_laws': analysis['deduplicated_laws'],
            'restructured_results': restructured_results,
            'optimization_summary': {
                'laws_processed': len(extracted_laws),
                'duplicates_removed': analysis['reduction_count'],
                'efficiency_gain': analysis['reduction_rate'],
                'expected_api_savings': analysis['reduction_count']
            }
        }

    def _restructure_by_laws(self, violation_results: List[Dict], duplicate_groups: List[List[str]], deduplicated_laws: List[str]) -> List[Dict]:
        """ë²•ë ¹ë³„ë¡œ ìœ„ë²•ì„± ê²°ê³¼ ì¬êµ¬ì„±"""

        # ë²•ë ¹ëª… ë§¤í•‘ ìƒì„± (ì›ë³¸ â†’ í‘œì¤€ëª…)
        law_mapping = {}

        for group in duplicate_groups:
            standard_name = self.deduplicator.select_best_name(group)
            for law_name in group:
                law_mapping[law_name] = standard_name

        # ê·¸ë£¹ì— ì†í•˜ì§€ ì•Šì€ ë²•ë ¹ë„ ë§¤í•‘ì— ì¶”ê°€
        all_grouped_laws = set()
        for group in duplicate_groups:
            all_grouped_laws.update(group)

        for law in deduplicated_laws:
            if law not in all_grouped_laws:
                law_mapping[law] = law

        # í‘œì¤€ ë²•ë ¹ëª…ë³„ë¡œ ê²°ê³¼ ê·¸ë£¹í™”
        law_based_results = {}

        for result in violation_results:
            text_content = self._extract_text_from_result(result)
            found_laws = self._find_law_patterns(text_content)

            # ì´ ê²°ê³¼ê°€ ê´€ë ¨ëœ í‘œì¤€ ë²•ë ¹ëª…ë“¤ ì°¾ê¸°
            related_standard_laws = set()
            for found_law in found_laws:
                # ì •ê·œí™” í›„ ë§¤í•‘ì—ì„œ ì°¾ê¸°
                normalized = self.deduplicator.normalize_law_name(found_law)
                for original, standard in law_mapping.items():
                    if self.deduplicator.calculate_similarity(normalized, original) >= 0.8:
                        related_standard_laws.add(standard)
                        break

            # ê° í‘œì¤€ ë²•ë ¹ì— ê²°ê³¼ ì¶”ê°€
            for standard_law in related_standard_laws:
                if standard_law not in law_based_results:
                    law_based_results[standard_law] = {
                        'law_name': standard_law,
                        'violation_cases': [],
                        'total_risk_score': 0,
                        'case_count': 0
                    }

                law_based_results[standard_law]['violation_cases'].append(result)
                law_based_results[standard_law]['case_count'] += 1

                # ìœ„í—˜ë„ ì ìˆ˜ ëˆ„ì  (ê²°ê³¼ì— ìˆë‹¤ë©´)
                if isinstance(result, dict) and 'risk_score' in result:
                    law_based_results[standard_law]['total_risk_score'] += result['risk_score']

        # í‰ê·  ìœ„í—˜ë„ ê³„ì‚° ë° ì •ë ¬
        final_results = []
        for law_name, data in law_based_results.items():
            avg_risk = data['total_risk_score'] / data['case_count'] if data['case_count'] > 0 else 0

            final_results.append({
                'law_name': law_name,
                'case_count': data['case_count'],
                'average_risk_score': avg_risk,
                'total_risk_score': data['total_risk_score'],
                'violation_cases': data['violation_cases'][:5],  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                'summary': f"{law_name}ì— ëŒ€í•œ {data['case_count']}ê°œ ìœ„ë²• ì‚¬ë¡€ (í‰ê·  ìœ„í—˜ë„: {avg_risk:.2f})"
            })

        # ìœ„í—˜ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        final_results.sort(key=lambda x: x['average_risk_score'], reverse=True)

        return final_results

    def create_optimized_gemini_prompt(self, processed_results: Dict[str, Any]) -> str:
        """ìµœì í™”ëœ Gemini í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        if not processed_results.get('success'):
            return "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        restructured = processed_results['restructured_results']
        optimization = processed_results['optimization_summary']

        prompt_parts = []

        # í—¤ë” ë° ìµœì í™” ì •ë³´
        prompt_parts.append(f"""# ì¡°ë¡€ ìœ„ë²•ì„± ì¢…í•© ë¶„ì„ (ì¤‘ë³µ ì œê±° ìµœì í™”)

## ìµœì í™” ì„±ê³¼
- ì›ë³¸ ë²•ë ¹ ìˆ˜: {optimization['laws_processed']}ê°œ
- ì¤‘ë³µ ì œê±° í›„: {processed_results['deduplicated_law_count']}ê°œ
- ì¤‘ë³µ ì œê±°ìœ¨: {processed_results['reduction_rate']:.1f}%
- API í˜¸ì¶œ ì ˆì•½: {optimization['expected_api_savings']}íšŒ ì˜ˆìƒ

## ì£¼ìš” ìœ„ë²• ìœ„í—˜ ë²•ë ¹ (ìœ„í—˜ë„ ìˆœ)

""")

        # ìƒìœ„ 10ê°œ ë²•ë ¹ë§Œ í¬í•¨ (API íš¨ìœ¨ì„±)
        top_laws = restructured[:10]

        for i, law_data in enumerate(top_laws, 1):
            prompt_parts.append(f"""
### {i}. {law_data['law_name']}
- ìœ„ë²• ì‚¬ë¡€ ìˆ˜: {law_data['case_count']}ê°œ
- í‰ê·  ìœ„í—˜ë„: {law_data['average_risk_score']:.2f}
- ì´ ìœ„í—˜ ì ìˆ˜: {law_data['total_risk_score']:.2f}
- ë¶„ì„ ìš”ì•½: {law_data['summary']}

""")

        # ë¶„ì„ ìš”ì²­
        prompt_parts.append("""
## ë¶„ì„ ìš”ì²­

ìœ„ ìµœì í™”ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **ìœ„ë²•ì„± ìš°ì„ ìˆœìœ„**: í‰ê·  ìœ„í—˜ë„ì™€ ì‚¬ë¡€ ìˆ˜ë¥¼ ê³ ë ¤í•œ ì‹œê¸‰ì„± í‰ê°€
2. **ì¤‘ë³µ ì œê±° íš¨ê³¼**: ì •í™•í•œ ë²•ë ¹ë³„ ë¶„ì„ì´ ê°€ëŠ¥í•´ì§„ ì¥ì  ì„¤ëª…
3. **í•µì‹¬ ê°œì„ ì‚¬í•­**: ìœ„í—˜ë„ê°€ ë†’ì€ ìƒìœ„ 3ê°œ ë²•ë ¹ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ
4. **ë²•ì  ê·¼ê±°**: ê° ìœ„ë²• ìœ í˜•ë³„ ê´€ë ¨ ë²•ì¡°ë¬¸ ë° íŒë¡€
5. **ì‹¤í–‰ ê³„íš**: ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ë‹¨ê³„ë³„ ì¡°ë¡€ ê°œì„  ë¡œë“œë§µ

**ë¶„ì„ ì¥ì **:
- ì¤‘ë³µ ë²•ë ¹ì´ í†µí•©ë˜ì–´ í˜¼ë€ ì—†ëŠ” ì •í™•í•œ ë¶„ì„
- ë²•ë ¹ë³„ ì¢…í•©ì  ìœ„í—˜ë„ í‰ê°€ ê°€ëŠ¥
- API í˜¸ì¶œ ìµœì í™”ë¡œ ë¹„ìš© íš¨ìœ¨ì  ë¶„ì„

""")

        return "".join(prompt_parts)

def demo_integrated_analyzer():
    """í†µí•© ë¶„ì„ê¸° ë°ëª¨"""

    st.title("ğŸš€ í†µí•© ìœ„ë²•ì„± ë¶„ì„ê¸°")
    st.markdown("ë²•ë ¹ëª… ì¤‘ë³µ ì œê±° + Gemini API ìµœì í™” í†µí•© ì†”ë£¨ì…˜")
    st.markdown("---")

    # ìƒ˜í”Œ ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼
    sample_violation_results = [
        {
            "content": "ê³µê³µê¸°ê´€ì˜ìš´ì˜ì—ê´€í•œë²•ë¥  ì œ4ì¡°ì— ë”°ë¥´ë©´ ì§€ë°©ìì¹˜ë‹¨ì²´ëŠ” ê¸°ê´€ìœ„ì„ì‚¬ë¬´ë¥¼ ì²˜ë¦¬í•  ë•Œ...",
            "violation_type": "ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ìœ„ë°˜",
            "risk_score": 0.85,
            "similarity": 0.92
        },
        {
            "content": "ê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œë²•ë¥  ì œ10ì¡° ìœ„ë°˜ìœ¼ë¡œ ì¸í•œ ê¶Œí•œ ìœ„ì„ í•œê³„ ì´ˆê³¼ ì‚¬ë¡€",
            "violation_type": "ê¶Œí•œìœ„ì„ í•œê³„ ì´ˆê³¼",
            "risk_score": 0.78,
            "similarity": 0.88
        },
        {
            "content": "ê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¥¸ ê°ë…ê¶Œí•œì„ ì§€ë°©ìì¹˜ë²• ì œ22ì¡°ì™€ í•¨ê»˜ ê²€í† í•˜ë©´...",
            "violation_type": "ê°ë…ê¶Œí•œ ë²”ìœ„ ìœ„ë°˜",
            "risk_score": 0.65,
            "similarity": 0.75
        },
        {
            "content": "ì§€ë°©ìì¹˜ë²•ê³¼ í–‰ì • ì ˆì°¨ ë²•ì˜ ê·œì •ì— ë”°ë¥¸ ì ˆì°¨ì  í•˜ìê°€ ë°œê²¬ë¨",
            "violation_type": "ì ˆì°¨ì  í•˜ì",
            "risk_score": 0.55,
            "similarity": 0.70
        },
        {
            "content": "ì§€ë°© ìì¹˜ë²• ì œ9ì¡°ì™€ êµ­ê°€ ì¬ì •ë²•ì˜ ì¶©ëŒë¡œ ì¸í•œ ìœ„ë²• ì†Œì§€",
            "violation_type": "ìƒìœ„ë²•ë ¹ ì¶©ëŒ",
            "risk_score": 0.72,
            "similarity": 0.82
        }
    ]

    # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
    st.subheader("ğŸ“Š ìƒ˜í”Œ ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼")
    with st.expander("ì›ë³¸ ë°ì´í„° ë³´ê¸°", expanded=False):
        st.json(sample_violation_results)

    if st.button("ğŸ” í†µí•© ë¶„ì„ ì‹¤í–‰", type="primary"):

        analyzer = IntegratedViolationAnalyzer()

        with st.spinner("í†µí•© ë¶„ì„ ì²˜ë¦¬ ì¤‘..."):
            # í†µí•© ë¶„ì„ ì‹¤í–‰
            processed_results = analyzer.process_with_deduplication(sample_violation_results)

        if processed_results.get('success'):
            st.success("âœ… í†µí•© ë¶„ì„ ì™„ë£Œ!")

            # ìµœì í™” íš¨ê³¼ í‘œì‹œ
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ì›ë³¸ ë²•ë ¹", f"{processed_results['original_law_count']}ê°œ")

            with col2:
                st.metric("ì •ë¦¬ í›„", f"{processed_results['deduplicated_law_count']}ê°œ")

            with col3:
                st.metric("ì¤‘ë³µ ì œê±°ìœ¨", f"{processed_results['reduction_rate']:.1f}%")

            with col4:
                st.metric("API ì ˆì•½", f"{processed_results['optimization_summary']['expected_api_savings']}íšŒ")

            # ë²•ë ¹ë³„ ë¶„ì„ ê²°ê³¼
            st.subheader("ğŸ“‹ ë²•ë ¹ë³„ ìœ„ë²•ì„± ë¶„ì„")

            for result in processed_results['restructured_results']:
                with st.expander(f"âš–ï¸ {result['law_name']} (ìœ„í—˜ë„: {result['average_risk_score']:.2f})", expanded=False):
                    col1, col2 = st.columns(2)

                    with col1:
                        st.write(f"**ì‚¬ë¡€ ìˆ˜**: {result['case_count']}ê°œ")
                        st.write(f"**í‰ê·  ìœ„í—˜ë„**: {result['average_risk_score']:.2f}")
                        st.write(f"**ì´ ìœ„í—˜ ì ìˆ˜**: {result['total_risk_score']:.2f}")

                    with col2:
                        st.write("**ì£¼ìš” ìœ„ë²• ìœ í˜•**:")
                        for case in result['violation_cases'][:3]:
                            if isinstance(case, dict) and 'violation_type' in case:
                                st.write(f"- {case['violation_type']}")

            # ìµœì í™”ëœ Gemini í”„ë¡¬í”„íŠ¸
            st.subheader("ğŸ¤– ìµœì í™”ëœ Gemini í”„ë¡¬í”„íŠ¸")

            optimized_prompt = analyzer.create_optimized_gemini_prompt(processed_results)

            st.text_area(
                "ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 1000ì):",
                optimized_prompt[:1000] + "..." if len(optimized_prompt) > 1000 else optimized_prompt,
                height=200
            )

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.download_button(
                label="ğŸ“„ ì „ì²´ í”„ë¡¬í”„íŠ¸ ë‹¤ìš´ë¡œë“œ",
                data=optimized_prompt,
                file_name="optimized_violation_analysis_prompt.txt",
                mime="text/plain"
            )

            # ìµœì í™” ìš”ì•½
            st.subheader("ğŸ“ˆ ìµœì í™” ìš”ì•½")

            optimization = processed_results['optimization_summary']

            st.info(f"""
            **ğŸ¯ ìµœì í™” ì„±ê³¼**:
            - ë²•ë ¹ ì¤‘ë³µ {optimization['duplicates_removed']}ê°œ ì œê±°
            - ë¶„ì„ íš¨ìœ¨ì„± {optimization['efficiency_gain']:.1f}% í–¥ìƒ
            - Gemini API í˜¸ì¶œ {optimization['expected_api_savings']}íšŒ ì ˆì•½
            - ë²•ë ¹ë³„ ì¢…í•© ìœ„í—˜ë„ í‰ê°€ ê°€ëŠ¥
            - ì¤‘ë³µìœ¼ë¡œ ì¸í•œ í˜¼ë€ ì œê±°
            """)

        else:
            st.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {processed_results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

if __name__ == "__main__":
    demo_integrated_analyzer()