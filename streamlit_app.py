import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from docx import Document
from docx.shared import Inches, Mm
from docx.enum.section import WD_ORIENT
import PyPDF2
import google.generativeai as genai
import openai
import os
import tempfile
import re
from docx.shared import RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
import io
import base64
import numpy as np
import pickle
import hashlib
from typing import Dict, List
from sklearn.metrics.pairwise import cosine_similarity

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê´‘ì—­ì§€ìì²´ ì¡°ë¡€ ê²€ìƒ‰, ë¹„êµ, ë¶„ì„",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì‚¬ìš©ì ì •ì˜ CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #4f46e5, #7c3aed);
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        color: white;
        margin-bottom: 2rem;
    }
    .step-card {
        background: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .result-card {
        background: #ffffff;
        border: 1px solid #d1d5db;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .law-title {
        color: #dc2626;
        font-weight: bold;
    }

    /* íƒ­ ê¸€ì í¬ê¸° í‚¤ìš°ê¸° */
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
        font-size: 18px !important;
        font-weight: 600 !important;
        font-size: 1.1em;
        margin-bottom: 0.5rem;
    }
    .metro-name {
        color: #1e40af;
        font-weight: 600;
        margin-bottom: 0.3rem;
    }
    .stButton > button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

# API ì„¤ì •
OC = "climsneys85"
search_url = "http://www.law.go.kr/DRF/lawSearch.do"
detail_url = "http://www.law.go.kr/DRF/lawService.do"
precedent_search_url = "http://www.law.go.kr/DRF/lawSearch.do"  # íŒë¡€ ê²€ìƒ‰ API

# ê´‘ì—­ì§€ìì²´ ì½”ë“œ ë° ì´ë¦„
metropolitan_govs = {
    '6110000': 'ì„œìš¸íŠ¹ë³„ì‹œ',
    '6260000': 'ë¶€ì‚°ê´‘ì—­ì‹œ',
    '6270000': 'ëŒ€êµ¬ê´‘ì—­ì‹œ',
    '6280000': 'ì¸ì²œê´‘ì—­ì‹œ',
    '6290000': 'ê´‘ì£¼ê´‘ì—­ì‹œ',
    '6300000': 'ëŒ€ì „ê´‘ì—­ì‹œ',
    '5690000': 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ',
    '6310000': 'ìš¸ì‚°ê´‘ì—­ì‹œ',
    '6410000': 'ê²½ê¸°ë„',
    '6530000': 'ê°•ì›íŠ¹ë³„ìì¹˜ë„',
    '6430000': 'ì¶©ì²­ë¶ë„',
    '6440000': 'ì¶©ì²­ë‚¨ë„',
    '6540000': 'ì „ë¶íŠ¹ë³„ìì¹˜ë„',
    '6460000': 'ì „ë¼ë‚¨ë„',
    '6470000': 'ê²½ìƒë¶ë„',
    '6480000': 'ê²½ìƒë‚¨ë„',
    '6500000': 'ì œì£¼íŠ¹ë³„ìì¹˜ë„'
}

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'search_results' not in st.session_state:
    st.session_state.search_results = []
if 'uploaded_pdf' not in st.session_state:
    st.session_state.uploaded_pdf = None
if 'search_query' not in st.session_state:
    st.session_state.search_query = ""
if 'word_doc_ready' not in st.session_state:
    st.session_state.word_doc_ready = False
if 'word_doc_data' not in st.session_state:
    st.session_state.word_doc_data = None
if 'selected_ordinances' not in st.session_state:
    st.session_state.selected_ordinances = []
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None

def get_ordinance_detail(ordinance_id):
    """ì¡°ë¡€ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    params = {
        'OC': OC,
        'target': 'ordin',
        'ID': ordinance_id,
        'type': 'XML'
    }
    try:
        response = requests.get(detail_url, params=params, timeout=60)
        root = ET.fromstring(response.text)
        articles = []
        for article in root.findall('.//ì¡°'):
            content = article.find('ì¡°ë‚´ìš©').text if article.find('ì¡°ë‚´ìš©') is not None else ""
            if content:
                content = content.replace('<![CDATA[', '').replace(']]>', '')
                content = content.replace('<p>', '').replace('</p>', '\n')
                content = content.replace('<br/>', '\n')
                content = content.replace('<br>', '\n')
                content = content.replace('&nbsp;', ' ')
                content = content.strip()
            if content:
                articles.append(content)
        return articles
    except Exception:
        return []

def search_ordinances(query):
    """ì¡°ë¡€ ê²€ìƒ‰ í•¨ìˆ˜"""
    results = []
    total_count = 0
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_metros = len(metropolitan_govs)
    
    for idx, (org_code, metro_name) in enumerate(metropolitan_govs.items()):
        status_text.text(f"ê²€ìƒ‰ ì¤‘... {metro_name} ({idx + 1}/{total_metros})")
        progress_bar.progress((idx + 1) / total_metros)
        
        try:
            params = {
                'OC': OC,
                'target': 'ordin',
                'type': 'XML',
                'query': query,
                'display': 100,
                'search': 1,
                'sort': 'ddes',
                'page': 1,
                'org': org_code
            }
            
            response = requests.get(search_url, params=params, timeout=60)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            
            for law in root.findall('.//law'):
                ordinance_name = law.find('ìì¹˜ë²•ê·œëª…').text if law.find('ìì¹˜ë²•ê·œëª…') is not None else ""
                ordinance_id = law.find('ìì¹˜ë²•ê·œID').text if law.find('ìì¹˜ë²•ê·œID') is not None else None
                ê¸°ê´€ëª… = law.find('ì§€ìì²´ê¸°ê´€ëª…').text if law.find('ì§€ìì²´ê¸°ê´€ëª…') is not None else ""
                
                if ê¸°ê´€ëª… != metro_name:
                    continue
                
                # ê²€ìƒ‰ì–´ ë§¤ì¹­ ë¡œì§
                search_terms = [term.lower() for term in query.split() if term.strip()]
                ordinance_name_clean = ordinance_name.replace(' ', '').lower()
                if not all(term in ordinance_name_clean for term in search_terms):
                    continue
                
                total_count += 1
                articles = get_ordinance_detail(ordinance_id)
                
                results.append({
                    'name': ordinance_name,
                    'content': articles,
                    'metro': metro_name
                })
                
        except Exception as e:
            st.warning(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({metro_name}): {str(e)}")
            continue
    
    progress_bar.empty()
    status_text.empty()
    
    return results, total_count

def create_word_document(query, results):
    """Word ë¬¸ì„œ ìƒì„± í•¨ìˆ˜"""
    doc = Document()
    section = doc.sections[-1]
    section.orientation = WD_ORIENT.LANDSCAPE
    section.page_width = Mm(420)
    section.page_height = Mm(297)

    # ì œëª© ì¶”ê°€
    title = doc.add_heading('ì¡°ë¡€ ê²€ìƒ‰ ê²°ê³¼', level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph(f'ê²€ìƒ‰ì–´: {query}')
    doc.add_paragraph(f'ì´ {len(results)}ê±´ì˜ ì¡°ë¡€ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.\n')

    # ì¡°ë¡€ë¥¼ 3ê°œì”© ê·¸ë£¹í™”í•˜ì—¬ 3ë‹¨ ë¹„êµí‘œ í˜•íƒœë¡œ ìƒì„±
    for i in range(0, len(results), 3):
        current_laws = results[i:i+3]
        while len(current_laws) < 3:
            current_laws.append({'name': '', 'content': [], 'metro': ''})

        # í‘œ ìƒì„± (1í–‰, 3ì—´ ê³ ì •)
        table = doc.add_table(rows=1, cols=3)
        table.style = 'Table Grid'
        table.autofit = True

        # ê° ì…€ì— ì¡°ë¡€ ë‚´ìš© ì¶”ê°€
        for idx, law in enumerate(current_laws):
            cell = table.cell(0, idx)
            paragraph = cell.paragraphs[0]
            
            if law['name']:
                # ì¡°ë¡€ëª… ì¶”ê°€ (ì§€ìì²´ëª… + ì¡°ë¡€ëª…)
                run = paragraph.add_run(f"{law['metro']}\n{law['name']}\n\n")
                run.bold = True
                run.font.color.rgb = RGBColor(255, 0, 0)  # ë¹¨ê°„ìƒ‰
                
                # ì¡°ë¬¸ ë‚´ìš© ì¶”ê°€
                if law['content']:
                    content_text = '\n\n'.join(law['content'])
                    paragraph.add_run(content_text)
                else:
                    paragraph.add_run('(ì¡°ë¬¸ ì—†ìŒ)')

        # ë§ˆì§€ë§‰ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ í˜ì´ì§€ ë‚˜ëˆ„ê¸° ì¶”ê°€
        if i + 3 < len(results):
            doc.add_page_break()

    return doc

def extract_pdf_text(pdf_file):
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜"""
    try:
        reader = PyPDF2.PdfReader(pdf_file)
        text = ''
        for page in reader.pages:
            text += page.extract_text() + '\n'
        return text
    except Exception as e:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def extract_superior_laws(pdf_text):
    """ì¡°ë¡€ì•ˆì—ì„œ ìƒìœ„ë²•ë ¹ ì¶”ì¶œ í•¨ìˆ˜ - GUI ê²€ì¦ëœ ë¡œì§ ì ìš©"""
    import re

    # ìƒìœ„ë²• í›„ë³´ ì¶”ì¶œì„ ìœ„í•œ í‚¤ì›Œë“œ (ì¡°ë¡€ì•ˆì—ì„œ ìƒìœ„ë²•ë ¹ ì–¸ê¸‰í•˜ëŠ” ëª¨ë“  ë§¥ë½ í¬í•¨)
    law_check_keywords = [
        'ìœ„ë°˜', 'ìœ„ë°°', 'ì¶©ëŒ', 'ì €ì´‰', 'ì¤€ìˆ˜', 'ì í•©', 'ë¶ˆì¼ì¹˜',
        'ìƒìœ„ë²•', 'ìƒìœ„ ë²•ë ¹', 'ìƒìœ„ë²•ë ¹', 'ë²•ë ¹ê³¼ì˜ ê´€ê³„', 'ë²•ë ¹ê³¼ì˜ ì¶©ëŒ', 'ë²•ë ¹ê³¼ì˜ ìœ„ë°°',
        'ê´€ê³„ë²•ë ¹', 'ê·¼ê±°ë²•ë ¹', 'ë²•ì ê·¼ê±°', 'ì°¸ê³ ì‚¬í•­', 'ê´€ë ¨ë²•ë ¹', 'ì†Œê´€ë²•ë ¹',
        'ë²•ë ¹', 'ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ê·œì •', 'ê°œì •', 'ì œì •', 'ë²•'  # ì¼ë°˜ì ì¸ ë²•ë ¹ ì–¸ê¸‰
    ]

    # ë²•ë ¹ëª… íŒ¨í„´ (ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ì¶”ì¶œ ê°œì„ )
    law_pattern = re.compile(r'([ê°€-í£\w\s]*(?:ë²•|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™))\s*(?:[ã€]|$|[.,;:\s])', re.MULTILINE)

    # ìƒìœ„ë²• í›„ë³´ ì¶”ì¶œ
    upper_law_candidates = set()

    # 1. ìƒìœ„ë²• ê´€ë ¨ ë§¥ë½ì´ ìˆëŠ” ì¤„ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ
    for line in pdf_text.split('\n'):
        if any(keyword in line for keyword in law_check_keywords):
            for match in law_pattern.finditer(line):
                law_name = match.group(1).strip()
                if law_name:
                    upper_law_candidates.add(law_name)

    # 2. ì¶”ê°€ íŒ¨í„´: ã€Œë²•ë ¹ëª…ã€ í˜•ì‹ìœ¼ë¡œ ë”°ì˜´í‘œ ì•ˆì— ìˆëŠ” ë²•ë ¹ëª… ì¶”ì¶œ
    quote_pattern = re.compile(r'[ã€Œã€]([^ã€ã€]*(?:ë²•|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™))[ã€ã€]')
    for match in quote_pattern.finditer(pdf_text):
        law_name = match.group(1).strip()
        if law_name:
            upper_law_candidates.add(law_name)

    # 3. ì¶”ê°€ íŒ¨í„´: "â—‹â—‹ë²•ë ¹:" ë˜ëŠ” "ê´€ê³„ë²•ë ¹:" ë’¤ì— ì˜¤ëŠ” ë²•ë ¹ëª…
    relation_pattern = re.compile(r'(?:ê´€ê³„ë²•ë ¹|ê·¼ê±°ë²•ë ¹|ë²•ì ê·¼ê±°|ì†Œê´€ë²•ë ¹|ê´€ë ¨ë²•ë ¹)\s*[:ï¼š]\s*[ã€Œã€]?([^ã€ã€\n]*(?:ë²•|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™))[ã€ã€]?')
    for match in relation_pattern.finditer(pdf_text):
        law_name = match.group(1).strip()
        if law_name:
            upper_law_candidates.add(law_name)

    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì‹¤ì¡´í•˜ì§€ ì•ŠëŠ” ë²•ë ¹ëª…)
    invalid_law_names = {
        'ìì¹˜ì…ë²•', 'ì¡°ë¡€', 'ê·œì¹™', 'ì§€ì¹¨', 'ë‚´ê·œ', 'ì˜ˆê·œ', 'í›ˆë ¹', 'ì ë²•',
        'ì…ë²•', 'ìƒìœ„ë²•', 'ìœ„ë²•', 'í•©ë²•', 'ë¶ˆë²•', 'ë°©ë²•', 'í—Œë²•ìƒ', 'í—Œë²•ì ',
        'ë²•ì ', 'ë²•ë¥ ì ', 'ë²•ë ¹ìƒ', 'ë²•ë¥ ìƒ', 'ë²•ë¥ ', 'ë²•ë ¹', 'ë²•', 'ê·œì •',
        'ì¡°í•­', 'ì¡°ë¬¸', 'ê·œë²”', 'ì›ì¹™', 'ê¸°ì¤€', 'ì‚¬í•­', 'ë‚´ìš©', 'ê´€ë ¨ë²•',
        'ê´€ë ¨ ë²•', 'ê´€ë ¨ë²•ë ¹', 'ê´€ë ¨ ë²•ë ¹'
    }

    def is_valid_law_name(name):
        """ìœ íš¨í•œ ë²•ë ¹ëª…ì¸ì§€ ê²€ì¦"""
        # ëŒ€ì†Œë¬¸ì, ê³µë°± ëª¨ë‘ ì œê±° í›„ ë¹„êµ
        name_clean = name.strip().replace(' ', '').lower()

        # ë¶ˆìš©ì–´ ì²´í¬
        for invalid in invalid_law_names:
            if name_clean == invalid.replace(' ', '').lower():
                return False

        # ìˆ«ì+ë²•(ì˜ˆ: 1ë²•, 2ë²• ë“±)ë„ ì œì™¸
        if name_clean and name_clean[0].isdigit():
            return False

        # ë„ˆë¬´ ì§§ì€ ì´ë¦„ ì œì™¸
        if len(name_clean) < 3:
            return False

        return True

    # ìœ íš¨í•œ ë²•ë ¹ëª…ë§Œ í•„í„°ë§
    valid_laws = []
    for law_name in upper_law_candidates:
        if is_valid_law_name(law_name):
            valid_laws.append(law_name)

    # ğŸ†• ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ìë™ ìœ ì¶” ì¶”ê°€
    additional_laws = []
    for law in valid_laws:
        if law.endswith('ë²•') and 'ì‹œí–‰' not in law:
            # í•´ë‹¹ ë²•ë¥ ì˜ ì‹œí–‰ë ¹ê³¼ ì‹œí–‰ê·œì¹™ì„ ìë™ìœ¼ë¡œ ì¶”ê°€
            base_name = law

            # ì‹œí–‰ë ¹ ì¶”ê°€ (ì¼ë°˜ì ì¸ íŒ¨í„´)
            potential_decree = f"{base_name} ì‹œí–‰ë ¹"
            if potential_decree not in valid_laws:
                additional_laws.append(potential_decree)

            # ì‹œí–‰ê·œì¹™ ì¶”ê°€ (ì¼ë°˜ì ì¸ íŒ¨í„´)
            potential_rule = f"{base_name} ì‹œí–‰ê·œì¹™"
            if potential_rule not in valid_laws:
                additional_laws.append(potential_rule)

    # ì¶”ê°€ëœ ë²•ë ¹ë“¤ì„ í¬í•¨
    if additional_laws:
        import streamlit as st
        st.info(f"ğŸ”„ ìë™ ì¶”ê°€ëœ í•˜ìœ„ ë²•ë ¹: {len(additional_laws)}ê°œ")
        with st.expander("ğŸ“‹ ìë™ ì¶”ê°€ëœ ë²•ë ¹", expanded=False):
            for law in additional_laws:
                st.markdown(f"- {law}")
        valid_laws.extend(additional_laws)

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    unique_laws = list(set(valid_laws))
    unique_laws.sort()

    return unique_laws[:20]  # ìµœëŒ€ 20ê°œ ë°˜í™˜

def get_superior_law_content_xml(law_name):
    """XML APIë¥¼ í†µí•´ ìƒìœ„ë²•ë ¹ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì„±ê³µì ì¸ ë¡œì§ ì ìš©)"""
    try:
        import xml.etree.ElementTree as ET
        import re


        # ê²€ìƒ‰ì–´ ìµœì í™”: ë„ì–´ì“°ê¸°ì™€ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        search_query = law_name.strip()

        # 1ë‹¨ê³„: ë²•ë ¹ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ ë°˜í™˜)
        search_params = {
            'OC': OC,
            'target': 'law',
            'type': 'XML',
            'query': search_query,
            'display': 10  # ë” ë§ì€ ê²°ê³¼ ê²€ìƒ‰
        }
        
        search_response = requests.get(search_url, params=search_params, timeout=30)
        if search_response.status_code != 200:
            st.error(f"[DEBUG] ê²€ìƒ‰ ì‹¤íŒ¨: HTTP {search_response.status_code}")
            return get_superior_law_content_xml_fallback(law_name)
        
        search_root = ET.fromstring(search_response.text)
        
        # í˜„í–‰ ë²•ë ¹ ì°¾ê¸° - ë” ìœ ì—°í•œ ê²€ìƒ‰
        current_laws = []
        for law in search_root.findall('.//law'):
            status = law.find('í˜„í–‰ì—°í˜ì½”ë“œ')
            if status is not None and status.text == 'í˜„í–‰':
                law_id_elem = law.find('ë²•ë ¹ID')
                law_name_elem = law.find('ë²•ë ¹ëª…í•œê¸€')
                if law_id_elem is not None and law_name_elem is not None:
                    current_laws.append({
                        'id': law_id_elem.text,
                        'name': law_name_elem.text
                    })
        
        if not current_laws:
            st.warning(f"[DEBUG] {law_name}ì˜ í˜„í–‰ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return get_superior_law_content_xml_fallback(law_name)
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë²•ë ¹ ì„ íƒ (ê°œì„ ëœ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜)
        best_law = None
        best_score = -1

        for law_info in current_laws:
            found_name = law_info['name']
            score = 0

            # 1. ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
            if found_name == law_name:
                score += 1000

            # 2. ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜ (ì–‘ë°©í–¥)
            if law_name in found_name:
                score += 500
            if found_name in law_name:
                score += 300

            # 3. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°œì„ ëœ ë¡œì§)
            law_lower = law_name.lower().replace(' ', '')
            found_lower = found_name.lower().replace(' ', '')

            # ì—¬ê°ìë™ì°¨ ìš´ìˆ˜ì‚¬ì—…ë²• ê´€ë ¨ íŠ¹ë³„ ì ìˆ˜
            if 'ì—¬ê°ìë™ì°¨' in law_lower and 'ìš´ìˆ˜ì‚¬ì—…' in law_lower:
                if 'ì—¬ê°ìë™ì°¨' in found_lower and 'ìš´ìˆ˜ì‚¬ì—…' in found_lower:
                    score += 400  # ì—¬ê°ìë™ì°¨ ìš´ìˆ˜ì‚¬ì—…ë²• ê´€ë ¨ ë†’ì€ ì ìˆ˜
                    if 'ì‹œí–‰ê·œì¹™' in law_lower and 'ì‹œí–‰ê·œì¹™' in found_lower:
                        score += 200  # ì‹œí–‰ê·œì¹™ ë§¤ì¹­ ì¶”ê°€ ì ìˆ˜

            # ë„ë¡œêµí†µë²• ê´€ë ¨
            if 'ë„ë¡œ' in law_lower and 'êµí†µ' in law_lower:
                if 'ë„ë¡œêµí†µ' in found_lower and 'íŠ¹ë³„íšŒê³„' not in found_lower:
                    score += 300
                elif 'êµí†µì‹œì„¤' in found_lower:
                    score -= 100

            # 4. ë²•ë ¹ ìœ í˜• ë§¤ì¹­ ì ìˆ˜ (ìš”ì²­ëœ ìœ í˜•ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€)
            requested_type = ''
            if 'ì‹œí–‰ê·œì¹™' in law_lower:
                requested_type = 'ì‹œí–‰ê·œì¹™'
            elif 'ì‹œí–‰ë ¹' in law_lower:
                requested_type = 'ì‹œí–‰ë ¹'
            elif 'ë²•' in law_lower and 'ì‹œí–‰' not in law_lower:
                requested_type = 'ë²•'

            if requested_type:
                if requested_type in found_lower:
                    score += 300  # ìš”ì²­ëœ ë²•ë ¹ ìœ í˜•ê³¼ ì¼ì¹˜í•˜ë©´ ë†’ì€ ì ìˆ˜
                elif requested_type == 'ë²•' and found_lower.endswith('ë²•') and 'ì‹œí–‰' not in found_lower:
                    score += 300
            else:
                # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ (ë²•ë¥  > ì‹œí–‰ë ¹ > ì‹œí–‰ê·œì¹™)
                if found_lower.endswith('ë²•') and not ('ì‹œí–‰ë ¹' in found_lower or 'ì‹œí–‰ê·œì¹™' in found_lower):
                    score += 100
                elif 'ì‹œí–‰ë ¹' in found_lower:
                    score += 50
                elif 'ì‹œí–‰ê·œì¹™' in found_lower:
                    score += 25

            # 5. ê¸¸ì´ í˜ë„í‹° ì™„í™” (ë„ˆë¬´ ê¸´ ë²•ë ¹ëª…ì€ ì•½ê°„ ê°ì )
            if len(found_name) > 30:
                score -= 30
                
            
            if score > best_score:
                best_score = score
                best_law = law_info
        
        if best_law:
            law_id = best_law['id']
            exact_law_name = best_law['name']
        else:
            # í´ë°±: ì²« ë²ˆì§¸ ë²•ë ¹
            law_id = current_laws[0]['id']
            exact_law_name = current_laws[0]['name']
        
        if not law_id:
            st.warning(f"[DEBUG] {law_name}ì˜ í˜„í–‰ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return get_superior_law_content_xml_fallback(law_name)
        
        # 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        detail_params = {
            'OC': OC,
            'target': 'law',
            'type': 'XML',
            'ID': law_id
        }
        
        detail_response = requests.get(detail_url, params=detail_params, timeout=30)
        if detail_response.status_code != 200:
            st.error(f"[DEBUG] ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: HTTP {detail_response.status_code}")
            return get_superior_law_content_xml_fallback(law_name)
        
        detail_root = ET.fromstring(detail_response.text)
        
        # 3ë‹¨ê³„: ì„±ê³µì ì¸ ì¶”ì¶œ ë¡œì§ ì ìš© - ì—°ê²°ëœ ë³¸ë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        upper_law_text = ""
        jo_count = 0
        hang_count = 0 
        ho_count = 0
        
        for node in detail_root.iter():
            if node.tag == 'ì¡°ë¬¸ë‚´ìš©' and node.text and node.text.strip():
                content = re.sub(r'<[^>]+>', '', node.text)
                content = content.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>').strip()
                upper_law_text += content + '\n'
                jo_count += 1
            elif node.tag == 'í•­ë‚´ìš©' and node.text and node.text.strip():
                content = re.sub(r'<[^>]+>', '', node.text)
                content = content.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>').strip()
                upper_law_text += '    ' + content + '\n'
                hang_count += 1
            elif node.tag == 'í˜¸ë‚´ìš©' and node.text and node.text.strip():
                content = re.sub(r'<[^>]+>', '', node.text)
                content = content.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>').strip()
                upper_law_text += '        ' + content + '\n'
                ho_count += 1
        
        
        if upper_law_text.strip():
            # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§: ì¡°ë¡€ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ ìš°ì„  ì¶”ì¶œ
            def smart_filter_content(content, max_length=50000):
                """ì¡°ë¡€ì™€ ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ì„ ìš°ì„  ì¶”ì¶œ"""
                lines = content.split('\n')
                
                # ì¡°ë¡€ ê´€ë ¨ í‚¤ì›Œë“œ (ë„ë¡œêµí†µë²• ê´€ë ¨)
                priority_keywords = [
                    'ì‹œì¥', 'êµ°ìˆ˜', 'êµ¬ì²­ì¥', 'ì§€ë°©ìì¹˜ë‹¨ì²´', 'ì¡°ë¡€', 'ì‹œë„', 'ì‹œêµ°êµ¬',
                    'ìœ„ì„', 'ìœ„íƒ', 'ê¶Œí•œ', 'ì‚¬ë¬´', 'ì‹ ê³ ', 'í—ˆê°€', 'ìŠ¹ì¸', 'ì§€ì •',
                    'ì£¼ì°¨', 'ì •ì°¨', 'ê¸ˆì§€', 'ì œí•œ', 'êµ¬ì—­', 'ì‹œì„¤', 'ì„¤ì¹˜', 'ê´€ë¦¬'
                ]
                
                # ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¼ì¸ ë¶„ë¥˜
                high_priority = []
                medium_priority = []
                low_priority = []
                
                for line in lines:
                    line_lower = line.lower()
                    priority_count = sum(1 for keyword in priority_keywords if keyword in line_lower)
                    
                    if priority_count >= 2:
                        high_priority.append(line)
                    elif priority_count >= 1:
                        medium_priority.append(line)
                    else:
                        low_priority.append(line)
                
                # ìš°ì„ ìˆœìœ„ë³„ë¡œ ê²°í•©
                filtered_content = []
                current_length = 0
                
                # 1ë‹¨ê³„: ë†’ì€ ìš°ì„ ìˆœìœ„
                for line in high_priority:
                    if current_length + len(line) < max_length:
                        filtered_content.append(line)
                        current_length += len(line)
                    else:
                        break
                
                # 2ë‹¨ê³„: ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                for line in medium_priority:
                    if current_length + len(line) < max_length:
                        filtered_content.append(line)
                        current_length += len(line)
                    else:
                        break
                
                # 3ë‹¨ê³„: ë‚®ì€ ìš°ì„ ìˆœìœ„ (ê³µê°„ì´ ë‚¨ìœ¼ë©´)
                for line in low_priority:
                    if current_length + len(line) < max_length:
                        filtered_content.append(line)
                        current_length += len(line)
                    else:
                        break
                
                result = '\n'.join(filtered_content)
                if len(content) > len(result):
                    result += "\n\n[... ì¡°ë¡€ ê´€ë ¨ì„±ì´ ë†’ì€ ë¶€ë¶„ì„ ìš°ì„  í‘œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤ ...]"
                
                return result
            
            # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì ìš© (Gemini 2.0 flash expëŠ” ë” í° ì»¨í…ìŠ¤íŠ¸ ì§€ì›)
            max_length = 80000
            if len(upper_law_text) > max_length:
                truncated_text = smart_filter_content(upper_law_text, max_length)
                st.warning(f"[DEBUG] ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ì–´ ì¡°ë¡€ ê´€ë ¨ ë¶€ë¶„ì„ ìš°ì„ í•˜ì—¬ {len(truncated_text):,}ìë¡œ ì¶•ì•½í–ˆìŠµë‹ˆë‹¤ (ì›ë³¸: {len(upper_law_text):,}ì)")
            else:
                truncated_text = upper_law_text.strip()
            
            # ëª¨ë“  ì¡°ë¬¸ì„ í•˜ë‚˜ì˜ ì—°ê²°ëœ ë³¸ë¬¸ìœ¼ë¡œ ì²˜ë¦¬
            result = {
                'law_name': exact_law_name,
                'law_id': law_id,
                'content': truncated_text
            }
            
            return result
        else:
            st.warning("[DEBUG] ì¡°ë¬¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
            return get_superior_law_content_xml_fallback(law_name)
        
    except Exception as e:
        st.error(f"[DEBUG] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return get_superior_law_content_xml_fallback(law_name)

def get_superior_law_content_xml_fallback(law_name):
    """XML ë°©ì‹ í´ë°± (ê°„ì†Œí™” ë²„ì „)"""
    try:
        st.write(f"[DEBUG XML] XML í´ë°± ëª¨ë“œ ì‹œì‘: {law_name}")
        
        search_params = {
            'OC': OC, 
            'target': 'law',
            'type': 'XML',
            'query': law_name,
            'display': 5,
            'search': 1
        }
        
        st.write(f"[DEBUG XML] XML ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: {search_params}")
        
        search_response = requests.get(search_url, params=search_params, timeout=30)
        st.write(f"[DEBUG XML] XML ê²€ìƒ‰ ì‘ë‹µ ìƒíƒœ: {search_response.status_code}")
        st.write(f"[DEBUG XML] ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 1000ì): {search_response.text[:1000]}")
        
        if search_response.status_code != 200:
            st.error(f"[DEBUG XML] XML API í˜¸ì¶œ ì‹¤íŒ¨: HTTP {search_response.status_code}")
            return None
            
        if not search_response.text.strip():
            st.error("[DEBUG XML] XML ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return None
            
        try:
            search_root = ET.fromstring(search_response.text)
        except ET.ParseError as xml_err:
            st.error(f"[DEBUG XML] XML íŒŒì‹± ì‹¤íŒ¨: {xml_err}")
            st.write(f"[DEBUG XML] ì›ë³¸ ì‘ë‹µ: {search_response.text}")
            return None
        st.write(f"[DEBUG XML] XML íŒŒì‹± ì™„ë£Œ")
        
        law_id = None
        exact_law_name = None
        
        for law in search_root.findall('.//law'):
            found_name = law.find('ë²•ë ¹ëª…').text if law.find('ë²•ë ¹ëª…') is not None else ""
            found_id = law.find('ë²•ë ¹ID').text if law.find('ë²•ë ¹ID') is not None else None
            
            if found_name == law_name or (law_name in found_name):
                law_id = found_id
                exact_law_name = found_name
                break
        
        if not law_id:
            return None
        
        detail_params = {
            'OC': OC,
            'target': 'law', 
            'ID': law_id,
            'type': 'XML'
        }
        
        detail_response = requests.get(detail_url, params=detail_params, timeout=30)
        detail_root = ET.fromstring(detail_response.text)
        
        articles = []
        for article in detail_root.findall('.//ì¡°'):
            article_num = article.find('ì¡°ë¬¸ë²ˆí˜¸').text if article.find('ì¡°ë¬¸ë²ˆí˜¸') is not None else ""
            article_title = article.find('ì¡°ë¬¸ì œëª©').text if article.find('ì¡°ë¬¸ì œëª©') is not None else ""
            article_content = article.find('ì¡°ë¬¸ë‚´ìš©').text if article.find('ì¡°ë¬¸ë‚´ìš©') is not None else ""
            
            if article_content:
                article_content = article_content.replace('<![CDATA[', '').replace(']]>', '')
                article_content = article_content.replace('<p>', '').replace('</p>', '\n')
                article_content = article_content.replace('<br/>', '\n').replace('<br>', '\n')
                article_content = article_content.replace('&nbsp;', ' ')
                article_content = article_content.strip()
                
                if article_content:
                    articles.append({
                        'number': article_num,
                        'title': article_title,
                        'content': article_content
                    })
        
        return {
            'law_name': exact_law_name,
            'law_id': law_id,
            'articles': articles
        }
        
    except Exception as e:
        st.error(f"[DEBUG XML] XML í´ë°± ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return None

# ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ìƒˆ XML ë°©ì‹ìœ¼ë¡œ êµì²´
def get_superior_law_content(law_name):
    """ìƒìœ„ë²•ë ¹ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (XML ë°©ì‹)"""
    return get_superior_law_content_xml(law_name)

def normalize_law_name(law_name):
    """ë²•ë ¹ëª…ì„ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì œê±°"""
    import re

    # 1. ê¸°ë³¸ ì •ë¦¬: ì•ë’¤ ê³µë°± ì œê±°
    normalized = law_name.strip()

    # 2. ê³¼ë„í•œ ë„ì–´ì“°ê¸° ì œê±° (2ê°œ ì´ìƒì˜ ê³µë°±ì„ 1ê°œë¡œ)
    normalized = re.sub(r'\s+', ' ', normalized)

    # 3. íŠ¹ì • íŒ¨í„´ ì •ê·œí™”
    # "ê´€ê´‘ì§„í¥ ë²•" -> "ê´€ê´‘ì§„í¥ë²•"
    normalized = re.sub(r'(\w+)\s+(ë²•|ë ¹|ê·œì¹™)$', r'\1\2', normalized)

    # 4. íê´‘ì§€ì—­ê°œë°œì§€ì› ê´€ë ¨ ë²•ë ¹ ì •ê·œí™”
    if 'íê´‘ì§€' in normalized or 'ì—­ê°œë°œ' in normalized:
        if 'íŠ¹ë³„ë²•' in normalized:
            normalized = "íê´‘ì§€ì—­ê°œë°œì§€ì›ì—ê´€í•œíŠ¹ë³„ë²•"

    # 5. ë„ˆë¬´ ì§§ì€ ë²•ë ¹ëª… ì œê±° (3ê¸€ì ì´í•˜)
    if len(normalized) <= 3:
        return None

    # 6. ëª…í™•íˆ ì˜ëª»ëœ ì¶”ì¶œ ì œê±°
    invalid_patterns = [
        r'^í•œíŠ¹ë³„ë²•$',  # "í•œíŠ¹ë³„ë²•"
        r'^\w{1,2}íŠ¹ë³„ë²•$',  # ë„ˆë¬´ ì§§ì€ íŠ¹ë³„ë²•
    ]

    for pattern in invalid_patterns:
        if re.match(pattern, normalized):
            return None

    return normalized

def group_laws_by_hierarchy(superior_laws):
    """ë²•ë ¹ì„ ê³„ì¸µë³„ë¡œ ê·¸ë£¹í™”í•˜ëŠ” í•¨ìˆ˜ (ì •ê·œí™” ì ìš©)"""
    law_groups = {}

    # 1ë‹¨ê³„: ë²•ë ¹ëª… ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
    normalized_laws = set()
    for law_name in superior_laws:
        normalized = normalize_law_name(law_name)
        if normalized:  # Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            normalized_laws.add(normalized)

    if len(superior_laws) != len(normalized_laws):
        import streamlit as st
        st.info(f"ğŸ”§ ë²•ë ¹ëª… ì •ê·œí™”: {len(superior_laws)}ê°œ â†’ {len(normalized_laws)}ê°œë¡œ ì¤‘ë³µ ì œê±°")

        # ì œê±°ëœ ì¤‘ë³µ ë²•ë ¹ í‘œì‹œ
        removed_laws = []
        for original in superior_laws:
            normalized = normalize_law_name(original)
            if not normalized or (normalized != original and normalized in normalized_laws):
                removed_laws.append(original)

        if removed_laws:
            with st.expander("ğŸ—‘ï¸ ì œê±°ëœ ì¤‘ë³µ/ì˜ëª»ëœ ë²•ë ¹ëª…", expanded=False):
                for removed in removed_laws:
                    st.markdown(f"- {removed}")

    # ì •ê·œí™” ê³¼ì • ë¡œê¹…
    for original in superior_laws:
        normalized = normalize_law_name(original)
        if normalized != original:
            pass  # ì •ê·œí™”ëœ ê²½ìš° ì²˜ë¦¬ (ë””ë²„ê¹… ì½”ë“œ ì œê±°ë¨)

    # 2ë‹¨ê³„: ì •ê·œí™”ëœ ë²•ë ¹ëª…ìœ¼ë¡œ ê·¸ë£¹í™”
    for law_name in normalized_laws:
        # ê¸°ë³¸ ë²•ë ¹ëª… ì¶”ì¶œ (ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ ì œê±°)
        base_name = law_name
        law_type = 'law'  # ê¸°ë³¸ê°’: ë²•ë¥ 
        
        if 'ì‹œí–‰ê·œì¹™' in law_name:
            base_name = law_name.replace(' ì‹œí–‰ê·œì¹™', '').replace('ì‹œí–‰ê·œì¹™', '')
            law_type = 'rule'
        elif 'ì‹œí–‰ë ¹' in law_name:
            base_name = law_name.replace(' ì‹œí–‰ë ¹', '').replace('ì‹œí–‰ë ¹', '')
            law_type = 'decree'
        elif law_name.endswith('ë ¹') and not law_name.endswith('ë²•ë ¹'):
            law_type = 'decree'
        elif law_name.endswith('ê·œì¹™'):
            law_type = 'rule'
            
        # ê·¸ë£¹ì— ì¶”ê°€
        if base_name not in law_groups:
            law_groups[base_name] = {'law': None, 'decree': None, 'rule': None}
        
        law_groups[base_name][law_type] = law_name
    
    return law_groups

def get_all_superior_laws_content(superior_laws):
    """ëª¨ë“  ìƒìœ„ë²•ë ¹ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ - ê³„ì¸µë³„ ê·¸ë£¹í™”"""
    superior_laws_content = []
    
    if not superior_laws:
        return superior_laws_content
    
    # 1ë‹¨ê³„: ë²•ë ¹ì„ ê³„ì¸µë³„ë¡œ ê·¸ë£¹í™”
    law_groups = group_laws_by_hierarchy(superior_laws)
    
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_laws = sum(1 for laws in law_groups.values() for law in laws.values() if law is not None)
    current_idx = 0
    
    # 2ë‹¨ê³„: ê° ê·¸ë£¹ì˜ ëª¨ë“  ê³„ì¸µ ìˆ˜ì§‘
    for base_name, laws in law_groups.items():
        group_content = {
            'base_name': base_name,
            'laws': {},
            'combined_articles': []
        }
        
        # ë²•ë¥  â†’ ì‹œí–‰ë ¹ â†’ ì‹œí–‰ê·œì¹™ ìˆœì„œë¡œ ìˆ˜ì§‘
        for law_type in ['law', 'decree', 'rule']:
            law_name = laws[law_type]
            if law_name:
                current_idx += 1
                status_text.text(f"ìƒìœ„ë²•ë ¹ ì¡°íšŒ ì¤‘... {law_name} ({current_idx}/{total_laws})")
                progress_bar.progress(current_idx / total_laws)
                
                law_content = get_superior_law_content(law_name)
                if law_content:
                    group_content['laws'][law_type] = law_content
                    # ìƒˆë¡œìš´ ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬: contentê°€ ìˆìœ¼ë©´ ì‚¬ìš©, articlesê°€ ìˆìœ¼ë©´ ë³€í™˜
                    if 'content' in law_content:
                        # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì €ì¥
                        if 'combined_content' not in group_content:
                            group_content['combined_content'] = ""
                        group_content['combined_content'] += law_content['content'] + '\n'
                    elif 'articles' in law_content:
                        # ê¸°ì¡´ articles êµ¬ì¡°ê°€ ìˆìœ¼ë©´ ë³€í™˜
                        group_content['combined_articles'].extend(law_content['articles'])
        
        if group_content['laws']:  # í•˜ë‚˜ ì´ìƒì˜ ë²•ë ¹ì´ ìˆ˜ì§‘ëœ ê²½ìš°ë§Œ ì¶”ê°€
            superior_laws_content.append(group_content)
    
    progress_bar.empty()
    status_text.empty()
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (8ë§Œì) ë° ê´€ë ¨ì„± í•„í„°ë§
    max_chars = 80000
    total_chars = 0
    
    # ê° ë²•ë ¹ ê·¸ë£¹ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
    for group in superior_laws_content:
        group_chars = 0
        
        # combined_contentê°€ ìˆëŠ” ê²½ìš°
        if 'combined_content' in group and group['combined_content']:
            group_chars += len(group['combined_content'])
        
        # combined_articlesê°€ ìˆëŠ” ê²½ìš°
        if 'combined_articles' in group and group['combined_articles']:
            for article in group['combined_articles']:
                group_chars += len(article.get('content', ''))
        
        # laws êµ¬ì¡°ê°€ ìˆëŠ” ê²½ìš°
        if 'laws' in group and group['laws']:
            for law_type, law_info in group['laws'].items():
                if law_info and 'articles' in law_info:
                    for article in law_info['articles']:
                        group_chars += len(article.get('content', ''))
        
        group['text_length'] = group_chars
        total_chars += group_chars
        
    
    # 8ë§Œìë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ëª¨ë“  ë‚´ìš© ìœ ì§€ (í•„í„°ë§ ë¹„í™œì„±í™”)
    if total_chars > max_chars:
        st.warning(f"âš ï¸ ë²•ë ¹ ë‚´ìš©ì´ {total_chars:,}ìë¡œ 8ë§Œìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë“  ë‚´ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
        st.info("ğŸ’¡ Geminiê°€ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§í•˜ì§€ ì•Šê³  ì „ì²´ ë‚´ìš©ì„ ì „ë‹¬í•©ë‹ˆë‹¤.")
    
    st.success(f"âœ… ì´ {len(superior_laws_content)}ê°œ ë²•ë ¹ ê·¸ë£¹, {total_chars:,}ìë¥¼ Geminiì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.")
    
    return superior_laws_content

def chunk_text(text, chunk_size=1000, overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = min(start + chunk_size, text_length)
        chunk = text[start:end]
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ëë‚˜ë„ë¡ ì¡°ì •
        if end < text_length:
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            last_break = max(last_period, last_newline)
            if last_break > start + chunk_size * 0.7:  # ë„ˆë¬´ ì§§ì§€ ì•Šìœ¼ë©´ ì¡°ì •
                end = start + last_break + 1
                chunk = text[start:end]
        
        if chunk.strip():
            chunks.append({
                'text': chunk.strip(),
                'start': start,
                'end': end
            })
        
        start = end - overlap
    
    return chunks

def get_gemini_embedding(text, api_key):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        genai.configure(api_key=api_key)
        result = genai.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

def create_vector_store(pdf_path, api_key):
    """ìì¹˜ë²•ê·œ ê°€ì´ë“œ PDFë¥¼ ë²¡í„°ìŠ¤í† ì–´ë¡œ ë³€í™˜ (pickle ë°©ì‹)"""
    try:
        vector_store_path = "jachi_guide_2022_vectorstore.pkl"
        
        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        if os.path.exists(vector_store_path):
            with open(vector_store_path, 'rb') as f:
                vector_store = pickle.load(f)
            st.info("âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return vector_store
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            full_text = ''
            for page in reader.pages:
                full_text += page.extract_text() + '\n'
        
        # í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = chunk_text(full_text)
        st.info(f"ğŸ“„ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
        
        # ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ê³  ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        documents = []
        embeddings = []
        metadatas = []
        
        for i, chunk in enumerate(chunks):
            status_text.text(f"ì„ë² ë”© ìƒì„± ì¤‘... ({i+1}/{len(chunks)})")
            progress_bar.progress((i + 1) / len(chunks))
            
            # Geminië¡œ ì„ë² ë”© ìƒì„±
            embedding = get_gemini_embedding(chunk['text'], api_key)
            if embedding:
                documents.append(chunk['text'])
                embeddings.append(embedding)
                metadatas.append({
                    'start': chunk['start'],
                    'end': chunk['end'],
                    'page': 'guide_2022',
                    'chunk_id': i
                })
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vector_store = {
            'documents': documents,
            'embeddings': np.array(embeddings),
            'metadatas': metadatas,
            'created_at': datetime.now().isoformat()
        }
        
        # pickleë¡œ ì €ì¥
        if documents:
            with open(vector_store_path, 'wb') as f:
                pickle.dump(vector_store, f)
            st.success(f"âœ… {len(documents)}ê°œ ì²­í¬ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        progress_bar.empty()
        status_text.empty()
        
        return vector_store
        
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

def is_valid_text(text):
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬"""
    if not text or len(text.strip()) < 10:
        return False

    # í•œê¸€ ê¹¨ì§ ê²€ì‚¬ (ê¹¨ì§„ ë¬¸ì ë¹„ìœ¨ì´ 30% ì´ìƒì´ë©´ ì œì™¸)
    broken_chars = sum(1 for char in text if ord(char) > 55000)  # í•œê¸€ ê¹¨ì§ ë¬¸ì ë²”ìœ„
    if len(text) > 0 and broken_chars / len(text) > 0.3:
        return False

    # ì ì„  ê³¼ë‹¤ ê²€ì‚¬ (ì ì„ ì´ 50% ì´ìƒì´ë©´ ì œì™¸)
    dot_chars = text.count('Â·') + text.count('â€¦') + text.count('.')
    if len(text) > 0 and dot_chars / len(text) > 0.5:
        return False

    # ë°˜ë³µ ë¬¸ì ê³¼ë‹¤ ê²€ì‚¬
    import re
    repeated_patterns = re.findall(r'(.)\1{10,}', text)  # ê°™ì€ ë¬¸ìê°€ 10ë²ˆ ì´ìƒ ë°˜ë³µ
    if repeated_patterns:
        return False

    return True

def clean_text_content(text):
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    import re

    # 1. ê³¼ë„í•œ ì ì„  ì œê±°
    text = re.sub(r'[Â·â€¦]{3,}', ' ', text)
    text = re.sub(r'\.{3,}', ' ', text)

    # 2. ê³¼ë„í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    # 3. í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±°
    text = re.sub(r'\b\d+\s*í˜ì´ì§€?\b', '', text)
    text = re.sub(r'\b\d+\s*ìª½?\b', '', text)

    # 4. ëª©ì°¨ ê´€ë ¨ íŒ¨í„´ ì œê±°
    text = re.sub(r'^[IVX]+\.?\s*', '', text, flags=re.MULTILINE)  # ë¡œë§ˆìˆ«ì
    text = re.sub(r'^\d+\.?\s*$', '', text, flags=re.MULTILINE)   # ë‹¨ë… ìˆ«ì

    # 5. ë°˜ë³µë˜ëŠ” íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = re.sub(r'[~`!@#$%^&*()_+=\[\]{}|\\:";\'<>?/,-]{5,}', ' ', text)

    return text.strip()

def extract_legal_reasoning_from_analysis(analysis_text):
    """Gemini ë¶„ì„ ê²°ê³¼ì—ì„œ ë²•ì  ê·¼ê±°ì™€ ë…¼ë¦¬ ì¶”ì¶œ"""
    import re

    extracted_context = {
        'legal_basis': [],      # ë²•ì  ê·¼ê±° (ë²•ë ¹, ì¡°í•­)
        'reasoning': [],        # ì¶”ë¡  ê³¼ì •
        'key_concepts': [],     # í•µì‹¬ ê°œë…
        'problem_details': []   # êµ¬ì²´ì ì¸ ë¬¸ì œì 
    }

    # 1. ë²•ë ¹ ë° ì¡°í•­ ì¶”ì¶œ
    legal_references = re.findall(r'(?:ì§€ë°©ìì¹˜ë²•|í—Œë²•|í–‰ì •ê¸°ë³¸ë²•|ê±´ì¶•ë²•|ë„ì‹œê³„íšë²•)\s*(?:ì œ\s*\d+ì¡°?(?:ì˜?\d+)?)?', analysis_text)
    extracted_context['legal_basis'].extend(legal_references)

    # 2. ë²•ì  ì›ì¹™/ê°œë… ì¶”ì¶œ
    legal_concepts = [
        'ê¸°ê´€ìœ„ì„ì‚¬ë¬´', 'ìì¹˜ì‚¬ë¬´', 'êµ­ê°€ì‚¬ë¬´', 'ë²•ë¥ ìœ ë³´ì›ì¹™', 'ê¶Œí•œë°°ë¶„',
        'ìƒìœ„ë²•ë ¹', 'ë²•ë ¹ìš°ìœ„', 'ì¡°ë¡€ì œì •ê¶Œ', 'ìœ„ì„ì…ë²•', 'ì²˜ë¶„ê¶Œí•œ',
        'í—Œë²•ìœ„ë°˜', 'ê¸°ë³¸ê¶Œì¹¨í•´', 'í‰ë“±ì›ì¹™', 'ë¹„ë¡€ì›ì¹™', 'ì‹ ë¢°ë³´í˜¸',
        'ì¬ì‚°ê¶Œì¹¨í•´', 'ì˜ì—…ì˜ììœ ', 'ê±°ì£¼ì´ì „ì˜ììœ ', 'í‘œí˜„ì˜ììœ ',
        'ì¡°ì„¸ë²•ë¥ ì£¼ì˜', 'ì£„í˜•ë²•ì •ì£¼ì˜', 'ì ë²•ì ˆì°¨', 'ì •ë‹¹í•œë³´ìƒ'
    ]

    for concept in legal_concepts:
        if concept in analysis_text:
            # í•´ë‹¹ ê°œë… ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ (ì•ë’¤ 50ì)
            matches = re.finditer(re.escape(concept), analysis_text)
            for match in matches:
                start = max(0, match.start() - 50)
                end = min(len(analysis_text), match.end() + 50)
                context = analysis_text[start:end].strip()
                extracted_context['key_concepts'].append({
                    'concept': concept,
                    'context': context
                })

    # 3. ë¬¸ì œì  ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
    problem_patterns = [
        r'ë¬¸ì œ(?:ì |ê°€|ëŠ”)[^.]*?(?:\.|$)',
        r'ìœ„ë²•[^.]*?(?:\.|$)',
        r'ìœ„ë°˜[^.]*?(?:\.|$)',
        r'ë¶€ì ì ˆ[^.]*?(?:\.|$)',
        r'í•œê³„[^.]*?(?:\.|$)'
    ]

    for pattern in problem_patterns:
        matches = re.findall(pattern, analysis_text, re.DOTALL)
        extracted_context['problem_details'].extend(matches)

    # 4. ì¶”ë¡  ê³¼ì • ì¶”ì¶œ (ë”°ë¼ì„œ, ê·¸ëŸ¬ë¯€ë¡œ, ì™œëƒí•˜ë©´ ë“±)
    reasoning_patterns = [
        r'(?:ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ì™œëƒí•˜ë©´|ì´ëŠ”|ì´ì— ë”°ë¼)[^.]*?(?:\.|$)',
        r'(?:ê·¼ê±°|ì´ìœ |ì›ì¸)ëŠ”[^.]*?(?:\.|$)'
    ]

    for pattern in reasoning_patterns:
        matches = re.findall(pattern, analysis_text, re.DOTALL)
        extracted_context['reasoning'].extend(matches)

    # 5. ë””ë²„ê¹…ìš© ì¶œë ¥

    return extracted_context

def search_precedents(query_keywords, max_results=10):
    """êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° APIë¥¼ í†µí•œ íŒë¡€ ê²€ìƒ‰"""
    try:
        # ğŸ†• ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™” ('ì¡°ë¡€' ë‹¨ë… ê²€ìƒ‰ ìš°ì„ )
        # ë¨¼ì € 'ì¡°ë¡€'ë§Œìœ¼ë¡œ ê²€ìƒ‰ì„ ì‹œë„í•˜ê³ , í•„ìš”ì‹œ ê°œë³„ í‚¤ì›Œë“œ ì¶”ê°€ ê²€ìƒ‰
        search_query = "ì¡°ë¡€"

        # API ìš”ì²­ íŒŒë¼ë¯¸í„°
        params = {
            'OC': OC,
            'target': 'prec',  # íŒë¡€ ê²€ìƒ‰
            'type': 'XML',
            'query': search_query,
            'display': min(max_results, 20)  # ìµœëŒ€ 20ê°œ
        }

        st.info(f"ğŸ” íŒë¡€ ê²€ìƒ‰ ì¤‘: '{search_query}'")

        response = requests.get(precedent_search_url, params=params, timeout=30)
        if response.status_code != 200:
            st.warning(f"íŒë¡€ ê²€ìƒ‰ API ì˜¤ë¥˜: HTTP {response.status_code}")
            return []

        root = ET.fromstring(response.text)
        precedents = []

        # ğŸ†• XML ì‘ë‹µ íŒŒì‹± (ì˜¬ë°”ë¥¸ êµ¬ì¡° ì‚¬ìš©)
        # ë£¨íŠ¸ê°€ PrecSearchì´ë¯€ë¡œ, prec íƒœê·¸ë¥¼ ì§ì ‘ ì°¾ìŒ
        for prec_elem in root.findall('prec'):
            try:
                prec_id = prec_elem.find('íŒë¡€ì¼ë ¨ë²ˆí˜¸')
                case_name = prec_elem.find('ì‚¬ê±´ëª…')
                court = prec_elem.find('ë²•ì›ëª…')
                date = prec_elem.find('ì„ ê³ ì¼ì')
                case_type = prec_elem.find('ì‚¬ê±´ì¢…ë¥˜ëª…')

                if all(elem is not None for elem in [prec_id, case_name]):
                    precedent = {
                        'id': prec_id.text,
                        'case_name': case_name.text,
                        'court': court.text if court is not None else '',
                        'date': date.text if date is not None else '',
                        'case_type': case_type.text if case_type is not None else '',
                        'summary': ''  # ìš”ì•½ë¬¸ì€ ìƒì„¸ ì¡°íšŒì—ì„œ ê°€ì ¸ì˜¬ ì˜ˆì •
                    }
                    precedents.append(precedent)
            except Exception as e:
                continue

        st.success(f"ğŸ“‹ {len(precedents)}ê°œì˜ ê´€ë ¨ íŒë¡€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        # ğŸ†• ì¶”ê°€ í‚¤ì›Œë“œë³„ ê²€ìƒ‰ (OR ë°©ì‹ êµ¬í˜„)
        if isinstance(query_keywords, list) and len(query_keywords) > 0:
            st.info("ğŸ”„ ê° í‚¤ì›Œë“œë³„ë¡œ ê´€ë ¨ íŒë¡€ë¥¼ ì¶”ê°€ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

            # ê° í‚¤ì›Œë“œë³„ë¡œ ê°œë³„ ê²€ìƒ‰ ìˆ˜í–‰
            additional_precedents = []
            search_keywords = [k for k in query_keywords[:3] if k.strip()]  # ë¹ˆ ë¬¸ìì—´ ì œê±°

            for keyword in search_keywords:
                try:
                    # 'ì¡°ë¡€ + í‚¤ì›Œë“œ' ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰
                    combined_query = f"ì¡°ë¡€ {keyword}"
                    st.info(f"ğŸ” í‚¤ì›Œë“œë³„ ê²€ìƒ‰: '{combined_query}'")

                    keyword_params = params.copy()
                    keyword_params['query'] = combined_query
                    keyword_params['display'] = 3  # ê° í‚¤ì›Œë“œë‹¹ 3ê°œì”©

                    keyword_response = requests.get(precedent_search_url, params=keyword_params, timeout=15)
                    if keyword_response.status_code == 200:
                        keyword_root = ET.fromstring(keyword_response.text)
                        keyword_precs = keyword_root.findall('prec')

                        st.info(f"   â†’ '{keyword}' í‚¤ì›Œë“œë¡œ {len(keyword_precs)}ê°œ íŒë¡€ ë°œê²¬")

                        for prec_elem in keyword_precs:
                            try:
                                prec_id = prec_elem.find('íŒë¡€ì¼ë ¨ë²ˆí˜¸')
                                case_name = prec_elem.find('ì‚¬ê±´ëª…')

                                if prec_id is not None and case_name is not None:
                                    # ì¤‘ë³µ ì œê±°
                                    all_existing_ids = [p['id'] for p in precedents + additional_precedents]

                                    if prec_id.text not in all_existing_ids:
                                        court = prec_elem.find('ë²•ì›ëª…')
                                        date = prec_elem.find('ì„ ê³ ì¼ì')
                                        case_type = prec_elem.find('ì‚¬ê±´ì¢…ë¥˜ëª…')

                                        additional_precedent = {
                                            'id': prec_id.text,
                                            'case_name': case_name.text,
                                            'court': court.text if court is not None else '',
                                            'date': date.text if date is not None else '',
                                            'case_type': case_type.text if case_type is not None else '',
                                            'summary': ''
                                        }
                                        additional_precedents.append(additional_precedent)

                            except Exception as e:
                                continue

                except Exception as e:
                    st.warning(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            if additional_precedents:
                st.success(f"âœ… ì¶”ê°€ë¡œ {len(additional_precedents)}ê°œì˜ íŒë¡€ë¥¼ ë” ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                precedents.extend(additional_precedents)
            else:
                st.info("ì¶”ê°€ ê²€ìƒ‰ì—ì„œëŠ” ìƒˆë¡œìš´ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        return precedents[:max_results]

    except Exception as e:
        st.error(f"íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def get_precedent_detail(precedent_id):
    """íŒë¡€ ìƒì„¸ ë‚´ìš© ì¡°íšŒ"""
    try:
        params = {
            'OC': OC,
            'target': 'prec',
            'ID': precedent_id,
            'type': 'XML'
        }

        response = requests.get(detail_url, params=params, timeout=30)
        if response.status_code != 200:
            return None

        root = ET.fromstring(response.text)

        # íŒë¡€ ë³¸ë¬¸ ì¶”ì¶œ
        content = ""

        # íŒì‹œì‚¬í•­
        decision_matters = root.find('.//íŒì‹œì‚¬í•­')
        if decision_matters is not None and decision_matters.text:
            content += f"[íŒì‹œì‚¬í•­]\n{decision_matters.text}\n\n"

        # íŒê²°ìš”ì§€
        decision_summary = root.find('.//íŒê²°ìš”ì§€')
        if decision_summary is not None and decision_summary.text:
            content += f"[íŒê²°ìš”ì§€]\n{decision_summary.text}\n\n"

        # ì°¸ì¡°ì¡°ë¬¸
        ref_articles = root.find('.//ì°¸ì¡°ì¡°ë¬¸')
        if ref_articles is not None and ref_articles.text:
            content += f"[ì°¸ì¡°ì¡°ë¬¸]\n{ref_articles.text}\n\n"

        # ì „ë¬¸ (ì£¼ìš” ë¶€ë¶„ë§Œ)
        full_text = root.find('.//ì „ë¬¸')
        if full_text is not None and full_text.text:
            # ì „ë¬¸ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° ì•ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´
            full_content = full_text.text
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            content += f"[ì „ë¬¸]\n{full_content}\n\n"

        return content.strip() if content else None

    except Exception as e:
        st.warning(f"íŒë¡€ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

def extract_legal_principles_from_precedents(precedents_content):
    """íŒë¡€ì—ì„œ ë²•ë¦¬ ì¶”ì¶œ"""
    legal_principles = []

    for i, content in enumerate(precedents_content):
        if not content:
            continue

        # ë²•ë¦¬ ì¶”ì¶œ íŒ¨í„´
        principle_patterns = [
            r'ë²•ì›ì€.*?ê³  íŒì‹œí•˜ì˜€ë‹¤',
            r'ëŒ€ë²•ì›ì€.*?ê³  ë³¸ë‹¤',
            r'ì´ ì‚¬ê±´ì— ê´€í•˜ì—¬.*?ê²ƒì´ë‹¤',
            r'ë”°ë¼ì„œ.*?í•  ê²ƒì´ë‹¤',
            r'ê·¸ëŸ¬ë¯€ë¡œ.*?ë¼ê³  í•  ê²ƒì´ë‹¤',
            r'í—Œë²•ì¬íŒì†ŒëŠ”.*?ê³  íŒë‹¨í•œë‹¤'
        ]

        extracted_principles = []
        for pattern in principle_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                # ë¬¸ì¥ ì •ë¦¬
                clean_principle = re.sub(r'\s+', ' ', match.strip())
                if len(clean_principle) > 50 and clean_principle not in extracted_principles:
                    extracted_principles.append(clean_principle)

        if extracted_principles:
            legal_principles.extend(extracted_principles[:2])  # íŒë¡€ë‹¹ ìµœëŒ€ 2ê°œ ë²•ë¦¬

    return legal_principles[:6]  # ì „ì²´ ìµœëŒ€ 6ê°œ ë²•ë¦¬

def search_relevant_guidelines(query, vector_store, api_key=None, top_k=3):
    """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ (Gemini ê¸°ë°˜ ë˜ëŠ” ë¬´ë£Œ ë²„ì „)"""
    try:

        if not vector_store or 'embeddings' not in vector_store:
            return []


        # ë²¡í„°ìŠ¤í† ì–´ íƒ€ì… í™•ì¸ (Gemini ê¸°ë°˜ vs ë¬´ë£Œ ë²„ì „)
        is_free_version = 'model_name' in vector_store and isinstance(vector_store['model_name'], str)

        if is_free_version:
            # ë¬´ë£Œ sentence-transformers ê¸°ë°˜ ê²€ìƒ‰
            try:
                from sentence_transformers import SentenceTransformer
                model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                query_embedding = model.encode([query])[0]
            except ImportError:
                st.warning("sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return []
        else:
            # Gemini ê¸°ë°˜ ê²€ìƒ‰
            if not api_key:
                return []
            query_embedding = get_gemini_embedding(query, api_key)
            if not query_embedding:
                return []

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        query_embedding = np.array(query_embedding).reshape(1, -1)
        similarities = cosine_similarity(query_embedding, vector_store['embeddings'])[0]

        # ìµœì†Œ ìœ ì‚¬ë„ í•„í„°ë§ (ê¸°ì¤€ ìƒí–¥)
        min_similarity = 0.5 if is_free_version else 0.3  # ê¸°ì¤€ì„ ë†’ì—¬ì„œ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ë§Œ
        valid_indices = np.where(similarities >= min_similarity)[0]

        # ì¶”ê°€ì ìœ¼ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ê²€ì‚¬ (ê°œì„ ëœ ë²„ì „)
        keyword_filtered_indices = []
        law_keywords = ['ì¡°ë¡€', 'ë²•ë¥ ', 'ê·œì •', 'ìœ„ë°˜', 'ìœ„ë²•', 'í—ˆê°€', 'ìŠ¹ì¸', 'ì‚¬ë¬´', 'ê¶Œí•œ', 'ê¸°ê´€ìœ„ì„', 'ì¬ì˜', 'ì œì†Œ', 'ì˜ê²°', 'ëŒ€ë²•ì›', 'íŒë¡€']

        for idx in valid_indices:
            try:
                text = vector_store['documents'][idx]
                # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆê³  ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
                korean_chars = sum(1 for char in text if '\uac00' <= char <= '\ud7af')

                if korean_chars >= 10:  # ìµœì†Œ 10ê°œ ì´ìƒì˜ í•œê¸€ì´ ìˆì–´ì•¼ í•¨
                    text_lower = text.lower()
                    keyword_count = sum(1 for keyword in law_keywords if keyword in text_lower)

                    # ì¡°ê±´ ì™„í™”: 1ê°œ ì´ìƒì˜ í‚¤ì›Œë“œë§Œ ìˆì–´ë„ í¬í•¨
                    if keyword_count >= 1:
                        keyword_filtered_indices.append(idx)
                    else:
                        pass  # í‚¤ì›Œë“œ ë¶€ì¡±í•œ ê²½ìš° (ë””ë²„ê¹… ì½”ë“œ ì œê±°ë¨)
                else:
                    pass  # í•œê¸€ ë¶€ì¡±í•œ ê²½ìš° (ë””ë²„ê¹… ì½”ë“œ ì œê±°ë¨)

            except Exception as e:
                continue

        if keyword_filtered_indices:
            valid_indices = np.array(keyword_filtered_indices)
        else:
            pass  # í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (ë””ë²„ê¹… ì½”ë“œ ì œê±°ë¨)

        if len(valid_indices) == 0:
            return []

        # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
        valid_similarities = similarities[valid_indices]
        top_indices = valid_indices[np.argsort(valid_similarities)[-top_k:][::-1]]
        
        relevant_chunks = []
        for idx in top_indices:
            original_text = vector_store['documents'][idx]

            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬ ë° í•„í„°ë§
            if not is_valid_text(original_text):
                continue

            # í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_text = clean_text_content(original_text)

            if len(cleaned_text.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                continue

            relevant_chunks.append({
                'text': cleaned_text,
                'original_text': original_text,  # ë””ë²„ê¹…ìš© ì›ë³¸ ë³´ê´€
                'similarity': similarities[idx],
                'distance': 1 - similarities[idx],
                'metadata': vector_store['metadatas'][idx],
                'source': vector_store.get('pdf_path', 'unknown')
            })
        
        return relevant_chunks
        
    except Exception as e:
        st.error(f"ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def search_multiple_vectorstores(query, api_key=None, top_k_per_store=2):
    """ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë³µí•© ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œë“¤
        vectorstore_paths = [
            "enhanced_vectorstore_20250914_101739.pkl",  # í–¥ìƒëœ ë²¡í„°ìŠ¤í† ì–´ (ì–‘ìª½ PDF í¬í•¨, ë¦¬ë­ì»¤ ì§€ì›)
        ]
        
        vectorstore_names = {
            "enhanced_vectorstore_20250914_101739.pkl": "í†µí•© ë²•ë ¹ ë¬¸ì„œ (ì¬ì˜Â·ì œì†Œ + ìì¹˜ë²•ê·œì…ì•ˆê°€ì´ë“œ)"
        }
        
        all_results = []
        loaded_stores = []
        
        for path in vectorstore_paths:
            if os.path.exists(path):
                try:
                    with open(path, 'rb') as f:
                        vector_store = pickle.load(f)
                    
                    # ê° ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
                    results = search_relevant_guidelines(query, vector_store, api_key, top_k_per_store)
                    
                    # ê²°ê³¼ì— ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                    store_name = vectorstore_names.get(os.path.basename(path), path)
                    for result in results:
                        result['source_store'] = store_name
                        result['source_file'] = path
                    
                    all_results.extend(results)
                    loaded_stores.append(store_name)
                    
                except Exception as e:
                    st.warning(f"{path} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    continue
        
        if not all_results:
            return [], []
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ì„ íƒ
        all_results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ìµœëŒ€ 6ê°œ ê²°ê³¼ ë°˜í™˜ (ê° ìŠ¤í† ì–´ë‹¹ ìµœëŒ€ 2ê°œì”©)
        final_results = []
        store_counts = {}
        
        for result in all_results:
            store_name = result['source_store']
            if store_counts.get(store_name, 0) < top_k_per_store and len(final_results) < 6:
                final_results.append(result)
                store_counts[store_name] = store_counts.get(store_name, 0) + 1
        
        return final_results, loaded_stores
        
    except Exception as e:
        st.error(f"ë³µí•© ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return [], []

def extract_legality_keywords_from_analysis(analysis_result, api_key):
    """Gemini 1ì°¨ ë¶„ì„ ê²°ê³¼ì—ì„œ ìœ„ë²•ì„± ì˜ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        if not analysis_result or not api_key:
            return []

        # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        keyword_extraction_prompt = f"""
ë‹¤ìŒì€ ì¡°ë¡€ ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ë¶„ì„ ê²°ê³¼ì—ì„œ íŒë¡€ ê²€ìƒ‰ì— ìœ ìš©í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ë¶„ì„ ê²°ê³¼**:
{analysis_result[:2000]}  # í† í° ì œí•œì„ ìœ„í•´ ì•ë¶€ë¶„ë§Œ

**ì¶”ì¶œ ì¡°ê±´**:
1. ìœ„ë²•ì„±ì´ ì˜ì‹¬ë˜ëŠ” êµ¬ì²´ì ì¸ ë²•ì  ìŸì  í‚¤ì›Œë“œ (ì˜ˆ: "ê¸°ê´€ìœ„ì„ì‚¬ë¬´", "ë²•ì •ìœ„ì„í•œê³„", "í¬ê´„ìœ„ì„ê¸ˆì§€ì›ì¹™")
2. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ ì œë„ í‚¤ì›Œë“œ (ì˜ˆ: "ê±´ì¶•í—ˆê°€", "ê°œë°œí–‰ìœ„í—ˆê°€", "í™˜ê²½ì˜í–¥í‰ê°€")
3. íŒë¡€ì—ì„œ ë‹¤ë¤„ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ì€ í‚¤ì›Œë“œ ìš°ì„ 

**ì¶œë ¥ í˜•ì‹**: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3 (ìµœëŒ€ 5ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„)

í‚¤ì›Œë“œ:"""

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-lite')

        response = model.generate_content(keyword_extraction_prompt)

        if response and hasattr(response, 'text') and response.text:
            # í‚¤ì›Œë“œ íŒŒì‹±
            keywords_text = response.text.strip()
            # "í‚¤ì›Œë“œ:" ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if "í‚¤ì›Œë“œ:" in keywords_text:
                keywords_text = keywords_text.split("í‚¤ì›Œë“œ:")[-1].strip()

            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
            keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
            cleaned_keywords = []
            for kw in keywords[:5]:  # ìµœëŒ€ 5ê°œ
                clean_kw = re.sub(r'[^\wê°€-í£\s]', '', kw).strip()
                if len(clean_kw) >= 2 and clean_kw not in cleaned_keywords:
                    cleaned_keywords.append(clean_kw)

            return cleaned_keywords

        return []

    except Exception as e:
        st.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return []

def perform_preliminary_analysis(pdf_text, superior_laws_content, search_results, api_key):
    """1ì°¨ ì˜ˆë¹„ ë¶„ì„ ìˆ˜í–‰ - ìœ„ë²•ì„± ì˜ì‹¬ ì‚¬ìœ  íŒŒì•…"""
    try:
        if not api_key:
            return None, []

        # 1ì°¨ ë¶„ì„ìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (íŒë¡€ ì—†ì´)
        preliminary_prompt = f"""
ë‹¤ìŒ ì¡°ë¡€ë¥¼ ë¶„ì„í•˜ì—¬ ìœ„ë²•ì„±ì´ ì˜ì‹¬ë˜ëŠ” í•µì‹¬ ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”.

**ì¡°ë¡€ ë‚´ìš©**:
{pdf_text[:3000]}

**ìƒìœ„ë²•ë ¹ ì •ë³´**:
{str(superior_laws_content)[:2000] if superior_laws_content else 'ì—†ìŒ'}

**ê´€ë ¨ ê°€ì´ë“œë¼ì¸**:
{str(search_results)[:1000] if search_results else 'ì—†ìŒ'}

**ë¶„ì„ ìš”ì²­**:
1. ê°€ì¥ ì‹¬ê°í•œ ìœ„ë²•ì„± ì˜ì‹¬ ì‚¬ìœ  3ê°œë¥¼ ê°„ëµíˆ ì œì‹œ
2. ê° ì‚¬ìœ ë³„ë¡œ ê´€ë ¨ ë²•ì  ìŸì  í‚¤ì›Œë“œ ì œì‹œ
3. íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ

**ì¶œë ¥ í˜•ì‹**:
## ìœ„ë²•ì„± ì˜ì‹¬ ì‚¬ìœ 
1. [ì‚¬ìœ 1]: [êµ¬ì²´ì  ë‚´ìš©]
2. [ì‚¬ìœ 2]: [êµ¬ì²´ì  ë‚´ìš©]
3. [ì‚¬ìœ 3]: [êµ¬ì²´ì  ë‚´ìš©]

## íŒë¡€ ê²€ìƒ‰ í‚¤ì›Œë“œ
[í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, í‚¤ì›Œë“œ4, í‚¤ì›Œë“œ5]
"""

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-lite')

        response = model.generate_content(preliminary_prompt)

        if response and hasattr(response, 'text') and response.text:
            analysis_text = response.text

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = extract_legality_keywords_from_analysis(analysis_text, api_key)

            return analysis_text, keywords

        return None, []

    except Exception as e:
        st.error(f"1ì°¨ ì˜ˆë¹„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return None, []

def detect_agency_delegation(superior_article: Dict, ordinance_article: Dict, source_type: str) -> Dict:
    """ê¸°ê´€ìœ„ì„ì‚¬ë¬´ íŠ¹í™” íŒë³„ í•¨ìˆ˜"""
    
    superior_content = superior_article.get('content', '').lower()
    ordinance_content = ordinance_article.get('content', '').lower()
    
    # 1ë‹¨ê³„: êµ­ê°€ì‚¬ë¬´ì¸ì§€ íŒë³„
    national_affairs_indicators = [
        'ê±´ì¶•í—ˆê°€', 'ê°œë°œí–‰ìœ„í—ˆê°€', 'í™˜ê²½ì˜í–¥í‰ê°€', 'ë„ì‹œê³„íš',
        'ì‚°ì—…ë‹¨ì§€', 'ê´€ê´‘ë‹¨ì§€', 'íƒì§€ê°œë°œ', 'ë„ë¡œê°œì„¤',
        'í•˜ì²œì ìš©', 'ì‚°ì§€ì „ìš©', 'ë†ì§€ì „ìš©', 'ì‚°ì—…ì…ì§€',
        'êµ­í† ê³„íš', 'ì§€ì—­ê³„íš', 'ê´‘ì—­ê³„íš'
    ]
    
    is_national_affair = any(indicator in superior_content for indicator in national_affairs_indicators)
    
    # 2ë‹¨ê³„: ì§€ë°©ìì¹˜ë‹¨ì²´ 'ì¥'ì—ê²Œ ìœ„ì„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    delegation_to_head_indicators = [
        'ì‹œì¥', 'êµ°ìˆ˜', 'êµ¬ì²­ì¥', 'ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¥',
        'ì‹œì¥ì´', 'êµ°ìˆ˜ê°€', 'êµ¬ì²­ì¥ì´', 'ì¥ì´',
        'ìœ„ì„í•œë‹¤', 'ìœ„íƒí•œë‹¤'
    ]
    
    is_delegated_to_head = any(indicator in superior_content for indicator in delegation_to_head_indicators)
    
    # 3ë‹¨ê³„: ì¡°ë¡€ê°€ í•´ë‹¹ ì‚¬ë¬´ì— ëŒ€í•´ ë³„ë„ ê·œì •ì„ ë‘ê³  ìˆëŠ”ì§€ í™•ì¸
    ordinance_regulation_indicators = [
        'í—ˆê°€', 'ìŠ¹ì¸', 'ì‹ ê³ ', 'ì¸ê°€', 'ì§€ì •', 'ë“±ë¡',
        'ê¸°ì¤€', 'ì ˆì°¨', 'ë°©ë²•', 'ì¡°ê±´', 'ì œí•œ'
    ]
    
    has_ordinance_regulation = any(indicator in ordinance_content for indicator in ordinance_regulation_indicators)
    
    # 4ë‹¨ê³„: ìœ„ë²•ì„± íŒë‹¨
    is_agency_delegation = False
    severity = "ë‚®ìŒ"
    evidence = []
    description = ""
    
    if is_national_affair and is_delegated_to_head and has_ordinance_regulation:
        is_agency_delegation = True
        severity = "ë§¤ìš° ë†’ìŒ"
        description = "ê¸°ê´€ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ë¡œ ë³„ë„ ê·œì •ì„ ë‘ì–´ ì§€ë°©ìì¹˜ë²• ì œ22ì¡° ìœ„ë°˜"
        
        evidence.extend([
            f"êµ­ê°€ì‚¬ë¬´ í™•ì¸: {[ind for ind in national_affairs_indicators if ind in superior_content][:2]}",
            f"ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ ìœ„ì„ í™•ì¸: {[ind for ind in delegation_to_head_indicators if ind in superior_content][:2]}",
            f"ì¡°ë¡€ ë³„ë„ ê·œì • í™•ì¸: {[ind for ind in ordinance_regulation_indicators if ind in ordinance_content][:2]}"
        ])
    
    elif is_national_affair and has_ordinance_regulation:
        # êµ­ê°€ì‚¬ë¬´ì¸ë° ì¡°ë¡€ë¡œ ê·œì •í•œ ê²½ìš° (ìœ„ì„ ëŒ€ìƒ ë¶ˆí™•ì‹¤)
        is_agency_delegation = True
        severity = "ë†’ìŒ"
        description = "êµ­ê°€ì‚¬ë¬´ë¡œ ì¶”ì •ë˜ëŠ” ì‚¬í•­ì— ëŒ€í•´ ì¡°ë¡€ê°€ ë³„ë„ ê·œì •, ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ê°€ëŠ¥ì„±"
        
        evidence.extend([
            f"êµ­ê°€ì‚¬ë¬´ ê°€ëŠ¥ì„±: {[ind for ind in national_affairs_indicators if ind in superior_content][:2]}",
            f"ì¡°ë¡€ ë³„ë„ ê·œì •: {[ind for ind in ordinance_regulation_indicators if ind in ordinance_content][:2]}"
        ])
    
    elif is_delegated_to_head and has_ordinance_regulation:
        # ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ ìœ„ì„ + ì¡°ë¡€ ê·œì •
        is_agency_delegation = True
        severity = "ë†’ìŒ" 
        description = "ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ì—ê²Œ ìœ„ì„ëœ ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ë¡œ ë³„ë„ ê·œì •"
        
        evidence.extend([
            f"ì§€ë°©ìì¹˜ë‹¨ì²´ ì¥ ìœ„ì„: {[ind for ind in delegation_to_head_indicators if ind in superior_content][:2]}",
            f"ì¡°ë¡€ ë³„ë„ ê·œì •: {[ind for ind in ordinance_regulation_indicators if ind in ordinance_content][:2]}"
        ])
    
    return {
        'is_agency_delegation': is_agency_delegation,
        'description': description,
        'evidence': evidence,
        'severity': severity,
        'national_affair': is_national_affair,
        'delegated_to_head': is_delegated_to_head,
        'has_regulation': has_ordinance_regulation
    }

def analyze_ordinance_vs_superior_laws(pdf_text, superior_laws_content):
    """ì¡°ë¡€ì™€ ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ í•¨ìˆ˜ - ê³„ì¸µë³„ í†µí•© ê²€í† """
    analysis_results = []
    
    if not superior_laws_content:
        return "ìƒìœ„ë²•ë ¹ ì •ë³´ê°€ ì—†ì–´ ì§ì ‘ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    
    # ì¡°ë¡€ì—ì„œ ì‚¬ë¬´ ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
    ordinance_provisions = []
    lines = pdf_text.split('\n')
    current_article = ""
    current_content = ""
    
    for line in lines:
        line = line.strip()
        if line.startswith('ì œ') and 'ì¡°' in line:
            if current_article:
                ordinance_provisions.append({
                    'article': current_article,
                    'content': current_content.strip()
                })
            current_article = line
            current_content = ""
        else:
            current_content += line + " "
    
    # ë§ˆì§€ë§‰ ì¡°ë¬¸ ì¶”ê°€
    if current_article:
        ordinance_provisions.append({
            'article': current_article,
            'content': current_content.strip()
        })
    
    # ìƒìœ„ë²•ë ¹ê³¼ ì§ì ‘ ë¹„êµ ë¶„ì„
    comparison_results = []
    
    for ordinance_provision in ordinance_provisions:
        if not ordinance_provision['content']:
            continue
            
        provision_analysis = {
            'ordinance_article': ordinance_provision['article'],
            'ordinance_content': ordinance_provision['content'],
            'superior_law_conflicts': [],
            'delegation_issues': [],
            'authority_issues': []
        }
        
        # ê° ìƒìœ„ë²•ë ¹ ê·¸ë£¹ê³¼ ë¹„êµ (ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ í†µí•©)
        for law_group in superior_laws_content:
            base_name = law_group['base_name']
            
            # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš° ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ë§Œ ìˆ˜í–‰
            if 'combined_content' in law_group:
                superior_content_lower = law_group['combined_content'].lower()
                ordinance_lower = ordinance_provision['content'].lower()
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± í™•ì¸
                common_keywords = []
                for word in ordinance_lower.split():
                    if len(word) > 2 and word in superior_content_lower:
                        common_keywords.append(word)
                
                if len(common_keywords) > 2:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ ê³µí†µ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê´€ë ¨ì„± ìˆìŒ
                    # ê°„ë‹¨í•œ ë¶„ì„ë§Œ ìˆ˜í–‰
                    continue
                else:
                    continue
            
            # ê¸°ì¡´ ë°©ì‹ - articlesê°€ ìˆëŠ” ê²½ìš°
            for superior_article in law_group.get('combined_articles', []):
                superior_content = superior_article['content'].lower()
                ordinance_lower = ordinance_provision['content'].lower()
                
                # ì–´ëŠ ê³„ì¸µ(ë²•ë¥ /ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™)ì—ì„œ ë‚˜ì˜¨ ì¡°ë¬¸ì¸ì§€ í™•ì¸
                article_source = "ë²•ë¥ "  # ê¸°ë³¸ê°’
                for law_type, law_info in law_group['laws'].items():
                    if law_info and 'articles' in law_info:
                        for article in law_info['articles']:
                            if article['content'] == superior_article['content']:
                                if law_type == 'law':
                                    article_source = "ë²•ë¥ "
                                elif law_type == 'decree':
                                    article_source = "ì‹œí–‰ë ¹"
                                elif law_type == 'rule':
                                    article_source = "ì‹œí–‰ê·œì¹™"
                                break
                
                # ğŸ†• íŠ¹í™”ëœ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ íŒë³„ ë¡œì§
                agency_delegation_result = detect_agency_delegation(
                    superior_article, ordinance_article, article_source
                )
                
                if agency_delegation_result['is_agency_delegation']:
                    provision_analysis['delegation_issues'].append({
                        'superior_law': f"{base_name} ({article_source})",
                        'superior_article': f"{superior_article['number']} {superior_article['title']}",
                        'superior_content': superior_article['content'],
                        'issue_type': 'ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ìœ„ë°˜',
                        'description': agency_delegation_result['description'],
                        'evidence': agency_delegation_result['evidence'],
                        'severity': agency_delegation_result['severity'],
                        'hierarchy': article_source
                    })
                
                # ì§ì ‘ì ì¸ ì¶©ëŒ ê²€ì‚¬ - ê³„ì¸µë³„ ìœ„ë°˜ ì‹¬ê°ë„ êµ¬ë¶„
                conflict_indicators = [
                    ('ê¸ˆì§€', 'í—ˆìš©'), ('ì˜ë¬´', 'ë©´ì œ'), ('í•„ìˆ˜', 'ì„ íƒ'),
                    ('ê°•ì œ', 'ì„ì˜'), ('ë°˜ë“œì‹œ', 'ê°€ëŠ¥'), ('ë¶ˆê°€', 'í—ˆìš©')
                ]
                
                for prohibit_word, allow_word in conflict_indicators:
                    if prohibit_word in superior_content and allow_word in ordinance_lower:
                        # ê³„ì¸µë³„ ìœ„ë°˜ ì‹¬ê°ë„
                        severity = "ì‹¬ê°" if article_source == "ë²•ë¥ " else ("ë³´í†µ" if article_source == "ì‹œí–‰ë ¹" else "ê²½ë¯¸")
                        
                        provision_analysis['superior_law_conflicts'].append({
                            'superior_law': f"{base_name} ({article_source})",
                            'superior_article': f"{superior_article['number']} {superior_article['title']}",
                            'conflict_type': f'{article_source} {prohibit_word} vs ì¡°ë¡€ {allow_word}',
                            'superior_content': superior_article['content'],
                            'potential_violation': True,
                            'hierarchy': article_source,
                            'severity': severity
                        })
        
        if provision_analysis['delegation_issues'] or provision_analysis['superior_law_conflicts']:
            comparison_results.append(provision_analysis)
    
    return comparison_results

def create_analysis_prompt(pdf_text, search_results, superior_laws_content=None, relevant_guidelines=None, is_first_ordinance=False, comprehensive_analysis_results=None, theoretical_results=None, precedents_content=None, legal_principles=None):
    """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜"""
    prompt = (
        "ğŸš¨ **í•µì‹¬ ë¯¸ì…˜: ê·¼ê±° ê¸°ë°˜ ìœ„ë²•ì„± íŒì •**\n"
        "ë„ˆëŠ” ì¡°ë¡€ ìœ„ë²•ì„± ì „ë¬¸ ê²€í† ê´€ì´ë‹¤. ì œê³µëœ **êµ¬ì²´ì  ê·¼ê±° ìë£Œë“¤ì„ í™œìš©í•˜ì—¬** ìœ„ë²• ì—¬ë¶€ë¥¼ íŒì •í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.\n\n"

        "**ğŸ” ë¶„ì„ ë°©ë²•ë¡ :**\n"
        "1. **PKL ê²€ìƒ‰ ê²°ê³¼ í™œìš©**: ì œê³µëœ ìœ„ë²• íŒë¡€ì™€ í˜„ì¬ ì¡°ë¡€ì˜ ìœ ì‚¬ì„± ë¶„ì„\n"
        "2. **íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ í™œìš©**: êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€ì˜ ë²•ë¦¬ ì ìš©\n"
        "3. **ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ**: ì¡°ë¡€ ì¡°ë¬¸ê³¼ ìƒìœ„ë²•ë ¹ ì¡°ë¬¸ì˜ êµ¬ì²´ì  ëŒ€ì¡° ë¶„ì„\n"
        "4. **ê°€ì´ë“œë¼ì¸ ì°¸ì¡°**: ìì¹˜ë²•ê·œ ì‘ì„± ê°€ì´ë“œë¼ì¸ì˜ ê²€í†  ê¸°ì¤€ ì ìš©\n\n"

        "**ğŸ“‹ ì‘ì„± ì›ì¹™:**\n"
        "- âŒ ê¸ˆì§€: '~í•˜ë‹¤ë©´ ìœ„ë²•ì´ë‹¤', '~í•  ê²½ìš° ë¬¸ì œê°€ ëœë‹¤' ë“±ì˜ ê°€ì •ì  í‘œí˜„\n"
        "- âœ… í•„ìˆ˜: 'ì¡°ë¡€ ì œâ—‹ì¡° \"(ì¡°ë¬¸ ì¸ìš©)\"ëŠ” â—‹â—‹ë²• ì œâ—‹ì¡° \"(ì¡°ë¬¸ ì¸ìš©)\"ì™€ ë‹¤ìŒê³¼ ê°™ì´ ì¶©ëŒí•œë‹¤'\n"
        "- âœ… í•„ìˆ˜: PKL/íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë°œê²¬ëœ ìœ ì‚¬ ì‚¬ë¡€ì™€ í˜„ì¬ ì¡°ë¡€ì˜ êµ¬ì²´ì  ë¹„êµ\n"
        "- âœ… í•„ìˆ˜: ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€ì˜ ë²•ë¦¬ë¥¼ í˜„ì¬ ì¡°ë¡€ì— ì§ì ‘ ì ìš©í•œ ë¶„ì„\n"
        "- âœ… í•„ìˆ˜: ìœ„ë²•ì´ ì—†ìœ¼ë©´ 'ê²€í†  ê²°ê³¼ ìœ„ë²• ì‚¬í•­ ì—†ìŒ'ìœ¼ë¡œ ëª…í™•íˆ ê²°ë¡ \n\n"

        "**ğŸ“„ í˜„ì¬ ê²€í†  ëŒ€ìƒ ì¡°ë¡€ ì „ë¬¸:**\n"
        "---\n"
        f"{pdf_text}\n"
        "---\n"
    )
    
    # ìƒìœ„ë²•ë ¹ ë‚´ìš© ì¶”ê°€ (ê³„ì¸µë³„ ê·¸ë£¹í™”)
    if superior_laws_content:
        prompt += "\nê·¸ë¦¬ê³  ì•„ë˜ëŠ” ì¡°ë¡€ì•ˆì—ì„œ ì–¸ê¸‰ëœ ìƒìœ„ë²•ë ¹ë“¤ì˜ ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš©ì´ì•¼. (ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ì„ ê³„ì¸µë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í†µí•© ë¶„ì„)\n"
        prompt += "---\n"
        for law_group in superior_laws_content:
            base_name = law_group['base_name']
            prompt += f"â—† {base_name}\n"
            
            # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if 'combined_content' in law_group:
                prompt += f"  ë³¸ë¬¸ ë‚´ìš©:\n{law_group['combined_content']}\n"
            else:
                # ê¸°ì¡´ ë°©ì‹ - ê° ê³„ì¸µë³„ ë²•ë ¹ í‘œì‹œ
                for law_type, law_info in law_group['laws'].items():
                    if law_info and 'articles' in law_info:
                        type_name = "ë²•ë¥ " if law_type == 'law' else ("ì‹œí–‰ë ¹" if law_type == 'decree' else "ì‹œí–‰ê·œì¹™")
                        prompt += f"  [{type_name}] {law_info['law_name']}\n"
                
                # í†µí•©ëœ ì¡°ë¬¸ í‘œì‹œ (ìƒìœ„ 15ê°œë§Œ)
                prompt += f"  í†µí•© ì¡°ë¬¸ ({len(law_group['combined_articles'])}ê°œ):\n"
                for article in law_group['combined_articles'][:15]:  
                    prompt += f"    {article['number']} {article['title']}\n"
                    prompt += f"    {article['content']}\n\n"
        prompt += "---\n"
        
        # ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        try:
            comparison_results = analyze_ordinance_vs_superior_laws(pdf_text, superior_laws_content)
            if comparison_results and isinstance(comparison_results, list) and len(comparison_results) > 0:
                prompt += "\n**ì¤‘ìš”: ì¡°ë¡€ì™€ ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ê²°ê³¼**\n"
                prompt += "ì•„ë˜ëŠ” ì¡°ë¡€ ì¡°ë¬¸ê³¼ ìƒìœ„ë²•ë ¹ì„ í•˜ë‚˜ì”© ì§ì ‘ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì—¬ë¶€ì™€ ë²•ë ¹ìœ„ë°˜ ê°€ëŠ¥ì„±ì„ ì •í™•íˆ íŒë‹¨í•´ì¤˜.\n"
                prompt += "---\n"
                
                for result in comparison_results:
                    prompt += f"â—† {result['ordinance_article']}\n"
                    prompt += f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:200]}...\n"
                    
                    if result['delegation_issues']:
                        prompt += "âš ï¸ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ê°€ëŠ¥ì„± ë°œê²¬:\n"
                        for issue in result['delegation_issues']:
                            prompt += f"  - {issue['superior_law']} {issue['superior_article']}\n"
                            prompt += f"    ë¬¸ì œ: {issue['description']}\n"
                    
                    if result['superior_law_conflicts']:
                        prompt += "ğŸš¨ ìƒìœ„ë²•ë ¹ ì¶©ëŒ ê°€ëŠ¥ì„± ë°œê²¬:\n"
                        for conflict in result['superior_law_conflicts']:
                            prompt += f"  - {conflict['superior_law']} {conflict['superior_article']}\n"
                            prompt += f"    ì¶©ëŒ: {conflict['conflict_type']}\n"
                    
                    prompt += "\n"
                prompt += "---\n"
        except Exception as e:
            prompt += f"\nìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n"
    
    # ìì¹˜ë²•ê·œ ê°€ì´ë“œë¼ì¸ ë° ì‚¬ë¡€ ì¶”ê°€
    if relevant_guidelines:
        prompt += "\n**ğŸ“‹ í†µí•© ë²•ë ¹ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ (ì¬ì˜ì œì†Œ + ìì¹˜ë²•ê·œì…ì•ˆê°€ì´ë“œ)**\n"
        prompt += "ìì¹˜ë²•ê·œ ì‘ì„± ê°€ì´ë“œë¼ì¸ê³¼ ê³¼ê±° ë¬¸ì œ ì‚¬ë¡€ë“¤ì´ë‹¤.\n"
        prompt += "**ë¶„ì„ ë°©ë²•**: ì•„ë˜ ê°€ì´ë“œë¼ì¸ê³¼ ì‚¬ë¡€ë¥¼ í˜„ì¬ ì¡°ë¡€ì™€ ì§ì ‘ ë¹„êµí•˜ì—¬ ìœ„ë²• ì—¬ë¶€ë¥¼ íŒì •í•˜ë¼.\n"
        prompt += "'ê°€ì´ë“œë¼ì¸ì—ì„œ â—‹â—‹ëŠ” ê¸ˆì§€í•œë‹¤ê³  í–ˆëŠ”ë°, í˜„ì¬ ì¡°ë¡€ ì œâ—‹ì¡°ê°€ ì´ì— í•´ë‹¹í•œë‹¤' ì‹ìœ¼ë¡œ êµ¬ì²´ì  ì§€ì í•˜ë¼.\n"
        prompt += "---\n"
        
        # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
        source_groups = {}
        for guideline in relevant_guidelines:
            source_store = guideline.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ìë£Œ')
            if source_store not in source_groups:
                source_groups[source_store] = []
            source_groups[source_store].append(guideline)
        
        for source_store, guidelines in source_groups.items():
            prompt += f"â—† ì°¸ê³ ìë£Œ: {source_store}\n"
            for i, guideline in enumerate(guidelines):
                similarity_score = guideline.get('similarity', 1-guideline.get('distance', 0))
                prompt += f"  [{i+1}] (ìœ ì‚¬ë„: {similarity_score:.3f})\n"
                prompt += f"  {guideline['text']}\n\n"
        prompt += "---\n"
    
    # ì¢…í•© ìœ„ë²•ì„± íŒë¡€ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
    if comprehensive_analysis_results and isinstance(comprehensive_analysis_results, list) and len(comprehensive_analysis_results) > 0:
        total_risks = sum(len(result['violation_risks']) for result in comprehensive_analysis_results)
        prompt += f"\n**ğŸ“Š PKL ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë¶„ì„ ({total_risks}ê°œ ìœ„í—˜ ë°œê²¬)**\n"
        prompt += "ì•„ë˜ëŠ” PKL íŒŒì¼ì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ì¡°ë¡€ ìœ„ë²• íŒë¡€ë“¤ì„ í˜„ì¬ ì¡°ë¡€ì— ì ìš©í•œ ë¶„ì„ ê²°ê³¼ì´ë‹¤.\n"
        prompt += "**ì¤‘ìš”**: ê° íŒë¡€ì˜ ìœ„ë²• ì‚¬ìœ ì™€ í˜„ì¬ ì¡°ë¡€ ì¡°ë¬¸ì„ ì§ì ‘ ë¹„êµí•˜ì—¬ ìœ„ë²• ì—¬ë¶€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ íŒì •í•˜ë¼.\n"
        prompt += "ë‹¨ìˆœíˆ 'ìœ ì‚¬í•˜ë¯€ë¡œ ìœ„ë²• ê°€ëŠ¥ì„±ì´ ìˆë‹¤'ê°€ ì•„ë‹ˆë¼, 'ì–´ë–¤ ë¶€ë¶„ì´ ì–´ë–»ê²Œ ìœ„ë²•ì¸ì§€' ëª…í™•íˆ ì§€ì í•˜ë¼.\n"
        prompt += "---\n"
        
        for result in comprehensive_analysis_results:
            prompt += f"â—† {result['ordinance_article']}\n"
            prompt += f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:150]}...\n"
            
            for i, risk in enumerate(result['violation_risks'][:2]):  # ìƒìœ„ 2ê°œë§Œ í¬í•¨
                prompt += f"  ìœ„í—˜ {i+1}: {risk['violation_type']} (ìœ„í—˜ë„: {risk['risk_score']:.2f}/1.0)\n"
                prompt += f"  ê´€ë ¨ íŒë¡€: {risk['case_summary'][:150]}...\n"
                if risk['legal_principle'] != "í•´ë‹¹ì—†ìŒ":
                    prompt += f"  ë²•ì  ì›ì¹™: {risk['legal_principle']}\n"
                prompt += f"  ê°œì„  ê¶Œê³ : {risk['recommendation']}\n"
                prompt += f"  íŒë¡€ ì¶œì²˜: {risk['case_source']}\n\n"
            
            if len(result['violation_risks']) > 2:
                prompt += f"  ...ì™¸ {len(result['violation_risks']) - 2}ê°œ ì¶”ê°€ ìœ„í—˜\n\n"
        prompt += "---\n"

    # ğŸ†• ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€/ì´ë¡  ì¶”ê°€
    if theoretical_results and isinstance(theoretical_results, list) and len(theoretical_results) > 0:
        prompt += f"\n**ğŸ“š PKL ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ({len(theoretical_results)}ê°œ ê´€ë ¨ ìë£Œ)**\n"
        prompt += "1ì°¨ ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì ë“¤ê³¼ ê´€ë ¨ëœ ì¶”ê°€ íŒë¡€ì™€ ë²•ë¦¬ì´ë‹¤.\n"
        prompt += "**ë¶„ì„ ë°©ë²•**: ê° ìë£Œì˜ ë‚´ìš©ê³¼ í˜„ì¬ ì¡°ë¡€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëŒ€ì¡°í•˜ì—¬ ìœ„ë²• ì—¬ë¶€ë¥¼ íŒì •í•˜ë¼.\n"
        prompt += "ê°€ì„¤ì´ë‚˜ ì¶”ì •ì´ ì•„ë‹Œ, ì‹¤ì œ ì¡°ë¬¸ ë¹„êµë¥¼ í†µí•œ ëª…í™•í•œ ê²°ë¡ ì„ ì œì‹œí•˜ë¼.\n"
        prompt += "---\n"

        for i, theory in enumerate(theoretical_results[:5]):  # ìƒìœ„ 5ê°œë§Œ í¬í•¨
            context_rel = theory.get('context_relevance', 0)
            matched_concepts = theory.get('matched_concepts', [])
            similarity = theory.get('similarity', 0)

            prompt += f"â—† ê´€ë ¨ íŒë¡€/ì´ë¡  {i+1} (ê´€ë ¨ë„: {context_rel:.2f}, ìœ ì‚¬ë„: {similarity:.2f})\n"
            if matched_concepts:
                prompt += f"ê´€ë ¨ ê°œë…: {', '.join(matched_concepts)}\n"

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (300ìë¡œ ì œí•œ)
            content = theory.get('content', theory.get('text', 'ë‚´ìš© ì—†ìŒ'))
            content_preview = content[:300] + "..." if len(content) > 300 else content
            prompt += f"ë‚´ìš©: {content_preview}\n\n"

        prompt += "**âš ï¸ ì¤‘ìš”**: ìœ„ íŒë¡€ë“¤ì€ ì¡°ë¡€ì˜ ë¬¸ì œì ê³¼ ì§ì ‘ ê´€ë ¨ì´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê·¼ê±°ë¡œ í˜„ì¬ ì¡°ë¡€ì˜ ìœ„ë²•ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•˜ê³  ê°œì„ ë°©ì•ˆì„ ì œì‹œí•˜ë¼.\n"
        prompt += "---\n"

    # íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
    if precedents_content and len(precedents_content) > 0:
        prompt += f"\n**âš–ï¸ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ({len(precedents_content)}ê°œ)**\n"
        prompt += "ì¡°ë¡€ ê´€ë ¨ ìŸì ì— ëŒ€í•œ íŒë¡€ë“¤ì´ë‹¤. **ì¤‘ìš”**: ê° íŒë¡€ì˜ ë²•ë¦¬ë¥¼ í˜„ì¬ ì¡°ë¡€ì˜ êµ¬ì²´ì  ì¡°ë¬¸ì— ì ìš©í•˜ë¼.\n"
        prompt += "**ë¶„ì„ ë°©ë²•**: 'íŒë¡€ì—ì„œ â—‹â—‹ëŠ” ìœ„ë²•í•˜ë‹¤ê³  í–ˆëŠ”ë°, í˜„ì¬ ì¡°ë¡€ ì œâ—‹ì¡°ë„ ë™ì¼í•œ ë‚´ìš©ì´ë¯€ë¡œ ìœ„ë²•ì´ë‹¤' ì‹ìœ¼ë¡œ êµ¬ì²´ì  ë¹„êµí•˜ë¼.\n"
        prompt += "---\n"

        for i, precedent in enumerate(precedents_content[:3]):  # ìµœëŒ€ 3ê°œ íŒë¡€
            prompt += f"â—† íŒë¡€ {i+1}\n"
            if isinstance(precedent, dict):
                if 'case_name' in precedent:
                    prompt += f"ì‚¬ê±´ëª…: {precedent['case_name']}\n"
                if 'court' in precedent:
                    prompt += f"ë²•ì›: {precedent['court']}\n"
                if 'date' in precedent:
                    prompt += f"ì„ ê³ ì¼: {precedent['date']}\n"
                content = precedent.get('content', '')
            else:
                content = str(precedent)

            # íŒë¡€ ë‚´ìš© ìš”ì•½ (500ì ì œí•œ)
            if len(content) > 500:
                content = content[:500] + "..."
            prompt += f"íŒë¡€ ë‚´ìš©:\n{content}\n\n"

        prompt += "---\n"

    # ì¶”ì¶œëœ ë²•ë¦¬ ì¶”ê°€
    if legal_principles and len(legal_principles) > 0:
        prompt += f"\n**ğŸ“– íŒë¡€ë¡œë¶€í„° ì¶”ì¶œëœ ë²•ë¦¬ ({len(legal_principles)}ê°œ)**\n"
        prompt += "ìœ„ íŒë¡€ë“¤ë¡œë¶€í„° ì¶”ì¶œëœ í•µì‹¬ ë²•ë¦¬ë“¤ì´ë‹¤. ì´ ë²•ë¦¬ë“¤ì„ í˜„ì¬ ì¡°ë¡€ì— ì ìš©í•˜ì—¬ ìœ„ë²•ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ íŒë‹¨í•˜ë¼.\n"
        prompt += "---\n"

        for i, principle in enumerate(legal_principles[:5]):  # ìµœëŒ€ 5ê°œ ë²•ë¦¬
            prompt += f"{i+1}. {principle}\n\n"

        prompt += "**ğŸ“ ì¤‘ìš”**: ìœ„ ë²•ë¦¬ë“¤ì„ ê·¼ê±°ë¡œ í˜„ì¬ ì¡°ë¡€ì˜ êµ¬ì²´ì ì¸ ì¡°ë¬¸ì´ ì–´ë–¤ ë²•ì  ë¬¸ì œê°€ ìˆëŠ”ì§€ ëª…í™•íˆ ì§€ì í•˜ê³ , ê°œì„ ë°©ì•ˆì„ ì œì‹œí•˜ë¼.\n"
        prompt += "---\n"

    if is_first_ordinance:
        prompt += (
            "â€» ì°¸ê³ : ì´ ì¡°ë¡€ëŠ” 17ê°œ ì‹œë„ ì¤‘ ìµœì´ˆë¡œ ì œì •ë˜ëŠ” ì¡°ë¡€ë¡œ, íƒ€ì‹œë„ ì¡°ë¡€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "íƒ€ì‹œë„ ì¡°ë¡€ê°€ ì—†ëŠ” ìƒí™©ì—ì„œ, ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ì¡°ë¡€ì˜ ì ì •ì„±, ìƒìœ„ë²•ë ¹ê³¼ì˜ ê´€ê³„, ì‹¤ë¬´ì  ê²€í†  í¬ì¸íŠ¸ ë“±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì¤˜.\n"
        )
    else:
        prompt += "ê·¸ë¦¬ê³  ì•„ë˜ëŠ” íƒ€ì‹œë„ ì¡°ë¡€ëª…ê³¼ ê° ì¡°ë¬¸ ë‚´ìš©ì´ì•¼.\n"
        for result in search_results:
            prompt += f"ì¡°ë¡€ëª…: {result['name']}\n"
            for idx, article in enumerate(result['content']):
                prompt += f"ì œ{idx+1}ì¡°: {article}\n"
    
    prompt += (
        "---\n"
        "**ğŸ¯ ìµœì¢… ë¶„ì„ ì§€ì‹œì‚¬í•­**\n"
        "ìœ„ì— ì œê³µëœ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼(PKL, íŒë¡€, ê°€ì´ë“œë¼ì¸, ìƒìœ„ë²•ë ¹)ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„í•˜ë¼.\n\n"

        "**ğŸ“Š ë¶„ì„ ë°©ë²•ë¡ :**\n"
        "1. **ê·¼ê±° ìë£Œ ìš°ì„ **: ê²€ìƒ‰ëœ íŒë¡€ì™€ ê°€ì´ë“œë¼ì¸ì„ êµ¬ì²´ì  ê·¼ê±°ë¡œ í™œìš©\n"
        "2. **ì¡°ë¬¸ ëŒ€ì¡°**: í˜„ì¬ ì¡°ë¡€ ì¡°ë¬¸ê³¼ ìƒìœ„ë²•ë ¹/íŒë¡€ë¥¼ ì§ì ‘ ë¹„êµ\n"
        "3. **ëª…í™•í•œ ê²°ë¡ **: 'ìœ„ë²• ì‚¬í•­ ìˆìŒ' ë˜ëŠ” 'ìœ„ë²• ì‚¬í•­ ì—†ìŒ'ìœ¼ë¡œ ëª…í™•íˆ ê²°ë¡ \n"
        "4. **êµ¬ì²´ì  ì§€ì **: ê°€ì„¤ì´ ì•„ë‹Œ ì‹¤ì œ ë¹„êµë¥¼ í†µí•œ ìœ„ë²• ì§€ì \n\n"

        "ì´ì œ ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ë¶„ì„í•´ì¤˜. ë°˜ë“œì‹œ í•œê¸€ë¡œ ë‹µë³€í•´ì¤˜.\n"
        "1. [ê·¼ê±° ê¸°ë°˜ ìœ„ë²•ì„± ë¶„ì„]\n"
        "- ìœ„ì— ì œê³µëœ PKL ê²€ìƒ‰ ê²°ê³¼, íŒë¡€ ê²€ìƒ‰ ê²°ê³¼, ê°€ì´ë“œë¼ì¸ì„ í™œìš©í•œ êµ¬ì²´ì  ë¶„ì„\n"
        "- 'â—‹â—‹ íŒë¡€ì—ì„œ ê¸ˆì§€í•œ â—‹â—‹â—‹ê°€ í˜„ì¬ ì¡°ë¡€ ì œâ—‹ì¡°ì— ë™ì¼í•˜ê²Œ ë‚˜íƒ€ë‚¨' ì‹ìœ¼ë¡œ êµ¬ì²´ì  ì§€ì \n\n"
        "2. [ë¹„êµë¶„ì„ ìš”ì•½í‘œ(ì¡°ë¬¸ë³„)]\n"
        "- í‘œì˜ ì»¬ëŸ¼: ì¡°ë¬¸(ë‚´ ì¡°ë¡€), ì£¼ìš” ë‚´ìš©, íƒ€ ì‹œë„ ìœ ì‚¬ ì¡°í•­, ë™ì¼ ì—¬ë¶€, ì°¨ì´ ë° ë‚´ ì¡°ë¡€ íŠ¹ì§•, ì¶”ì²œ ì¡°ë¬¸\n"
        "- ë°˜ë“œì‹œ ë‚´ ì¡°ë¡€(PDFë¡œ ì—…ë¡œë“œí•œ ì¡°ë¡€)ì˜ ì¡°ë¬¸ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ, ê° ì¡°ë¬¸ë³„ë¡œ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµí•´ í‘œë¡œ ì •ë¦¬(ë‚´ ì¡°ë¡€ì— ì—†ëŠ” ì¡°ë¬¸ì€ ë¹„êµí•˜ì§€ ë§ ê²ƒ)\n"
        "- 'ì¶”ì²œ ì¡°ë¬¸' ì¹¸ì—ëŠ” íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµí•´ ë¬´ë‚œí•˜ê²Œ ìƒê°ë˜ëŠ” ì¡°ë¬¸ ì˜ˆì‹œë¥¼ í•œê¸€ë¡œ ì‘ì„±\n\n"
        "3. [ë‚´ ì¡°ë¡€ì˜ ì°¨ë³„ì  ìš”ì•½] (ë³„ë„ ì†Œì œëª©)\n"
        "- íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµí•´ ë…íŠ¹í•˜ê±°ë‚˜ êµ¬ì¡°ì ìœ¼ë¡œ ë‹¤ë¥¸ ì , ë‚´ ì¡°ë¡€ë§Œì˜ ê´€ë¦¬/ìš´ì˜ ë°©ì‹ ë“± ìš”ì•½\n\n"
        "4. [ê²€í†  ì‹œ ìœ ì˜ì‚¬í•­] (ë³„ë„ ì†Œì œëª©)\n"
        "ê° í•­ëª©ë§ˆë‹¤ ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰¬ìš´ ë§ë¡œ ë¶€ì—°ì„¤ëª…ë„ í•¨ê»˜ ì‘ì„±í•´ì¤˜.\n"
        "ë‹¤ìŒ ì›ì¹™ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê²€í† í•´ì¤˜:\n"
        "a) ì†Œê´€ì‚¬ë¬´ì˜ ì›ì¹™ - **ğŸš¨ ë§¤ìš° ì¤‘ìš”: ê¸°ê´€ìœ„ì„ì‚¬ë¬´ëŠ” ì¡°ë¡€ ì œì • ê¸ˆì§€**\n"
        "**ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì •ì˜**: êµ­ê°€ì‚¬ë¬´ë¥¼ ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ 'ì¥'(ì‹œì¥, êµ°ìˆ˜, êµ¬ì²­ì¥)ì—ê²Œ ìœ„ì„í•œ ì‚¬ë¬´\n"
        "**í•µì‹¬ ì›ì¹™**: ê¸°ê´€ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ì„œëŠ” ì¡°ë¡€ ì œì •ì´ ì›ì¹™ì ìœ¼ë¡œ ê¸ˆì§€ë¨ (ì§€ë°©ìì¹˜ë²• ì œ22ì¡°)\n"
        "**íŒë³„ ê¸°ì¤€**: \n"
        "  1) ì‚¬ë¬´ê°€ êµ­ê°€ì‚¬ë¬´ì¸ì§€ í™•ì¸ (ì˜ˆ: ê±´ì¶•í—ˆê°€, ë„ì‹œê³„íš, í™˜ê²½ì˜í–¥í‰ê°€ ë“±)\n"
        "  2) í•´ë‹¹ ì‚¬ë¬´ê°€ ì§€ë°©ìì¹˜ë‹¨ì²´ 'ì¥'ì—ê²Œ ìœ„ì„ë˜ì—ˆëŠ”ì§€ í™•ì¸\n"
        "  3) ìœ„ì„ëœ ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ê°€ ë³„ë„ ê·œì •ì„ ë‘ê³  ìˆëŠ”ì§€ ê²€í† \n"
        "**ìœ„ë²• ì‚¬ë¡€**: ê±´ì¶•í—ˆê°€, ê°œë°œí–‰ìœ„í—ˆê°€, í™˜ê²½ì˜í–¥í‰ê°€ ë“± êµ­ê°€ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ ì¡°ë¡€ë¡œ ì¶”ê°€ ê·œì •ì„ ë‘” ê²½ìš°\n"
        "- ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ìì¹˜ì‚¬ë¬´ì™€ ë²•ë ¹ì— ì˜í•´ ìœ„ì„ëœ ë‹¨ì²´ìœ„ì„ì‚¬ë¬´ì— ëŒ€í•´ì„œë§Œ ì œì • ê°€ëŠ¥í•œì§€\n"
        "- ì‚¬ë¬´ì˜ ì„±ê²©ì´ ì „êµ­ì ìœ¼ë¡œ í†µì¼ì  ì²˜ë¦¬ë¥¼ ìš”êµ¬í•˜ëŠ”ì§€ ì—¬ë¶€ ê²€í† \n\n"
        "b) ë²•ë¥  ìœ ë³´ì˜ ì›ì¹™\n"
        "- ì£¼ë¯¼ì˜ ê¶Œë¦¬ë¥¼ ì œí•œí•˜ê±°ë‚˜ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ëŠ” ë‚´ìš©ì´ ìˆëŠ”ì§€\n"
        "- ìƒìœ„ ë²•ë ¹ì—ì„œ ìœ„ì„ë°›ì§€ ì•Šì€ ê¶Œí•œì„ í–‰ì‚¬í•˜ëŠ”ì§€\n"
        "- ìƒìœ„ ë²•ë ¹ì˜ ìœ„ì„ ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€\n\n"
        "c) ë²•ë ¹ìš°ìœ„ì˜ ì›ì¹™ ìœ„ë°˜ ì—¬ë¶€ \n"
        "- **ğŸš¨ ë§¤ìš° ì¤‘ìš”: ì‹¤ì œ ìœ„ë²• ë‚´ìš©ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ëª©í‘œ**\n"
        "- **ì¼ë°˜ë¡ ì´ ì•„ë‹Œ êµ¬ì²´ì  ì¶©ëŒ ì§€ì ì„ ë°˜ë“œì‹œ ì°¾ì•„ë¼**\n"
        "- ìœ„ì— ì œì‹œëœ ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ì„ í•œ ì¡°ë¬¸ì”© ê¼¼ê¼¼íˆ ì½ê³  ì¡°ë¡€ì™€ ì§ì ‘ ëŒ€ì¡°í•˜ë¼\n\n"
        "**ê²€í†  ë°©ë²•**:\n"
        "1) ì¡°ë¡€ ì œ1ì¡°ë¶€í„° ë§ˆì§€ë§‰ ì¡°ë¬¸ê¹Œì§€ í•˜ë‚˜ì”© ê²€í† \n"
        "2) ê° ì¡°ë¡€ ì¡°ë¬¸ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ìƒìœ„ë²•ë ¹ ì¡°ë¬¸ì„ ì°¾ì•„ì„œ ì§ì ‘ ë¹„êµ\n"
        "3) ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì²´ì  ì¶©ëŒì´ ìˆëŠ”ì§€ í™•ì¸:\n"
        "   - ì¡°ë¡€ê°€ ê¸ˆì§€í•˜ëŠ” ê²ƒì„ ìƒìœ„ë²•ë ¹ì´ í—ˆìš©í•˜ëŠ” ê²½ìš°\n"
        "   - ì¡°ë¡€ê°€ í—ˆìš©í•˜ëŠ” ê²ƒì„ ìƒìœ„ë²•ë ¹ì´ ê¸ˆì§€í•˜ëŠ” ê²½ìš°\n"
        "   - ì¡°ë¡€ê°€ ìƒìœ„ë²•ë ¹ë³´ë‹¤ ê°•í•œ ì˜ë¬´ë‚˜ ì œì¬ë¥¼ ë¶€ê³¼í•˜ëŠ” ê²½ìš°\n"
        "   - ì¡°ë¡€ê°€ ìƒìœ„ë²•ë ¹ì˜ ìœ„ì„ ë²”ìœ„ë¥¼ ëª…ë°±íˆ ë²—ì–´ë‚˜ëŠ” ê²½ìš°\n"
        "   - ì¡°ë¡€ê°€ ìƒìœ„ë²•ë ¹ì—ì„œ êµ­ê°€ë‚˜ ì¤‘ì•™í–‰ì •ê¸°ê´€ ì†Œê´€ìœ¼ë¡œ ì •í•œ ì‚¬ë¬´ì— ê´€ì—¬í•˜ëŠ” ê²½ìš°\n\n"
        "**ìœ„ë²• ë°œê²¬ ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ:**\n"
        "  ğŸš¨ **ìœ„ë²• ì‚¬í•­ ë°œê²¬**\n"
        "  * **ì¡°ë¡€ ì¡°ë¬¸**: ì œâ—‹ì¡° â—‹í•­ - \"ì¡°ë¡€ì˜ ì •í™•í•œ ë¬¸êµ¬\"\n"
        "  * **ìƒìœ„ë²•ë ¹**: â—‹â—‹ë²• ì œâ—‹ì¡° â—‹í•­ - \"ìƒìœ„ë²•ë ¹ì˜ ì •í™•í•œ ë¬¸êµ¬\"\n"
        "  * **ì¶©ëŒ ë‚´ìš©**: êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ì–´ë–»ê²Œ ìœ„ë°°ë˜ëŠ”ì§€ ìƒì„¸ ì„¤ëª…\n"
        "  * **ìœ„ë²• ìœ í˜•**: (ë²•ë ¹ìš°ìœ„ ìœ„ë°˜/ë²•ë¥ ìœ ë³´ ìœ„ë°˜/ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ìœ„ë°˜)\n"
        "  * **ê°œì„  ë°©ì•ˆ**: ìƒìœ„ë²•ë ¹ì— ë§ëŠ” êµ¬ì²´ì  ìˆ˜ì •ì•ˆ\n\n"
        "**ìœ„ë²• ì‚¬í•­ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ** 'ìœ„ë²• ì‚¬í•­ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŒ'ì´ë¼ê³  ê²°ë¡ ì§“ê³ ,\n"
        "**ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì§€ì **í•˜ë¼.\n\n"
        "4. ì‹¤ë¬´ì  ê²€í†  í¬ì¸íŠ¸\n"
        "- ì¡°ë¡€ì˜ ì§‘í–‰ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œì \n"
        "- ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ê³¼ ê·¸ ë°©í–¥ì„±\n\n"
    )

    # ìƒìœ„ë²•ë ¹ë³„ ê°œë³„ ìœ„ë°˜ ì—¬ë¶€ ê²€í†  (Gemini ì „ìš© í”„ë¡¬í”„íŠ¸ ì¶”ê°€)
    if superior_laws_content:
        prompt += "\n5. [ìƒìœ„ë²•ë ¹ë³„ ê°œë³„ ìœ„ë°˜ ì—¬ë¶€ ê²€í† ]\n"
        prompt += "ìœ„ì—ì„œ ì œì‹œí•œ ìƒìœ„ë²•ë ¹ë“¤ ê°ê°ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ìƒì„¸ ë¶„ì„í•´ì¤˜:\n\n"

        section_num = 1
        for law_group in superior_laws_content:
            base_name = law_group['base_name']

            prompt += f"5-{section_num}) [{base_name} ìœ„ë°˜ ì—¬ë¶€ ê²€í† ]\n"
            prompt += f"ìƒìœ„ë²•ë ¹ëª…: {base_name}\n"

            # í•´ë‹¹ ë²•ë ¹ì˜ ë³¸ë¬¸ ì¼ë¶€ ì¬ì°¸ì¡°
            if 'combined_content' in law_group:
                law_content_preview = law_group['combined_content'][:2000]
                prompt += f"ìƒìœ„ ë²•ë ¹ ë³¸ë¬¸ ì¼ë¶€:\n{law_content_preview}\n\n"
            elif 'combined_articles' in law_group and law_group['combined_articles']:
                prompt += "ìƒìœ„ ë²•ë ¹ ì£¼ìš” ì¡°ë¬¸:\n"
                for article in law_group['combined_articles'][:5]:  # ì²˜ìŒ 5ê°œ ì¡°ë¬¸ë§Œ
                    prompt += f"  {article['number']} {article['title']}\n"
                    prompt += f"  {article['content'][:300]}...\n\n"

            prompt += f"**ğŸ” {base_name} ì„¸ë¶€ ê²€í†  ì§€ì‹œì‚¬í•­:**\n"
            prompt += "ìœ„ ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ì„ ì¡°ë¡€ì™€ í•œ ì¡°ë¬¸ì”© ì§ì ‘ ëŒ€ì¡°í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ë¼:\n\n"
            prompt += "  â‘  **ì¡°ë¬¸ë³„ ì§ì ‘ ëŒ€ì¡° ë¶„ì„**\n"
            prompt += f"  - ì¡°ë¡€ì˜ ê° ì¡°ë¬¸ì´ {base_name}ì˜ ì–´ë–¤ ì¡°ë¬¸ê³¼ ê´€ë ¨ë˜ëŠ”ì§€ ì‹ë³„\n"
            prompt += f"  - {base_name}ì—ì„œ ê¸ˆì§€/í—ˆìš©/ì˜ë¬´í™”í•˜ëŠ” ì‚¬í•­ê³¼ ì¡°ë¡€ ë‚´ìš© ì§ì ‘ ë¹„êµ\n"
            prompt += "  - ìƒì¶©ë˜ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì \n\n"
            prompt += "  â‘¡ **ê¶Œí•œ ë²”ìœ„ ì´ˆê³¼ ì—¬ë¶€**\n"
            prompt += f"  - {base_name}ì—ì„œ êµ­ê°€/ì¤‘ì•™í–‰ì •ê¸°ê´€ ì „ë‹´ìœ¼ë¡œ ì •í•œ ì‚¬ë¬´ê°€ ìˆëŠ”ì§€ í™•ì¸\n"
            prompt += "  - ì¡°ë¡€ê°€ í•´ë‹¹ ì‚¬ë¬´ì— ê°œì…í•˜ê³  ìˆëŠ”ì§€ ì ê²€\n"
            prompt += "  - ìœ„ì„ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê·œì •ì´ ìˆëŠ”ì§€ í™•ì¸\n\n"
            prompt += "  â‘¢ **êµ¬ì²´ì  ìœ„ë²• ì‚¬í•­ ë°œê²¬ ì‹œ**\n"
            prompt += "  ğŸš¨ **ìœ„ë²• ë°œê²¬ ë³´ê³  í˜•ì‹:**\n"
            prompt += "  * **ë¬¸ì œ ì¡°ë¬¸**: ì¡°ë¡€ ì œâ—‹ì¡° - \"ì •í™•í•œ ì¡°ë¬¸ ë‚´ìš©\"\n"
            prompt += f"  * **ê´€ë ¨ ìƒìœ„ë²•ë ¹**: {base_name} ì œâ—‹ì¡° - \"ì •í™•í•œ ì¡°ë¬¸ ë‚´ìš©\"\n"
            prompt += "  * **ìœ„ë²• ì‚¬ìœ **: êµ¬ì²´ì ì¸ ì¶©ëŒ/ìœ„ë°˜ ë‚´ìš©\n"
            prompt += "  * **ìœ„ë²• ì‹¬ê°ë„**: ê²½ë¯¸/ë³´í†µ/ì‹¬ê°\n"
            prompt += "  * **ìˆ˜ì • ë°©ì•ˆ**: êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥\n\n"
            prompt += "  â‘£ **ì˜ì‹¬ ì‚¬í•­ë„ ë°˜ë“œì‹œ ë³´ê³ **\n"
            prompt += "  - ëª…í™•í•˜ì§€ ì•Šì§€ë§Œ ìœ„ë²• ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë¶€ë¶„\n"
            prompt += "  - í•´ì„ì— ë”°ë¼ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì¡°ë¬¸\n\n"

            section_num += 1

    return prompt

def parse_table_from_text(text_content):
    """í…ìŠ¤íŠ¸ì—ì„œ í‘œ í˜•íƒœì˜ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ Word í‘œ ë°ì´í„°ë¡œ ë³€í™˜"""
    tables_data = []
    lines = text_content.split('\n')
    current_table = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # í‘œì˜ ì‹œì‘ì„ ê°ì§€ (|ê°€ í¬í•¨ëœ ë¼ì¸)
        if '|' in line and len([cell for cell in line.split('|') if cell.strip()]) >= 3:
            # í‘œ í—¤ë”ì¸ì§€ êµ¬ë¶„ (ì²« ë²ˆì§¸ |ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸)
            cells = [cell.strip() for cell in line.split('|') if cell.strip()]

            if current_table is None:
                # ìƒˆ í‘œ ì‹œì‘
                current_table = {'headers': cells, 'rows': []}
                tables_data.append(current_table)
            else:
                # êµ¬ë¶„ì„ ì´ ì•„ë‹Œ ë°ì´í„° í–‰ì¸ì§€ í™•ì¸
                if not all(cell.replace('-', '').replace(':', '').strip() == '' for cell in cells):
                    current_table['rows'].append(cells)
        else:
            # í‘œê°€ ëë‚¨
            if current_table is not None:
                current_table = None

    return tables_data

def add_table_to_doc(doc, table_data):
    """Word ë¬¸ì„œì— í‘œ ì¶”ê°€"""
    if not table_data['headers']:
        return

    # ì—´ ìˆ˜ ê³„ì‚°
    max_cols = len(table_data['headers'])
    for row in table_data['rows']:
        max_cols = max(max_cols, len(row))

    # í–‰ ìˆ˜ ê³„ì‚° (í—¤ë” + ë°ì´í„° í–‰)
    row_count = 1 + len(table_data['rows'])

    if row_count == 1:  # í—¤ë”ë§Œ ìˆëŠ” ê²½ìš° ìŠ¤í‚µ
        return

    # í‘œ ìƒì„±
    table = doc.add_table(rows=row_count, cols=max_cols)
    table.style = 'Table Grid'
    table.autofit = True

    # í—¤ë” ì¶”ê°€
    header_cells = table.rows[0].cells
    for i, header in enumerate(table_data['headers']):
        if i < len(header_cells):
            header_cells[i].text = header
            # í—¤ë” ìŠ¤íƒ€ì¼ë§
            paragraph = header_cells[i].paragraphs[0]
            run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()
            run.bold = True

    # ë°ì´í„° í–‰ ì¶”ê°€
    for row_idx, row_data in enumerate(table_data['rows']):
        if row_idx + 1 < len(table.rows):
            cells = table.rows[row_idx + 1].cells
            for col_idx, cell_data in enumerate(row_data):
                if col_idx < len(cells):
                    cells[col_idx].text = cell_data

def create_comparison_document(pdf_text, search_results, analysis_results, superior_laws_content=None, relevant_guidelines=None):
    """ë¹„êµ ë¶„ì„ ë¬¸ì„œ ìƒì„± í•¨ìˆ˜"""
    doc = Document()
    section = doc.sections[-1]
    section.orientation = WD_ORIENT.LANDSCAPE
    section.page_width = Mm(420)
    section.page_height = Mm(297)

    # ì œëª© ì¶”ê°€
    title = doc.add_heading('ì¡°ë¡€ ë¹„êµ ë¶„ì„ ê²°ê³¼', level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f'ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')

    # ìƒìœ„ë²•ë ¹ ì •ë³´ ì¶”ê°€ (ê³„ì¸µë³„ ê·¸ë£¹í™”)
    if superior_laws_content:
        doc.add_heading('ê²€í† ëœ ìƒìœ„ë²•ë ¹', level=2)
        for law_group in superior_laws_content:
            base_name = law_group['base_name']
            
            # ê·¸ë£¹ ì œëª© ì¶”ê°€
            doc.add_paragraph(f"â—† {base_name}")
            
            # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
            if 'combined_content' in law_group:
                content_length = len(law_group['combined_content'])
                doc.add_paragraph(f"  â€¢ ë³¸ë¬¸ {content_length:,}ì")
            else:
                # ê¸°ì¡´ ë°©ì‹ - ê° ê³„ì¸µë³„ ë²•ë ¹ ì •ë³´ í‘œì‹œ
                for law_type, law_info in law_group['laws'].items():
                    if law_info and 'articles' in law_info:
                        type_name = "ë²•ë¥ " if law_type == 'law' else ("ì‹œí–‰ë ¹" if law_type == 'decree' else "ì‹œí–‰ê·œì¹™")
                        doc.add_paragraph(f"  â€¢ {law_info['law_name']} ({type_name}) - {len(law_info['articles'])}ê°œ ì¡°ë¬¸")
                
                combined_articles = law_group.get('combined_articles', [])
                doc.add_paragraph(f"  ì´ {len(combined_articles)}ê°œ ì¡°ë¬¸ í†µí•© ê²€í† ")
            
            doc.add_paragraph("")
        doc.add_paragraph("")
    
    # í™œìš©ëœ ìì¹˜ë²•ê·œ ìë£Œ ì •ë³´ ì¶”ê°€
    if relevant_guidelines:
        doc.add_heading('í™œìš©ëœ ìì¹˜ë²•ê·œ ì°¸ê³ ìë£Œ', level=2)
        
        # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
        source_groups = {}
        for guideline in relevant_guidelines:
            source_store = guideline.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ìë£Œ')
            if source_store not in source_groups:
                source_groups[source_store] = []
            source_groups[source_store].append(guideline)
        
        for source_store, guidelines in source_groups.items():
            doc.add_paragraph(f"â—† {source_store} ({len(guidelines)}ê°œ ë‚´ìš©)")
            for i, guideline in enumerate(guidelines):
                similarity_score = guideline.get('similarity', 1-guideline.get('distance', 0))
                doc.add_paragraph(f"   â€¢ ë‚´ìš© {i+1} (ìœ ì‚¬ë„: {similarity_score:.3f})")
        doc.add_paragraph("")

    # ê° API ë¶„ì„ ê²°ê³¼ ì¶”ê°€
    for result in analysis_results:
        if 'error' in result:
            doc.add_paragraph(f"{result['model']} API ì˜¤ë¥˜: {result['error']}")
            continue

        content = result['content']

        # ğŸ†• í‘œ íŒŒì‹± ë° ì²˜ë¦¬
        tables_data = parse_table_from_text(content)

        # í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ë³„ë¡œ ì²˜ë¦¬
        lines = content.split('\n')
        current_section = []

        for line in lines:
            line = line.strip()

            # í‘œ ë¼ì¸ì¸ì§€ í™•ì¸ (|ê°€ í¬í•¨ëœ ë¼ì¸)
            if '|' in line and len([cell for cell in line.split('|') if cell.strip()]) >= 3:
                # í‘œ ì‹œì‘ ì „ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                if current_section:
                    for text_line in current_section:
                        text_line_clean = text_line.strip()
                        if text_line_clean:
                            # ì œëª© ë¼ì¸ ì²˜ë¦¬ (1., 2., 3. ë“±ìœ¼ë¡œ ì‹œì‘í•˜ê±°ë‚˜ [ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°)
                            if (text_line_clean.startswith(('1.', '2.', '3.', '4.', '5.')) or
                                text_line_clean.startswith('[') and text_line_clean.endswith(']')):
                                # ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°í•˜ê³  ì œëª©ìœ¼ë¡œ ì¶”ê°€
                                title_text = re.sub(r'[#*`>\-\[\]]+', '', text_line_clean)
                                doc.add_heading(title_text, level=3)
                            else:
                                # ì¼ë°˜ í…ìŠ¤íŠ¸
                                clean_text = re.sub(r'[#*`>]+', '', text_line_clean)
                                if clean_text:
                                    doc.add_paragraph(clean_text)
                    current_section = []

                # í‘œ ì²˜ë¦¬ëŠ” skip (ì´ë¯¸ tables_dataì—ì„œ ì²˜ë¦¬ë¨)
                continue
            else:
                # êµ¬ë¶„ì„ ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                if not (line.replace('-', '').replace(':', '').replace('|', '').strip() == ''):
                    current_section.append(line)

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_section:
            for text_line in current_section:
                text_line_clean = text_line.strip()
                if text_line_clean:
                    if (text_line_clean.startswith(('1.', '2.', '3.', '4.', '5.')) or
                        text_line_clean.startswith('[') and text_line_clean.endswith(']')):
                        title_text = re.sub(r'[#*`>\-\[\]]+', '', text_line_clean)
                        doc.add_heading(title_text, level=3)
                    else:
                        clean_text = re.sub(r'[#*`>]+', '', text_line_clean)
                        if clean_text:
                            doc.add_paragraph(clean_text)

        # ğŸ†• íŒŒì‹±ëœ í‘œë“¤ì„ Word ë¬¸ì„œì— ì¶”ê°€
        for table_data in tables_data:
            add_table_to_doc(doc, table_data)
            doc.add_paragraph("")  # í‘œ ê°„ê²©

    return doc

def main():
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ›ï¸ ê´‘ì—­ì§€ìì²´ ì¡°ë¡€ ê²€ìƒ‰, ë¹„êµ, ë¶„ì„</h1>
        <p>17ê°œ ê´‘ì—­ì§€ìì²´ì˜ ì¡°ë¡€ë¥¼ ê²€ìƒ‰í•˜ê³ , AIë¥¼ í™œìš©í•˜ì—¬ ë¹„êµ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì…ë‹ˆë‹¤.</p>
    </div>
    """, unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“‹ ì‘ì—… ìˆœì„œ")
        st.markdown("""
        <div class="step-card">
            <strong>1ë‹¨ê³„:</strong> ì¡°ë¡€ ê²€ìƒ‰ ë° Word ì €ì¥<br>
            ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì—¬ 17ê°œ ì‹œë„ì˜ ì¡°ë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  3ë‹¨ ë¹„êµ í˜•íƒœë¡œ MS Word ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        </div>
        <div class="step-card">
            <strong>2ë‹¨ê³„:</strong> ì¡°ë¡€ì•ˆ PDF ì—…ë¡œë“œ<br>
            ì œì • ë˜ëŠ” ê°œì •í•  ì¡°ë¡€ì•ˆ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        </div>
        <div class="step-card">
            <strong>3ë‹¨ê³„:</strong> AI ë¹„êµ ë¶„ì„<br>
            ì—…ë¡œë“œí•œ ì¡°ë¡€ì•ˆê³¼ íƒ€ ì‹œë„ ì¡°ë¡€ë¥¼ AIë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ MS Word ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        </div>
        """, unsafe_allow_html=True)

        st.header("ğŸ”‘ API ì„¤ì •")
        gemini_api_key = st.text_input("Gemini API í‚¤", type="password", help="Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        openai_api_key = st.text_input("OpenAI API í‚¤", type="password", help="OpenAI í”Œë«í¼ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        
        st.header("ğŸ”‘ API í‚¤ ì„¤ì • ê°€ì´ë“œ")
        st.markdown("""
        <div class="step-card">
            <strong>ğŸ“‹ API í‚¤ ë°œê¸‰ ë° ì„¤ì • ë°©ë²•</strong><br>
            ì¡°ë¡€ ë¶„ì„ì„ ìœ„í•œ AI ì„œë¹„ìŠ¤ API í‚¤ë¥¼ ë°œê¸‰ë°›ê³  ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
        </div>
        """, unsafe_allow_html=True)
        
        # ğŸ†• ìƒì„¸í•œ API í‚¤ ì„¤ì • ê°€ì´ë“œ
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### ğŸ¤– Gemini API í‚¤ ë°œê¸‰")
            with st.expander("ğŸ“‹ ë‹¨ê³„ë³„ ê°€ì´ë“œ", expanded=False):
                st.markdown("""
                **1. Google AI Studio ì ‘ì†**
                - ë¸Œë¼ìš°ì €ì—ì„œ [aistudio.google.com](https://aistudio.google.com) ì ‘ì†
                - Google ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸

                **2. API í‚¤ ìƒì„±**
                - ì¢Œì¸¡ ë©”ë‰´ì—ì„œ 'API Keys' í´ë¦­
                - 'Create API Key' ë²„íŠ¼ í´ë¦­
                - í”„ë¡œì íŠ¸ ì„ íƒ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)

                **3. API í‚¤ ë³µì‚¬**
                - ìƒì„±ëœ API í‚¤ë¥¼ ë³µì‚¬
                - ì•ˆì „í•œ ê³³ì— ë³´ê´€ (ì¬í™•ì¸ ë¶ˆê°€)

                **4. ì‚¬ìš©ëŸ‰ í™•ì¸**
                - ë¬´ë£Œ í• ë‹¹ëŸ‰: ì›” 1,000ë²ˆ ìš”ì²­
                - ìœ ë£Œ ì „í™˜ ì‹œ ë” ë§ì€ ì‚¬ìš©ëŸ‰ ì œê³µ

                âš ï¸ **ì£¼ì˜**: API í‚¤ëŠ” ê°œì¸ì •ë³´ì´ë¯€ë¡œ íƒ€ì¸ê³¼ ê³µìœ í•˜ì§€ ë§ˆì„¸ìš”!
                """)

        with col2:
            st.markdown("### ğŸ§  OpenAI API í‚¤ ë°œê¸‰")
            with st.expander("ğŸ“‹ ë‹¨ê³„ë³„ ê°€ì´ë“œ", expanded=False):
                st.markdown("""
                **1. OpenAI í”Œë«í¼ ì ‘ì†**
                - ë¸Œë¼ìš°ì €ì—ì„œ [platform.openai.com](https://platform.openai.com) ì ‘ì†
                - OpenAI ê³„ì • ìƒì„±/ë¡œê·¸ì¸

                **2. API í‚¤ ìƒì„±**
                - ìš°ìƒë‹¨ í”„ë¡œí•„ â†’ 'API keys' í´ë¦­
                - 'Create new secret key' ë²„íŠ¼ í´ë¦­
                - í‚¤ ì´ë¦„ ì…ë ¥ í›„ ìƒì„±

                **3. ê²°ì œ ì •ë³´ ë“±ë¡**
                - 'Billing' ë©”ë‰´ì—ì„œ ê²°ì œìˆ˜ë‹¨ ë“±ë¡
                - ì‚¬ìš©ëŸ‰ í•œë„ ì„¤ì • (ê¶Œì¥: $10-20)

                **4. ìš”ê¸ˆ ì •ë³´**
                - GPT-4: ì…ë ¥ í† í°ë‹¹ $0.03/1K, ì¶œë ¥ í† í°ë‹¹ $0.06/1K
                - ì¼ë°˜ì ìœ¼ë¡œ ë¶„ì„ 1íšŒë‹¹ $0.5-2 ì •ë„ ì†Œìš”

                ğŸ’¡ **íŒ**: ì²˜ìŒì—ëŠ” ë‚®ì€ í•œë„ë¡œ ì‹œì‘í•˜ì—¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•´ë³´ì„¸ìš”.
                """)

        # ë²¡í„°ìŠ¤í† ì–´ ìë™ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¡°ìš©íˆ ì²˜ë¦¬)
        vector_store_path = "enhanced_vectorstore_20250914_101739.pkl"
        if st.session_state.vector_store is None and os.path.exists(vector_store_path):
            try:
                with open(vector_store_path, 'rb') as f:
                    st.session_state.vector_store = pickle.load(f)
            except Exception:
                pass  # ì¡°ìš©íˆ ì‹¤íŒ¨

    # ë©”ì¸ ì»¨í…ì¸ 
    tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ ì¡°ë¡€ ê²€ìƒ‰", "2ï¸âƒ£ PDF ì—…ë¡œë“œ", "3ï¸âƒ£ AI ë¶„ì„"])

    with tab1:
        st.header("ì¡°ë¡€ ê²€ìƒ‰")
        
        # ê²€ìƒ‰ í¼ (Enter í‚¤ ì§€ì›)
        with st.form(key="search_form"):
            col1, col2 = st.columns([3, 1])
            with col1:
                search_query = st.text_input(
                    "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (í‚¤ì›Œë“œ)", 
                    placeholder="ì˜ˆ: ì²­ë…„ì§€ì› (Enter í‚¤ë¡œë„ ê²€ìƒ‰ ê°€ëŠ¥)", 
                    value=st.session_state.search_query,
                    help="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•œ í›„ Enter í‚¤ë¥¼ ëˆ„ë¥´ê±°ë‚˜ ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                )
            with col2:
                search_button = st.form_submit_button("ğŸ” ê²€ìƒ‰", type="primary")

        # ê²€ìƒ‰ ì‹¤í–‰ (Enter í‚¤ ë˜ëŠ” ë²„íŠ¼ í´ë¦­ ì‹œ)
        if search_button and search_query.strip():
            st.session_state.search_query = search_query.strip()
            st.session_state.word_doc_ready = False  # ë¬¸ì„œ ì¤€ë¹„ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.selected_ordinances = []  # ì„ íƒëœ ì¡°ë¡€ ì´ˆê¸°í™”
            
            with st.spinner("ê²€ìƒ‰ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                try:
                    results, total_count = search_ordinances(search_query.strip())
                    st.session_state.search_results = results
                    # ì´ˆê¸°ì—ëŠ” ëª¨ë“  ì¡°ë¡€ë¥¼ ì„ íƒëœ ìƒíƒœë¡œ ì„¤ì •
                    st.session_state.selected_ordinances = list(range(len(results)))
                    st.success(f"ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(results)}ê±´ì˜ ì¡°ë¡€ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.session_state.search_results = []

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œ ì¡°ë¡€ ì„ íƒ ë° Word ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥
        if st.session_state.search_results:
            results = st.session_state.search_results
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
            if not st.session_state.word_doc_ready:
                st.success(f"ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(results)}ê±´ì˜ ì¡°ë¡€ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì¡°ë¡€ ì„ íƒ ì„¹ì…˜
            st.subheader("ğŸ“‹ Word ë¬¸ì„œì— í¬í•¨í•  ì¡°ë¡€ ì„ íƒ")
            
            # ì „ì²´ ì„ íƒ/í•´ì œ ë²„íŠ¼
            col1, col2, col3 = st.columns([1, 1, 2])
            
            with col1:
                if st.button("âœ… ì „ì²´ ì„ íƒ", key="select_all_btn"):
                    st.session_state.selected_ordinances = list(range(len(results)))
                    st.rerun()
            
            with col2:
                if st.button("âŒ ì „ì²´ í•´ì œ", key="deselect_all_btn"):
                    st.session_state.selected_ordinances = []
                    st.rerun()
            
            with col3:
                selected_count = len(st.session_state.selected_ordinances)
                st.markdown(f"**ì„ íƒëœ ì¡°ë¡€: {selected_count}ê°œ / ì´ {len(results)}ê°œ**")
            
            # ì¡°ë¡€ ì„ íƒ ì²´í¬ë°•ìŠ¤
            st.markdown("---")
            
            # ì¡°ë¡€ë³„ ì²´í¬ë°•ìŠ¤ í‘œì‹œ
            for idx, result in enumerate(results):
                # ğŸ†• ë‹¨ìˆœí™”: ì²´í¬ë°•ìŠ¤ ìƒíƒœë¥¼ ì§ì ‘ ê´€ë¦¬
                is_selected = idx in st.session_state.selected_ordinances
                checkbox_key = f"ordinance_checkbox_{idx}"

                # ì²´í¬ë°•ìŠ¤ì™€ ì¡°ë¡€ëª…ì„ í•œ ì¤„ì— í‘œì‹œ
                current_checked = st.checkbox(
                    f"**{result['metro']}** - {result['name']}",
                    value=is_selected,
                    key=checkbox_key
                )

                # ğŸ†• ìƒíƒœ ë³€ê²½ ê°ì§€ ë° ì¦‰ì‹œ ë°˜ì˜
                if current_checked != is_selected:
                    if current_checked:
                        # ì²´í¬ë¨ - ëª©ë¡ì— ì¶”ê°€
                        if idx not in st.session_state.selected_ordinances:
                            st.session_state.selected_ordinances.append(idx)
                    else:
                        # ì²´í¬ í•´ì œë¨ - ëª©ë¡ì—ì„œ ì œê±°
                        if idx in st.session_state.selected_ordinances:
                            st.session_state.selected_ordinances.remove(idx)
            
            st.markdown("---")
            
            # Word ë¬¸ì„œ ìƒì„± ë²„íŠ¼
            col1, col2 = st.columns([1, 1])
            
            with col1:
                # ì„ íƒëœ ì¡°ë¡€ê°€ ìˆì„ ë•Œë§Œ ìƒì„± ë²„íŠ¼ í™œì„±í™”
                disabled = len(st.session_state.selected_ordinances) == 0
                
                if st.button("ğŸ“„ ì„ íƒëœ ì¡°ë¡€ë¡œ Word ë¬¸ì„œ ìƒì„±", type="secondary", key="create_word_btn", disabled=disabled):
                    if st.session_state.selected_ordinances:
                        try:
                            with st.spinner("Word ë¬¸ì„œ ìƒì„± ì¤‘..."):
                                # ì„ íƒëœ ì¡°ë¡€ë§Œ í•„í„°ë§
                                selected_results = [results[i] for i in st.session_state.selected_ordinances]
                                
                                # Word ë¬¸ì„œ ìƒì„±
                                doc = create_word_document(st.session_state.search_query, selected_results)
                                
                                # Word ë¬¸ì„œë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
                                doc_io = io.BytesIO()
                                doc.save(doc_io)
                                doc_io.seek(0)
                                doc_bytes = doc_io.getvalue()
                                
                                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                                st.session_state.word_doc_data = doc_bytes
                                st.session_state.word_doc_ready = True
                                
                            st.success(f"âœ… ì„ íƒëœ {len(selected_results)}ê°œ ì¡°ë¡€ë¡œ Word ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
                            
                        except Exception as e:
                            st.error(f"âŒ Word ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                    else:
                        st.warning("ì¡°ë¡€ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
                if disabled:
                    st.caption("âš ï¸ ì¡°ë¡€ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            with col2:
                # Word ë¬¸ì„œê°€ ì¤€ë¹„ë˜ë©´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
                if st.session_state.word_doc_ready and st.session_state.word_doc_data:
                    filename = f"ì¡°ë¡€_ê²€ìƒ‰ê²°ê³¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
                    st.download_button(
                        label="ğŸ’¾ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=st.session_state.word_doc_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key="download_word_btn"
                    )
            # ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ (ì¡°ë¡€ ë‚´ìš© í™•ì¸ìš©)
            st.subheader("ğŸ“– ì¡°ë¡€ ë‚´ìš© ìƒì„¸ë³´ê¸°")
            
            for idx, result in enumerate(results):
                # ğŸ†• ë‹¨ìˆœí™”: ì„ íƒ ìƒíƒœë§Œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                is_selected = idx in st.session_state.selected_ordinances
                status = " âœ… ì„ íƒë¨" if is_selected else " â­• ì„ íƒì•ˆë¨"

                with st.expander(f"{result['metro']} - {result['name']}{status}", expanded=False):
                    st.markdown(f"<div class='metro-name'>{result['metro']}</div>", unsafe_allow_html=True)
                    st.markdown(f"<div class='law-title'>{result['name']}</div>", unsafe_allow_html=True)
                    
                    if result['content']:
                        for article_idx, article in enumerate(result['content']):
                            st.markdown(f"**ì œ{article_idx+1}ì¡°**")
                            st.markdown(article)
                            st.markdown("---")
                    else:
                        st.markdown("*(ì¡°ë¬¸ ì—†ìŒ)*")
        
        elif search_button and not search_query.strip():
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif not st.session_state.search_results:
            st.info("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  Enter í‚¤ë¥¼ ëˆ„ë¥´ê±°ë‚˜ ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    with tab2:
        st.header("ì¡°ë¡€ì•ˆ PDF ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader("ì œì • ë˜ëŠ” ê°œì •í•  ì¡°ë¡€ì•ˆ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])
        
        if uploaded_file is not None:
            st.session_state.uploaded_pdf = uploaded_file
            st.success(f"íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {uploaded_file.name}")
            
            # PDF ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            if st.checkbox("PDF ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
                with st.spinner("PDF ë‚´ìš©ì„ ì½ëŠ” ì¤‘..."):
                    pdf_text = extract_pdf_text(uploaded_file)
                    if pdf_text:
                        st.text_area("PDF ë‚´ìš©", pdf_text[:2000] + "..." if len(pdf_text) > 2000 else pdf_text, height=300)
                    else:
                        st.error("PDF ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    with tab3:
        st.header("AI ë¹„êµ ë¶„ì„")
        
        # ì¡°ê±´ í™•ì¸ - PDFê°€ ì—…ë¡œë“œë˜ê³  API í‚¤ê°€ ìˆìœ¼ë©´ ë¶„ì„ ê°€ëŠ¥
        pdf_uploaded = st.session_state.uploaded_pdf is not None
        has_api_key = bool(gemini_api_key or openai_api_key)
        has_search_results = bool(st.session_state.search_results)
        
        if not pdf_uploaded:
            st.warning("ğŸ“„ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif not has_api_key:
            st.warning("ğŸ”‘ API í‚¤ë¥¼ í•˜ë‚˜ ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # ê²€ìƒ‰ ê²°ê³¼ ì—¬ë¶€ì— ë”°ë¼ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
            if not has_search_results:
                st.info("ğŸ’¡ **ìµœì´ˆ ì œì • ì¡°ë¡€ ë¶„ì„**")
                st.markdown("""
                ê²€ìƒ‰ëœ íƒ€ ì‹œë„ ì¡°ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
                - ğŸ†• **ìµœì´ˆ ì œì • ì¡°ë¡€**: 17ê°œ ì‹œë„ ì¤‘ ìµœì´ˆë¡œ ì œì •ë˜ëŠ” ì¡°ë¡€
                - ğŸ” **ê²€ìƒ‰ì–´ ë¶ˆì¼ì¹˜**: ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ê²€ìƒ‰ í›„ ë¶„ì„ ê¶Œì¥
                
                ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ì¡°ë¡€ì•ˆì˜ **ë²•ì  ê²€í† **ì™€ **ìƒìœ„ë²•ë ¹ ìœ„ë°˜ ì—¬ë¶€** ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
                """)
            else:
                st.success(f"ğŸ“Š {len(st.session_state.search_results)}ê°œì˜ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        # ë¶„ì„ ê°€ëŠ¥í•œ ì¡°ê±´ì¼ ë•Œ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
        if pdf_uploaded and has_api_key:
            # ê²€ìƒ‰ì–´ ì…ë ¥ (ì„ íƒì‚¬í•­)
            search_query_analysis = st.text_input(
                "ê²€ìƒ‰ì–´ (ë¶„ì„ìš©)", 
                value=st.session_state.search_query if st.session_state.search_query else "", 
                key="analysis_query",
                help="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì„ íƒì‚¬í•­)"
            )
            
            # ë¶„ì„ íƒ€ì… í‘œì‹œ (ì„ íƒëœ ì¡°ë¡€ ìˆ˜ ë°˜ì˜)
            if not has_search_results:
                analysis_type = "ìµœì´ˆ ì œì • ì¡°ë¡€ ë¶„ì„"
            elif hasattr(st.session_state, 'selected_ordinances') and st.session_state.selected_ordinances:
                selected_count = len(st.session_state.selected_ordinances)
                analysis_type = f"ì„ íƒëœ {selected_count}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµ ë¶„ì„"
            else:
                analysis_type = f"ì „ì²´ {len(st.session_state.search_results)}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ì™€ ë¹„êµ ë¶„ì„"
            st.markdown(f"**ë¶„ì„ ìœ í˜•**: {analysis_type}")
            
            # PKL íŒŒì¼ ì°¸ê³  ì˜µì…˜ (ë¬¸ì œ ë°œê²¬ ì‹œ ìë™ í™œìš©)
            use_pkl_auto = st.checkbox(
                "ğŸ” ë¬¸ì œ ë°œê²¬ ì‹œ PKL íŒŒì¼ ìë™ ì°¸ê³ ", 
                value=True, 
                help="Geminiê°€ ë²•ì  ë¬¸ì œì ì„ ë°œê²¬í•œ ê²½ìš° ìë™ìœ¼ë¡œ ë¬´ë£Œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¸ê³ í•˜ì—¬ ê·¼ê±°ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤."
            )
            
            # ğŸ†• ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë¨¼ì € í‘œì‹œ
            if hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
                st.info("ğŸ’¾ **ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤**")

                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                if hasattr(st.session_state, 'analysis_metadata'):
                    metadata = st.session_state.analysis_metadata
                    st.caption(f"ğŸ“… ë¶„ì„ ì‹œê°„: {metadata.get('analysis_timestamp', 'ì•Œ ìˆ˜ ì—†ìŒ')}")

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ğŸ“‹ ì´ì „ ë¶„ì„ ê²°ê³¼ ë³´ê¸°", use_container_width=True):
                        st.session_state.show_previous_analysis = True
                        st.rerun()
                with col2:
                    if st.button("ğŸ”„ ìƒˆë¡œ ë¶„ì„í•˜ê¸°", use_container_width=True):
                        # ê¸°ì¡´ ê²°ê³¼ ì´ˆê¸°í™”
                        if hasattr(st.session_state, 'analysis_results'):
                            del st.session_state.analysis_results
                        if hasattr(st.session_state, 'analysis_metadata'):
                            del st.session_state.analysis_metadata
                        if hasattr(st.session_state, 'show_previous_analysis'):
                            del st.session_state.show_previous_analysis
                        st.rerun()

            # ì´ì „ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            if hasattr(st.session_state, 'show_previous_analysis') and st.session_state.show_previous_analysis and hasattr(st.session_state, 'analysis_results'):
                analysis_results = st.session_state.analysis_results
                metadata = st.session_state.analysis_metadata

                st.markdown("---")
                st.subheader("ğŸ“‹ ì €ì¥ëœ AI ë¶„ì„ ê²°ê³¼")

                # ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€ (ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
                has_problems = metadata.get('has_problems', False)
                relevant_guidelines = metadata.get('relevant_guidelines')
                loaded_stores = metadata.get('loaded_stores')
                is_first_ordinance = metadata.get('is_first_ordinance', False)

                if has_problems and relevant_guidelines and loaded_stores:
                    st.success(f"ğŸ¯ **ë³µí•© ìë£Œ ë³´ê°• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ {len(loaded_stores)}ê°œ ìë£Œ ì°¸ê³  â†’ ë³´ê°• ë¶„ì„")
                elif has_problems and relevant_guidelines:
                    st.success("ğŸ¯ **ì§€ëŠ¥í˜• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ PKL ì°¸ê³  â†’ ë³´ê°• ë¶„ì„")
                elif has_problems:
                    st.info("âš ï¸ **ë¬¸ì œì  íƒì§€ ë¶„ì„ ì™„ë£Œ**: PKL ì°¸ê³  ì—†ì´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                else:
                    st.success("âœ… **ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ**: íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")

                # ë¶„ì„ ê²°ê³¼ ìš”ì•½
                analysis_count = len([r for r in analysis_results if 'error' not in r])
                if analysis_count > 0:
                    # ğŸ†• ì €ì¥ëœ ë©”íƒ€ë°ì´í„°ì—ì„œ ì„ íƒëœ ì¡°ë¡€ ìˆ˜ ë°˜ì˜
                    if is_first_ordinance:
                        analysis_type_text = "ìµœì´ˆ ì œì • ì¡°ë¡€"
                    else:
                        saved_search_results = metadata.get('search_results_for_analysis', [])
                        selected_count = len(saved_search_results)
                        analysis_type_text = f"ì„ íƒëœ {selected_count}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ ë¹„êµ"
                    st.markdown(f"**ğŸ“‹ ë¶„ì„ ìœ í˜•**: {analysis_type_text}")
                    st.markdown(f"**ğŸ¤– ìˆ˜í–‰ëœ ë¶„ì„**: {analysis_count}ê°œ")
                    if relevant_guidelines:
                        guideline_count = len(relevant_guidelines) if isinstance(relevant_guidelines, list) else 0
                        st.markdown(f"**ğŸ“š ì°¸ê³  ê°€ì´ë“œë¼ì¸**: {guideline_count}ê°œ")

                # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                for result in analysis_results:
                    if 'error' not in result:
                        final_report = result
                        # ëª¨ë¸ì— ë”°ë¥¸ êµ¬ë¶„ í‘œì‹œ
                        if "ë³´ê°•" in final_report['model']:
                            st.success("ğŸ¯ **ë³µí•© ìë£Œ ì°¸ê³  ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                            st.caption(f"ğŸ“š **í™œìš© ëª¨ë¸**: {final_report['model']}")
                        elif "PKL ë³´ê°•" in final_report['model']:
                            st.success("ğŸ¯ **PKL ê°€ì´ë“œë¼ì¸ ì°¸ê³  ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                        elif "OpenAI" in final_report['model']:
                            st.info("ğŸ“Š **OpenAI ì¶”ê°€ ë¶„ì„ ê²°ê³¼**")
                        else:
                            st.info("ğŸ¤– **Gemini ê¸°ë³¸ ë¶„ì„ ê²°ê³¼**")
                        # ë³´ê³ ì„œ ë‚´ìš©
                        st.markdown(final_report['content'])

                # ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
                for result in analysis_results:
                    if 'error' in result:
                        st.error(f"âŒ {result['model']} ì˜¤ë¥˜: {result['error']}")

                # Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ (ë©”íƒ€ë°ì´í„°ì—ì„œ ë³µì›)
                with st.spinner("ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ìƒì„± ì¤‘..."):
                    superior_laws_content = metadata.get('superior_laws_content')
                    search_results_for_analysis = metadata.get('search_results_for_analysis')
                    pdf_text = metadata.get('pdf_text')
                    doc = create_comparison_document(pdf_text, search_results_for_analysis, analysis_results, superior_laws_content, relevant_guidelines)
                    doc_io = io.BytesIO()
                    doc.save(doc_io)
                    doc_bytes = doc_io.getvalue()
                    # íŒŒì¼ëª… ì„¤ì •
                    if has_problems and relevant_guidelines and loaded_stores:
                        stores_count = len(loaded_stores)
                        filename_prefix = f"ë³µí•©ìë£Œë³´ê°•ë¶„ì„({stores_count}ê°œìë£Œ)" if is_first_ordinance else f"ì¡°ë¡€ë¹„êµ_ë³µí•©ìë£Œë¶„ì„({stores_count}ê°œìë£Œ)"
                    elif has_problems and relevant_guidelines:
                        filename_prefix = "ì§€ëŠ¥í˜•PKLë³´ê°•ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_PKLë³´ê°•ë¶„ì„"
                    elif has_problems:
                        filename_prefix = "ë¬¸ì œì íƒì§€ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_ë¬¸ì œì ë¶„ì„"
                    else:
                        filename_prefix = "ìµœì´ˆì¡°ë¡€_ê¸°ë³¸ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€_ê¸°ë³¸ë¹„êµë¶„ì„"
                    st.download_button(
                        label="ğŸ“„ ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=doc_bytes,
                        file_name=f"{filename_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key="download_previous_analysis"
                    )

                st.markdown("---")
                st.markdown("ğŸ’¡ **ìƒˆë¡œ ë¶„ì„í•˜ë ¤ë©´ ìœ„ì˜ 'ğŸ”„ ìƒˆë¡œ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.**")

            else:
                # ì €ì¥ëœ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ìƒˆ ë¶„ì„ì„ ì„ íƒí•œ ê²½ìš°ë§Œ ë¶„ì„ ì‹œì‘ ë²„íŠ¼ í‘œì‹œ
                # ğŸ†• ì„ íƒëœ ì¡°ë¡€ê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  í‘œì‹œ
                if has_search_results and hasattr(st.session_state, 'selected_ordinances') and not st.session_state.selected_ordinances:
                    st.warning("âš ï¸ ë¹„êµí•  ì¡°ë¡€ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¡°ë¡€ ê²€ìƒ‰ íƒ­ì—ì„œ ì¡°ë¡€ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ì„ íƒ ì—†ì´ ìµœì´ˆ ì œì • ì¡°ë¡€ ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”.")

                if st.button("ğŸ¤– AI ë¶„ì„ ì‹œì‘", type="primary"):
                    with st.spinner("AIê°€ ì¡°ë¡€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        pdf_text = extract_pdf_text(st.session_state.uploaded_pdf)
                    
                    if not pdf_text:
                        st.error("PDF í…ìŠ¤íŠ¸ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        # 1ë‹¨ê³„: ìƒìœ„ë²•ë ¹ ì¶”ì¶œ
                        st.info("ğŸ“‹ 1ë‹¨ê³„: ì¡°ë¡€ì•ˆì—ì„œ ìƒìœ„ë²•ë ¹ì„ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                        superior_laws = extract_superior_laws(pdf_text)
                        
                        if superior_laws:
                            st.success(f"âœ… {len(superior_laws)}ê°œì˜ ìƒìœ„ë²•ë ¹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:")
                            for law in superior_laws:
                                st.markdown(f"   â€¢ {law}")
                            
                            # 2ë‹¨ê³„: ìƒìœ„ë²•ë ¹ ë‚´ìš© ì¡°íšŒ
                            st.info("ğŸ“š 2ë‹¨ê³„: êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ ìƒìœ„ë²•ë ¹ ë‚´ìš©ì„ ì¡°íšŒí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                            superior_laws_content = get_all_superior_laws_content(superior_laws)
                            
                            if superior_laws_content:
                                st.success(f"âœ… {len(superior_laws_content)}ê°œì˜ ìƒìœ„ë²•ë ¹ ê·¸ë£¹ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤:")
                                total_articles = 0
                                for i, law_group in enumerate(superior_laws_content):
                                    base_name = law_group['base_name']
                                    
                                    # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
                                    if 'combined_content' in law_group:
                                        content_length = len(law_group['combined_content'])
                                        st.markdown(f"   â€¢ **{base_name}**: ë³¸ë¬¸ {content_length:,}ì")
                                        total_articles += 1  # í•˜ë‚˜ì˜ ì—°ê²°ëœ ë²•ë ¹ìœ¼ë¡œ ì¹´ìš´íŠ¸
                                    else:
                                        # ê¸°ì¡´ ë°©ì‹
                                        available_laws = []
                                        group_articles = 0
                                        
                                        for law_type, law_info in law_group['laws'].items():
                                            if law_info:
                                                type_name = {"law": "ë²•ë¥ ", "decree": "ì‹œí–‰ë ¹", "rule": "ì‹œí–‰ê·œì¹™"}[law_type]
                                                article_count = len(law_info.get('articles', []))
                                                available_laws.append(f"{type_name}({article_count})")
                                                group_articles += article_count
                                        
                                        st.markdown(f"   â€¢ **{base_name}**: {', '.join(available_laws)} = ì´ {group_articles}ê°œ ì¡°ë¬¸")
                                        total_articles += group_articles
                                
                                st.markdown(f"   **ì „ì²´ ì¡°ë¬¸ ìˆ˜**: {total_articles}ê°œ")
                                
                                # ğŸ†• ìƒìœ„ë²•ë ¹ ë³¸ë¬¸ ë‚´ìš© ë””ë²„ê¹… í‘œì‹œ

                                # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
                                if 'combined_content' in law_group and law_group['combined_content']:
                                    content = law_group['combined_content']
                                    st.markdown(f"**ë³¸ë¬¸ ê¸¸ì´**: {len(content):,}ì")
                                    st.text_area(
                                        f"{law_group['base_name']} ë³¸ë¬¸",
                                        content,
                                        height=200,
                                        key=f"content_{i}"
                                    )
                                else:
                                    # ê°œë³„ ë²•ë ¹ë³„ í‘œì‹œ
                                    for law_type, law_info in law_group['laws'].items():
                                        if law_info and 'articles' in law_info:
                                            type_name = {"law": "ë²•ë¥ ", "decree": "ì‹œí–‰ë ¹", "rule": "ì‹œí–‰ê·œì¹™"}[law_type]
                                            st.markdown(f"#### {type_name}")

                                            # ì¡°ë¬¸ë³„ ë‚´ìš© í‘œì‹œ (ì²˜ìŒ 5ê°œë§Œ)
                                            for j, article in enumerate(law_info['articles'][:5]):
                                                st.markdown(f"**ì œ{article.get('number', '?')}ì¡°** {article.get('title', '')}")
                                                content = article.get('content', '')[:500]
                                                st.markdown(f"```\n{content}{'...' if len(article.get('content', '')) > 500 else ''}\n```")

                                            if len(law_info['articles']) > 5:
                                                st.markdown(f"... (ì´ {len(law_info['articles'])}ê°œ ì¡°ë¬¸ ì¤‘ 5ê°œë§Œ í‘œì‹œ)")

                                st.markdown("---")
                                
                                # 2-1ë‹¨ê³„: ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„
                                st.info("âš–ï¸ 2-1ë‹¨ê³„: ì¡°ë¡€ì™€ ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                                try:
                                    comparison_results = analyze_ordinance_vs_superior_laws(pdf_text, superior_laws_content)
                                    
                                    if comparison_results and isinstance(comparison_results, list) and len(comparison_results) > 0:
                                        st.warning(f"âš ï¸ {len(comparison_results)}ê°œ ì¡°ë¬¸ì—ì„œ ì ì¬ì  ë¬¸ì œì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                        
                                        with st.expander("ğŸ” ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ê²°ê³¼", expanded=True):
                                            for i, result in enumerate(comparison_results):
                                                st.markdown(f"**ğŸ” {result['ordinance_article']}**")
                                                st.markdown(f"ì¡°ë¡€ ë‚´ìš©: {result['ordinance_content'][:300]}...")
                                                
                                                if result['delegation_issues']:
                                                    st.error("âš ï¸ **ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ê°€ëŠ¥ì„± ë°œê²¬**")
                                                    for issue in result['delegation_issues']:
                                                        st.markdown(f"- **ê´€ë ¨ ìƒìœ„ë²•ë ¹**: {issue['superior_law']} {issue['superior_article']}")
                                                        st.markdown(f"- **ë¬¸ì œì **: {issue['description']}")
                                                        st.markdown(f"- **ìƒìœ„ë²•ë ¹ ë‚´ìš©**: {issue['superior_content'][:200]}...")
                                                
                                                if result['superior_law_conflicts']:
                                                    st.error("ğŸš¨ **ìƒìœ„ë²•ë ¹ ì¶©ëŒ ê°€ëŠ¥ì„± ë°œê²¬**")
                                                    for conflict in result['superior_law_conflicts']:
                                                        st.markdown(f"- **ê´€ë ¨ ìƒìœ„ë²•ë ¹**: {conflict['superior_law']} {conflict['superior_article']}")
                                                        st.markdown(f"- **ì¶©ëŒ ìœ í˜•**: {conflict['conflict_type']}")
                                                        st.markdown(f"- **ìƒìœ„ë²•ë ¹ ë‚´ìš©**: {conflict['superior_content'][:200]}...")
                                                
                                                st.markdown("---")
                                    else:
                                        st.success("âœ… ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµì—ì„œ ëª…ë°±í•œ ì¶©ëŒì´ë‚˜ ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ë¬¸ì œë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                        
                                except Exception as e:
                                    st.error(f"ìƒìœ„ë²•ë ¹ ì§ì ‘ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                                
                                # ìƒìœ„ë²•ë ¹ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ê³„ì¸µë³„ ê·¸ë£¹í™”)
                                    for law_group in superior_laws_content:
                                        base_name = law_group['base_name']
                                        
                                        # ì—°ê²°ëœ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°
                                        if 'combined_content' in law_group:
                                            content_preview = law_group['combined_content'][:500] + "..." if len(law_group['combined_content']) > 500 else law_group['combined_content']
                                            with st.expander(f"ğŸ“‹ {base_name} ({len(law_group['combined_content']):,}ì)", expanded=False):
                                                st.text_area("ë³¸ë¬¸ ë‚´ìš©", content_preview, height=300, disabled=True)
                                        else:
                                            # ê¸°ì¡´ ë°©ì‹
                                            with st.expander(f"ğŸ“‹ {base_name} ê³„ì¸µ ({len(law_group.get('combined_articles', []))}ê°œ ì¡°ë¬¸)", expanded=False):
                                                
                                                # ê³„ì¸µë³„ ë²•ë ¹ ì •ë³´ í‘œì‹œ
                                                st.markdown("**ğŸ“š í¬í•¨ëœ ë²•ë ¹:**")
                                                for law_type, law_info in law_group['laws'].items():
                                                    if law_info and 'articles' in law_info:
                                                        type_name = "ë²•ë¥ " if law_type == 'law' else ("ì‹œí–‰ë ¹" if law_type == 'decree' else "ì‹œí–‰ê·œì¹™")
                                                        st.markdown(f"- [{type_name}] {law_info['law_name']} ({len(law_info['articles'])}ê°œ ì¡°ë¬¸)")
                                                
                                                st.markdown("\n**ğŸ“– í†µí•© ì¡°ë¬¸ (ì²˜ìŒ 5ê°œ):**")
                                                combined_articles = law_group.get('combined_articles', [])
                                                for article in combined_articles[:5]:  
                                                    st.markdown(f"**{article['number']} {article['title']}**")
                                                    st.markdown(article['content'][:200] + "..." if len(article['content']) > 200 else article['content'])
                                                    st.markdown("---")
                                                if len(combined_articles) > 5:
                                                    st.markdown(f"*(ì´ {len(combined_articles)}ê°œ ì¡°ë¬¸ ì¤‘ 5ê°œë§Œ í‘œì‹œ)*")
                            else:
                                st.warning("âš ï¸ ìƒìœ„ë²•ë ¹ ë‚´ìš© ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                        else:
                            st.info("â„¹ï¸ ì¡°ë¡€ì•ˆì—ì„œ ëª…ì‹œì ì¸ ìƒìœ„ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            superior_laws_content = None
                        
                        # 3ë‹¨ê³„: Gemini 1ì°¨ ë¶„ì„ (ë¬¸ì œì  íƒì§€)
                        st.info("ğŸ¤– 3ë‹¨ê³„: Gemini 1ì°¨ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                        analysis_results = []
                        is_first_ordinance = not has_search_results

                        # ğŸ†• ì„ íƒëœ ì¡°ë¡€ë§Œ ë¶„ì„ì— ì‚¬ìš©
                        if has_search_results and hasattr(st.session_state, 'selected_ordinances'):
                            selected_results = [st.session_state.search_results[i] for i in st.session_state.selected_ordinances if i < len(st.session_state.search_results)]
                            search_results_for_analysis = selected_results
                            st.info(f"ğŸ“‹ ì„ íƒëœ {len(search_results_for_analysis)}ê°œ ì¡°ë¡€ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                        else:
                            search_results_for_analysis = st.session_state.search_results if has_search_results else []
                        
                        # Gemini 1ì°¨ ë¶„ì„ (ë¬¸ì œì  íƒì§€ìš©)
                        first_analysis = None
                        has_problems = False
                        
                        if gemini_api_key:
                            try:
                                # comprehensive_analysis_results ì´ˆê¸°í™”
                                comprehensive_analysis_results = None
                                
                                genai.configure(api_key=gemini_api_key)
                                model = genai.GenerativeModel('gemini-2.0-flash-lite')
                                
                                # ğŸ†• 1ì°¨ ì˜ˆë¹„ ë¶„ì„ìœ¼ë¡œ ìœ„ë²•ì„± ì˜ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                                st.info("ğŸ” 1ì°¨ ì˜ˆë¹„ ë¶„ì„: ìœ„ë²•ì„± ì˜ì‹¬ ì‚¬ìœ  íŒŒì•… ì¤‘...")
                                preliminary_analysis, legality_keywords = perform_preliminary_analysis(
                                    pdf_text, superior_laws_content, search_results_for_analysis, gemini_api_key
                                )

                                if preliminary_analysis:
                                    st.success("âœ… 1ì°¨ ì˜ˆë¹„ ë¶„ì„ ì™„ë£Œ")
                                    with st.expander("ğŸ” 1ì°¨ ì˜ˆë¹„ ë¶„ì„ ê²°ê³¼ ë³´ê¸°"):
                                        st.markdown(preliminary_analysis[:1500] + "..." if len(preliminary_analysis) > 1500 else preliminary_analysis)

                                # íŒë¡€ ê²€ìƒ‰ì€ PKL ë¶„ì„ ì´í›„ë¡œ ì´ë™
                                precedents = []
                                precedents_content = []
                                legal_principles = []

                                # ğŸ†• 2ì°¨ ìµœì¢… ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ (1ì°¨ ë¶„ì„ + íŒë¡€ ë²•ë¦¬ ì¢…í•©)
                                st.info("ğŸ“Š 2ì°¨ ìµœì¢… ë¶„ì„: íŒë¡€ ë²•ë¦¬ ì ìš©í•œ ì¢…í•© ìœ„ë²•ì„± íŒë‹¨ ì¤‘...")
                                theoretical_results = st.session_state.get('theoretical_results', None)
                                guideline_results = st.session_state.get('guideline_results', None)

                                # ê¸°ì¡´ í•¨ìˆ˜ì— preliminary_analysisë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©
                                final_prompt = create_analysis_prompt(
                                    pdf_text, search_results_for_analysis, superior_laws_content,
                                    guideline_results, is_first_ordinance, comprehensive_analysis_results,
                                    theoretical_results, precedents_content, legal_principles
                                )

                                # 1ì°¨ ì˜ˆë¹„ ë¶„ì„ ê²°ê³¼ë¥¼ ìµœì¢… í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
                                if preliminary_analysis:
                                    final_prompt = (
                                        "**ğŸ” 1ì°¨ ì˜ˆë¹„ ë¶„ì„ ê²°ê³¼**\n"
                                        "ë‹¤ìŒì€ ìœ„ë²•ì„± ì˜ì‹¬ ì‚¬ìœ ë¥¼ íŒŒì•…í•œ 1ì°¨ ì˜ˆë¹„ ë¶„ì„ ê²°ê³¼ì´ë‹¤.\n"
                                        "ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ìœ„ë²•ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ë¼.\n"
                                        "---\n" +
                                        preliminary_analysis +
                                        "\n---\n\n" +
                                        final_prompt
                                    )
                                
                                # ğŸ†• Gemini ì „ì†¡ í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹… í‘œì‹œ

                                # ìƒìœ„ë²•ë ¹ ë‚´ìš© ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                if "ìƒìœ„ë²•ë ¹ë“¤ì˜ ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš©" in final_prompt:
                                    law_start = final_prompt.find("ìƒìœ„ë²•ë ¹ë“¤ì˜ ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš©")
                                    law_end = final_prompt.find("3. [ê²€í†  ì‹œ ìœ ì˜ì‚¬í•­]")
                                    if law_end == -1:
                                        law_end = law_start + 5000  # ê¸°ë³¸ê°’

                                    law_content = final_prompt[law_start:law_end]
                                    st.markdown(f"**ìƒìœ„ë²•ë ¹ ë‚´ìš© ê¸¸ì´**: {len(law_content):,}ì")

                                    st.text_area(
                                        "ìƒìœ„ë²•ë ¹ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ë‚´ìš©",
                                                law_content[:3000] + "..." if len(law_content) > 3000 else law_content,
                                                height=300,
                                                key="prompt_law_content"
                                            )
                                # ì „ì²´ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ì²˜ìŒ 2000ìë§Œ)
                                st.text_area(
                                    "ìµœì¢… í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 2000ì)",
                                    final_prompt[:2000] + "..." if len(final_prompt) > 2000 else final_prompt,
                                    height=400,
                                    key="final_prompt"
                                )

                                response = model.generate_content(final_prompt)
                                
                                if response and hasattr(response, 'text') and response.text:
                                    first_analysis = response.text
                                    
                                    # ë¬¸ì œì  í‚¤ì›Œë“œ íƒì§€
                                    problem_keywords = [
                                        "ìœ„ë°˜", "ë¬¸ì œ", "ì¶©ëŒ", "ë¶€ì ì ˆ", "ê°œì„ ", "ìˆ˜ì •", "ë³´ì™„",
                                        "ë²•ë ¹ ìœ„ë°˜", "ìƒìœ„ë²•ë ¹", "ìœ„ë²•", "ë¶ˆì¼ì¹˜", "ëª¨ìˆœ", "ìš°ë ¤"
                                    ]
                                    
                                    has_problems = any(keyword in first_analysis for keyword in problem_keywords)
                                    
                                    if has_problems:
                                        st.warning(f"âš ï¸ Geminiê°€ ì ì¬ì  ë¬¸ì œì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                                        
                                        # ğŸ†• 2ì°¨ ë¶„ì„: ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•œ ê´€ë ¨ ìœ„ë²• íŒë¡€ ê²€ìƒ‰
                                        st.info("ğŸ” 2-0ë‹¨ê³„: ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•œ ê´€ë ¨ ìœ„ë²• íŒë¡€ë¥¼ PKLì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                                        
                                        try:
                                            from comprehensive_violation_analysis import search_theoretical_background

                                            # ğŸ” Gemini ë¶„ì„ ê²°ê³¼ì—ì„œ êµ¬ì²´ì ì¸ ê·¼ê±° ì¶”ì¶œ
                                            extracted_context = extract_legal_reasoning_from_analysis(first_analysis)

                                            # ë¬¸ì œì ë³„ ì´ë¡  ê²€ìƒ‰ (ì¶”ì¶œëœ ë¬¸ë§¥ í™œìš©)
                                            detected_problems = [kw for kw in problem_keywords if kw in first_analysis]
                                            theoretical_results = search_theoretical_background(
                                                detected_problems,
                                                ['3. ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¬ì˜Â·ì œì†Œ ì¡°ë¡€ ëª¨ìŒì§‘(â…¨) (1)_new_vectorstore.pkl'],
                                                max_results=8,
                                                context_analysis=extracted_context  # ì¶”ì¶œëœ ë¬¸ë§¥ ì „ë‹¬
                                            )
                                            
                                            if theoretical_results:
                                                st.success(f"âœ… {len(theoretical_results)}ê°œì˜ ê´€ë ¨ ì´ë¡ /íŒë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                                                
                                                with st.expander("ğŸ“š ë¬¸ì œì  ê´€ë ¨ ìœ„ë²• íŒë¡€", expanded=False):
                                                    for i, theory in enumerate(theoretical_results[:5]):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                                                        context_rel = theory.get('context_relevance', 0)
                                                        matched_concepts = theory.get('matched_concepts', [])

                                                        st.markdown(f"**[{i+1}] {theory['topic']}**")
                                                        st.markdown(f"ğŸ“Š **ê´€ë ¨ë„**: {theory['relevance_score']:.3f} | **ë¬¸ë§¥ê´€ë ¨ì„±**: {context_rel}")

                                                        if matched_concepts:
                                                            st.markdown(f"ğŸ” **ë§¤ì¹­ëœ ê°œë…**: {', '.join(matched_concepts[:3])}")

                                                        content_preview = theory['content'][:300] + "..." if len(theory['content']) > 300 else theory['content']
                                                        st.markdown(f"ğŸ“„ **ë‚´ìš©**: {content_preview}")
                                                        st.markdown("---")
                                                
                                                # ìœ„ë²• íŒë¡€ë¥¼ í¬í•¨í•œ ì¬ë¶„ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
                                                st.session_state['theoretical_results'] = theoretical_results

                                                # ğŸ†• í†µí•© ë²•ë ¹ ë¬¸ì„œ(ì¬ì˜ì œì†Œ + ìì¹˜ë²•ê·œì…ì•ˆê°€ì´ë“œ)ì—ì„œ ì¶”ê°€ ê²€ìƒ‰
                                                st.info("ğŸ“– í†µí•© ë²•ë ¹ ë¬¸ì„œì—ì„œ ê´€ë ¨ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì¤‘...")
                                                try:
                                                    # ì¶”ì¶œëœ í‚¤ì›Œë“œë¡œ í†µí•© ë²•ë ¹ ë¬¸ì„œ ê²€ìƒ‰
                                                    search_query = ' '.join(detected_problems[:3])  # ìƒìœ„ 3ê°œ ë¬¸ì œì 
                                                    guideline_results, loaded_stores = search_multiple_vectorstores(
                                                        search_query,
                                                        gemini_api_key,
                                                        top_k_per_store=3
                                                    )

                                                    if guideline_results:
                                                        st.success(f"âœ… {len(guideline_results)}ê°œì˜ ê´€ë ¨ ê°€ì´ë“œë¼ì¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

                                                        with st.expander("ğŸ“‹ í†µí•© ë²•ë ¹ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼", expanded=False):
                                                            for i, result in enumerate(guideline_results):
                                                                st.markdown(f"**[{i+1}] {result.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')}**")
                                                                st.markdown(f"ğŸ“Š **ìœ ì‚¬ë„**: {result.get('similarity', 0):.3f}")

                                                                content_preview = result['text'][:400] + "..." if len(result['text']) > 400 else result['text']
                                                                st.markdown(f"ğŸ“„ **ë‚´ìš©**: {content_preview}")
                                                                st.markdown("---")

                                                        # ì„¸ì…˜ì— ì €ì¥
                                                        st.session_state['guideline_results'] = guideline_results
                                                    else:
                                                        st.info("í†µí•© ë²•ë ¹ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                                        st.session_state['guideline_results'] = []

                                                except Exception as e:
                                                    st.warning(f"í†µí•© ë²•ë ¹ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                                                    st.session_state['guideline_results'] = []

                                            else:
                                                st.warning("ê´€ë ¨ ìœ„ë²• íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                                st.session_state['guideline_results'] = []

                                        except Exception as e:
                                            st.error(f"ìœ„ë²• íŒë¡€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                                            st.session_state['theoretical_results'] = None

                                        # ğŸ†• PKL ë¶„ì„ ì™„ë£Œ í›„ ì¶”ì¶œëœ í‚¤ì›Œë“œë¡œ íŒë¡€ ê²€ìƒ‰
                                        st.info("âš–ï¸ 3ë‹¨ê³„: êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ ê´€ë ¨ íŒë¡€ ê²€ìƒ‰ ì¤‘...")

                                        if legality_keywords:
                                            st.info(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(legality_keywords)}")

                                            # ìœ„ë²•ì„± í‚¤ì›Œë“œë¡œ íŒë¡€ ê²€ìƒ‰
                                            search_query = ' '.join(legality_keywords[:3])  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
                                            precedents = search_precedents(search_query, max_results=5)

                                            if precedents:
                                                st.success(f"ğŸ“‹ {len(precedents)}ê°œ íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ")

                                                # íŒë¡€ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                                                progress_bar = st.progress(0)
                                                for i, precedent in enumerate(precedents[:3]):  # ìµœëŒ€ 3ê°œë§Œ ìƒì„¸ ì¡°íšŒ
                                                    detail_content = get_precedent_detail(precedent['id'])
                                                    if detail_content:
                                                        precedent['content'] = detail_content
                                                        precedents_content.append(precedent)
                                                    progress_bar.progress((i+1) / min(len(precedents), 3))

                                                # íŒë¡€ì—ì„œ ë²•ë¦¬ ì¶”ì¶œ
                                                if precedents_content:
                                                    contents_only = [p.get('content', '') for p in precedents_content]
                                                    legal_principles = extract_legal_principles_from_precedents(contents_only)

                                                    if legal_principles:
                                                        st.success(f"âš–ï¸ {len(legal_principles)}ê°œ ë²•ë¦¬ ì¶”ì¶œ ì™„ë£Œ")
                                                        with st.expander("ğŸ“– ì¶”ì¶œëœ ë²•ë¦¬ ë³´ê¸°"):
                                                            for i, principle in enumerate(legal_principles):
                                                                st.markdown(f"{i+1}. {principle}")
                                                    else:
                                                        st.info("ë²•ë¦¬ ì¶”ì¶œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                            else:
                                                st.info("ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                        else:
                                            st.warning("ìœ„ë²•ì„± í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                                            # í´ë°±: ì¡°ë¡€ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                                            fallback_keywords = []
                                            if pdf_text:
                                                title_match = re.search(r'[ê°€-í£\s]{5,30}(?:ì¡°ë¡€|ê·œì¹™)', pdf_text[:200])
                                                if title_match:
                                                    title = title_match.group()
                                                    keywords = re.findall(r'[ê°€-í£]{2,6}', title)
                                                    fallback_keywords = keywords[:3]

                                            if fallback_keywords:
                                                search_query = ' '.join(fallback_keywords)
                                                precedents = search_precedents(search_query, max_results=3)

                                                if precedents:
                                                    st.success(f"ğŸ“‹ {len(precedents)}ê°œ íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ (í´ë°±)")
                                                    # ê°„ë‹¨í•œ ìƒì„¸ ì¡°íšŒ
                                                    for precedent in precedents[:2]:
                                                        detail_content = get_precedent_detail(precedent['id'])
                                                        if detail_content:
                                                            precedent['content'] = detail_content
                                                            precedents_content.append(precedent)
                                                else:
                                                    st.info("í´ë°± í‚¤ì›Œë“œë¡œë„ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    else:
                                        st.success("âœ… Gemini 1ì°¨ ë¶„ì„ì—ì„œ íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                        
                                    analysis_results.append({
                                        'model': 'Gemini (1ì°¨ ë¶„ì„)',
                                        'content': first_analysis
                                    })
                                else:
                                    st.error("Gemini 1ì°¨ ë¶„ì„ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"Gemini 1ì°¨ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                analysis_results.append({
                                    'model': 'Gemini (1ì°¨ ë¶„ì„)',
                                    'error': str(e)
                                })
                        
                        # 4ë‹¨ê³„: ë¬¸ì œ ë°œê²¬ ì‹œ ë³µí•© PKL ì°¸ê³  ë¶„ì„ ìˆ˜í–‰
                        relevant_guidelines = None
                        loaded_stores = []
                        enhanced_analysis = None
                        
                        if has_problems and use_pkl_auto and first_analysis:
                            st.info("ğŸ” 4ë‹¨ê³„: ë¬¸ì œì ì´ ë°œê²¬ë˜ì–´ ë³µí•© PKL íŒŒì¼ì„ ì°¸ê³ í•œ ë³´ê°• ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                            
                            # ğŸ†• 4-1ë‹¨ê³„: ì¢…í•©ì  ì¡°ë¡€ ìœ„ë²•ì„± ë¶„ì„ (ìš°ì„  ìˆ˜í–‰)
                            comprehensive_analysis_results = None
                            if has_problems:  # ë¬¸ì œê°€ ë°œê²¬ëœ ê²½ìš° í•­ìƒ ìˆ˜í–‰
                                st.info("âš–ï¸ 4-1ë‹¨ê³„: ëª¨ë“  ìœ í˜•ì˜ ì¡°ë¡€ ìœ„ë²• íŒë¡€ë¥¼ ì¢…í•© ê²€ìƒ‰í•˜ì—¬ ì ìš©í•©ë‹ˆë‹¤...")
                                
                                try:
                                    from comprehensive_violation_analysis import search_comprehensive_violation_cases, apply_violation_cases_to_ordinance
                                    
                                    vectorstore_paths = [
                                        '3. ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¬ì˜Â·ì œì†Œ ì¡°ë¡€ ëª¨ìŒì§‘(â…¨) (1)_new_vectorstore.pkl'
                                    ]
                                    
                                    # ëª¨ë“  ìœ í˜•ì˜ ìœ„ë²• ì‚¬ë¡€ ì¢…í•© ê²€ìƒ‰
                                    # first_analysisì—ì„œ ì¡°ë¡€ ì •ë³´ ì¶”ì¶œ
                                    ordinance_articles = []
                                    if first_analysis and 'ordinance_data' in first_analysis:
                                        ordinance_articles = first_analysis['ordinance_data']
                                    
                                    violation_cases = search_comprehensive_violation_cases(ordinance_articles, vectorstore_paths, max_results=12)
                                    
                                    if violation_cases:
                                        # ìœ í˜•ë³„ í†µê³„
                                        type_counts = {}
                                        for case in violation_cases:
                                            v_type = case['violation_type']
                                            type_counts[v_type] = type_counts.get(v_type, 0) + 1
                                        
                                        st.success(f"âœ… {len(violation_cases)}ê°œì˜ ì¡°ë¡€ ìœ„ë²• íŒë¡€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:")
                                        
                                        # ìœ í˜•ë³„ ìš”ì•½
                                        type_summary = []
                                        for v_type, count in type_counts.items():
                                            type_summary.append(f"{v_type} ({count}ê°œ)")
                                        st.markdown("**ë°œê²¬ëœ ìœ„ë²• ìœ í˜•**: " + ", ".join(type_summary))
                                        
                                        # ë°œê²¬ëœ íŒë¡€ ë¯¸ë¦¬ë³´ê¸°
                                        with st.expander("ğŸ“š ë°œê²¬ëœ ì¡°ë¡€ ìœ„ë²• íŒë¡€", expanded=False):
                                            for i, case in enumerate(violation_cases):
                                                st.markdown(f"**[{i+1}] {case['violation_type']}** (ìœ ì‚¬ë„: {case['similarity']:.3f})")
                                                st.markdown(f"ì¶œì²˜: {case['source_store'].replace('.pkl', '').replace('_', ' ').title()}")
                                                if case['legal_principle'] != "í•´ë‹¹ì—†ìŒ":
                                                    st.markdown(f"ë²•ì  ì›ì¹™: {case['legal_principle']}")
                                                st.markdown(f"ìš”ì•½: {case['case_summary'][:150]}...")
                                                st.markdown("---")
                                        
                                        # íŒë¡€ë¥¼ í˜„ì¬ ì¡°ë¡€ì— ì ìš©í•˜ì—¬ ì¢…í•© ìœ„ë²•ì„± ë¶„ì„
                                        comprehensive_analysis_results = apply_violation_cases_to_ordinance(
                                            violation_cases, pdf_text, superior_laws_content
                                        )
                                        
                                        if comprehensive_analysis_results and isinstance(comprehensive_analysis_results, list):
                                            total_risks = sum(len(result['violation_risks']) for result in comprehensive_analysis_results)
                                            st.warning(f"âš ï¸ {len(comprehensive_analysis_results)}ê°œ ì¡°ë¬¸ì—ì„œ ì´ {total_risks}ê°œì˜ ìœ„ë²• ìœ„í—˜ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                            
                                            with st.expander("ğŸš¨ ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ê²°ê³¼", expanded=True):
                                                for result in comprehensive_analysis_results:
                                                    st.error(f"**{result['ordinance_article']}**")
                                                    st.markdown(f"ì¡°ë¬¸ ë‚´ìš©: {result['ordinance_content'][:100]}...")
                                                    
                                                    for i, risk in enumerate(result['violation_risks'][:3]):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                                                        st.markdown(f"**ìœ„í—˜ {i+1}: {risk['violation_type']}**")
                                                        st.markdown(f"- ìœ„í—˜ë„: {risk['risk_score']:.2f}/1.0")
                                                        if risk['legal_principle'] != "í•´ë‹¹ì—†ìŒ":
                                                            st.markdown(f"- ë²•ì  ì›ì¹™: {risk['legal_principle']}")
                                                        st.markdown(f"- ê´€ë ¨ ì‚¬ë¡€: {risk['case_summary'][:100]}...")
                                                        st.markdown(f"- ê°œì„  ê¶Œê³ : {risk['recommendation']}")
                                                        st.markdown("")
                                                    
                                                    if len(result['violation_risks']) > 3:
                                                        st.markdown(f"*...ì™¸ {len(result['violation_risks']) - 3}ê°œ ì¶”ê°€ ìœ„í—˜*")
                                                    st.markdown("---")
                                        else:
                                            st.success("âœ… PKL ê²€ìƒ‰ ê²°ê³¼ ì§ì ‘ì ì¸ ìœ„ë²• ìœ„í—˜ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                    else:
                                        st.warning("ê´€ë ¨ ìœ„ë²• íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                        
                                except ImportError:
                                    st.error("ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                except Exception as e:
                                    st.error(f"ì¢…í•© ìœ„ë²•ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                            
                            # 4-2ë‹¨ê³„: ê¸°ì¡´ì˜ ì¼ë°˜ì ì¸ PKL ê²€ìƒ‰
                            # ë°œê²¬ëœ ë¬¸ì œì ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                            search_terms = []
                            
                            # ì‚¬ë¬´ ê´€ë ¨ ë¬¸ì œ
                            if any(word in first_analysis for word in ["ì†Œê´€ì‚¬ë¬´", "ì‚¬ë¬´êµ¬ë¶„", "ìœ„ì„ì‚¬ë¬´", "ìì¹˜ì‚¬ë¬´"]):
                                search_terms.extend(["ê¸°ê´€ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ì œì • ë¶ˆê°€", "ìœ„ì„ì‚¬ë¬´ ì¡°ë¡€ ì œì • í•œê³„"])
                            
                            # ë²•ë ¹ ìœ„ë°˜ ê´€ë ¨ ë¬¸ì œ  
                            if any(word in first_analysis for word in ["ë²•ë ¹ ìœ„ë°˜", "ìƒìœ„ë²•ë ¹", "ë²•ë ¹ìš°ìœ„", "ìœ„ë°˜"]):
                                search_terms.extend(["ë²•ë ¹ ìœ„ë°˜ ì¡°ë¡€ ì‚¬ë¡€", "ìƒìœ„ë²•ë ¹ ì¶©ëŒ ì¡°ë¡€"])
                            
                            # ì¡°ë¡€ ì œì • í•œê³„ ê´€ë ¨
                            if any(word in first_analysis for word in ["ì œì • í•œê³„", "ì…ë²•í•œê³„", "ë¶ˆê°€", "ìœ„ë²•"]):
                                search_terms.extend(["ì¡°ë¡€ ì œì • í•œê³„ íŒë¡€", "ìœ„ë²• ì¡°ë¡€ ì œì • ì‚¬ë¡€"])
                            
                            # ê¸°ë³¸ ê²€ìƒ‰ì–´ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ ì‚¬ìš©
                            if not search_terms:
                                search_terms = ["ë²•ë ¹ ìœ„ë°˜ ì¡°ë¡€ íŒë¡€", "ì¡°ë¡€ ì œì • í•œê³„ ì‚¬ë¡€"]
                            
                            # ì—¬ëŸ¬ ê²€ìƒ‰ì–´ ì¤‘ í•˜ë‚˜ ì„ íƒ (ê°€ì¥ êµ¬ì²´ì ì¸ ê²ƒ)
                            search_query_pkl = search_terms[0] if search_terms else "ìœ„ë²• ì¡°ë¡€ íŒë¡€"
                            
                            # í–¥ìƒëœ ë³µí•© ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ìˆ˜í–‰
                            try:
                                from enhanced_search import enhanced_legal_search
                                vectorstore_paths = [
                                    '3. ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¬ì˜Â·ì œì†Œ ì¡°ë¡€ ëª¨ìŒì§‘(â…¨) (1)_new_vectorstore.pkl'
                                ]
                                enhanced_results = enhanced_legal_search(search_query_pkl, vectorstore_paths, max_results=6)
                                
                                # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                relevant_guidelines = []
                                loaded_stores = set()
                                
                                for result in enhanced_results:
                                    relevant_guidelines.append({
                                        'text': result['text'],
                                        'similarity': result['similarity'],
                                        'distance': 1 - result['similarity'],
                                        'metadata': result['metadata'],
                                        'source_store': result['source_store'].replace('.pkl', '').replace('_', ' ').title(),
                                        'source_file': result['source_store']
                                    })
                                    loaded_stores.add(result['source_store'].replace('.pkl', '').replace('_', ' ').title())
                                
                                loaded_stores = list(loaded_stores)
                                
                            except ImportError:
                                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                                relevant_guidelines, loaded_stores = search_multiple_vectorstores(
                                    search_query_pkl, 
                                    api_key=gemini_api_key, 
                                    top_k_per_store=2
                                )
                            
                            if relevant_guidelines and loaded_stores:
                                st.success(f"âœ… {len(loaded_stores)}ê°œ ìë£Œì—ì„œ {len(relevant_guidelines)}ê°œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤:")
                                for store in loaded_stores:
                                    st.markdown(f"   â€¢ {store}")
                                
                                # ê°€ì´ë“œë¼ì¸ ë¯¸ë¦¬ë³´ê¸° (ì„ íƒì‚¬í•­)
                                with st.expander("ğŸ“– ê²€ìƒ‰ëœ ë¬¸ì œ ê´€ë ¨ ìë£Œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                                    source_groups = {}
                                    for guideline in relevant_guidelines:
                                        source_store = guideline.get('source_store', 'ì•Œ ìˆ˜ ì—†ëŠ” ìë£Œ')
                                        if source_store not in source_groups:
                                            source_groups[source_store] = []
                                        source_groups[source_store].append(guideline)
                                    
                                    for source_store, guidelines in source_groups.items():
                                        st.markdown(f"**ğŸ“š {source_store}**")
                                        for i, guideline in enumerate(guidelines):
                                            similarity_score = guideline.get('similarity', 1-guideline.get('distance', 0))
                                            st.markdown(f"   [{i+1}] (ìœ ì‚¬ë„: {similarity_score:.3f})")
                                            st.markdown(guideline['text'][:200] + "..." if len(guideline['text']) > 200 else guideline['text'])
                                            st.markdown("---")
                                
                                # 2ì°¨ ë³´ê°• ë¶„ì„ ìˆ˜í–‰ (ì¡°ìš©íˆ)
                                if gemini_api_key:
                                    try:
                                        # ë³´ê°• ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸
                                        enhanced_prompt = create_analysis_prompt(
                                            pdf_text,
                                            search_results_for_analysis,
                                            superior_laws_content,
                                            relevant_guidelines,
                                            is_first_ordinance,
                                            comprehensive_analysis_results,
                                            theoretical_results
                                        )
                                        
                                        enhanced_response = model.generate_content(enhanced_prompt)
                                        if enhanced_response and hasattr(enhanced_response, 'text') and enhanced_response.text:
                                            enhanced_analysis = enhanced_response.text
                                            analysis_results.append({
                                                'model': f'Gemini (ë³µí•©PKL ë³´ê°•ë¶„ì„ - {len(loaded_stores)}ê°œ ìë£Œ)',
                                                'content': enhanced_analysis
                                            })
                                    except Exception as e:
                                        st.error(f"ë³µí•© PKL ë³´ê°• ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                            else:
                                st.info("ë¬¸ì œì ê³¼ ê´€ë ¨ëœ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        elif not has_problems:
                            st.info("âœ… ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•Šì•„ PKL ì°¸ê³ ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        elif not use_pkl_auto:
                            st.info("ğŸ”„ PKL ìë™ ì°¸ê³  ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        
                        # 5ë‹¨ê³„: OpenAI ì¶”ê°€ ë¶„ì„ (ì„ íƒì‚¬í•­)
                        if openai_api_key:
                            st.info("ğŸ”„ 5ë‹¨ê³„: OpenAI ì¶”ê°€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                            try:
                                openai.api_key = openai_api_key
                                # ê°€ì¥ ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ë¡œ OpenAI ë¶„ì„
                                openai_prompt = create_analysis_prompt(pdf_text, search_results_for_analysis, superior_laws_content, relevant_guidelines, is_first_ordinance, comprehensive_analysis_results, theoretical_results)
                                
                                response = openai.ChatCompletion.create(
                                    model="gpt-4",
                                    messages=[
                                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¡°ë¡€ ë¶„ì„ê³¼ ê²€í† ë¥¼ ë„ì™€ì£¼ì„¸ìš”."},
                                        {"role": "user", "content": openai_prompt}
                                    ],
                                    temperature=0.7,
                                    max_tokens=4000
                                )
                                
                                if response.choices[0].message.content:
                                    analysis_results.append({
                                        'model': 'OpenAI (ì¶”ê°€ ë¶„ì„)',
                                        'content': response.choices[0].message.content
                                    })
                            except Exception as e:
                                st.error(f"OpenAI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                analysis_results.append({
                                    'model': 'OpenAI (ì¶”ê°€ ë¶„ì„)',
                                    'error': str(e)
                                })
                        
                        if analysis_results:
                            # ğŸ†• ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                            st.session_state.analysis_results = analysis_results
                            # í†µí•© ë²•ë ¹ ë¬¸ì„œ ê²°ê³¼ë„ metadataì— í¬í•¨
                            final_guideline_results = st.session_state.get('guideline_results', relevant_guidelines)
                            st.session_state.analysis_metadata = {
                                'has_problems': has_problems,
                                'relevant_guidelines': final_guideline_results,
                                'loaded_stores': loaded_stores,
                                'is_first_ordinance': is_first_ordinance,
                                'superior_laws_content': superior_laws_content,
                                'search_results_for_analysis': search_results_for_analysis,
                                'pdf_text': pdf_text,
                                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            }

                            # ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€
                            st.markdown("---")
                            if has_problems and relevant_guidelines and loaded_stores:
                                st.success(f"ğŸ¯ **ë³µí•© ìë£Œ ë³´ê°• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ {len(loaded_stores)}ê°œ ìë£Œ ì°¸ê³  â†’ ë³´ê°• ë¶„ì„")
                            elif has_problems and relevant_guidelines:
                                st.success("ğŸ¯ **ì§€ëŠ¥í˜• ë¶„ì„ ì™„ë£Œ**: ë¬¸ì œì  íƒì§€ â†’ PKL ì°¸ê³  â†’ ë³´ê°• ë¶„ì„")
                            elif has_problems:
                                st.info("âš ï¸ **ë¬¸ì œì  íƒì§€ ë¶„ì„ ì™„ë£Œ**: PKL ì°¸ê³  ì—†ì´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                            else:
                                st.success("âœ… **ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ**: íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
                            
                            # ë¶„ì„ ê²°ê³¼ ìš”ì•½
                            analysis_count = len([r for r in analysis_results if 'error' not in r])
                            error_count = len([r for r in analysis_results if 'error' in r])
                            
                            if analysis_count > 0:
                                # ğŸ†• ì„ íƒëœ ì¡°ë¡€ ìˆ˜ ì •í™•íˆ ë°˜ì˜
                                if is_first_ordinance:
                                    analysis_type_text = "ìµœì´ˆ ì œì • ì¡°ë¡€"
                                else:
                                    selected_count = len(search_results_for_analysis)
                                    analysis_type_text = f"ì„ íƒëœ {selected_count}ê°œ íƒ€ ì‹œë„ ì¡°ë¡€ ë¹„êµ"
                                st.markdown(f"**ğŸ“‹ ë¶„ì„ ìœ í˜•**: {analysis_type_text}")
                                st.markdown(f"**ğŸ¤– ìˆ˜í–‰ëœ ë¶„ì„**: {analysis_count}ê°œ")
                                if relevant_guidelines:
                                    st.markdown(f"**ğŸ“š ì°¸ê³ ëœ ê°€ì´ë“œë¼ì¸**: {len(relevant_guidelines)}ê°œ")
                            
                            # ìµœì¢… ë³´ê³ ì„œë§Œ í‘œì‹œ (PKL ë³´ê°• ë¶„ì„ ë˜ëŠ” OpenAI ë¶„ì„)
                            final_report = None
                            
                            # ìš°ì„ ìˆœìœ„: ë³µí•©PKL ë³´ê°•ë¶„ì„ > PKL ë³´ê°•ë¶„ì„ > OpenAI ì¶”ê°€ ë¶„ì„ > 1ì°¨ ë¶„ì„
                            for result in reversed(analysis_results):  # ì—­ìˆœìœ¼ë¡œ ìµœì‹  ê²°ê³¼ ìš°ì„ 
                                if 'error' not in result:
                                    if "ë³µí•©PKL ë³´ê°•ë¶„ì„" in result['model']:
                                        final_report = result
                                        break
                                    elif "PKL ë³´ê°•" in result['model'] or "OpenAI" in result['model']:
                                        final_report = result
                                        break
                            
                            # PKL ë³´ê°•ì´ë‚˜ OpenAIê°€ ì—†ìœ¼ë©´ 1ì°¨ ë¶„ì„ ì‚¬ìš©
                            if not final_report:
                                for result in analysis_results:
                                    if 'error' not in result and "1ì°¨ ë¶„ì„" in result['model']:
                                        final_report = result
                                        break
                            
                            # ìµœì¢… ë³´ê³ ì„œ í‘œì‹œ
                            if final_report:
                                st.markdown("### ğŸ“‹ ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ")
                                
                                # ë³´ê³ ì„œ íƒ€ì… í‘œì‹œ
                                if "ë³µí•©PKL ë³´ê°•ë¶„ì„" in final_report['model']:
                                    st.success("ğŸ¯ **ë³µí•© ìë£Œ ì°¸ê³  ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                                    st.caption(f"ğŸ“š **í™œìš© ëª¨ë¸**: {final_report['model']}")
                                elif "PKL ë³´ê°•" in final_report['model']:
                                    st.success("ğŸ¯ **PKL ê°€ì´ë“œë¼ì¸ ì°¸ê³  ë³´ê°• ë¶„ì„ ê²°ê³¼**")
                                elif "OpenAI" in final_report['model']:
                                    st.info("ğŸ“Š **OpenAI ì¶”ê°€ ë¶„ì„ ê²°ê³¼**")
                                else:
                                    st.info("ğŸ¤– **Gemini ê¸°ë³¸ ë¶„ì„ ê²°ê³¼**")
                                
                                # ë³´ê³ ì„œ ë‚´ìš©
                                st.markdown(final_report['content'])
                                
                            # ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ ë³„ë„ í‘œì‹œ
                            for result in analysis_results:
                                if 'error' in result:
                                    st.error(f"âŒ {result['model']} ì˜¤ë¥˜: {result['error']}")
                            
                            # Word ë¬¸ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
                            with st.spinner("ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ìƒì„± ì¤‘..."):
                                doc = create_comparison_document(pdf_text, search_results_for_analysis, analysis_results, superior_laws_content, relevant_guidelines)
                                
                                doc_io = io.BytesIO()
                                doc.save(doc_io)
                                doc_bytes = doc_io.getvalue()
                                
                                # íŒŒì¼ëª…ì— ë¶„ì„ ë°©ì‹ í‘œì‹œ
                                if has_problems and relevant_guidelines and loaded_stores:
                                    stores_count = len(loaded_stores)
                                    filename_prefix = f"ë³µí•©ìë£Œë³´ê°•ë¶„ì„({stores_count}ê°œìë£Œ)" if is_first_ordinance else f"ì¡°ë¡€ë¹„êµ_ë³µí•©ìë£Œë¶„ì„({stores_count}ê°œìë£Œ)"
                                elif has_problems and relevant_guidelines:
                                    filename_prefix = "ì§€ëŠ¥í˜•PKLë³´ê°•ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_PKLë³´ê°•ë¶„ì„"
                                elif has_problems:
                                    filename_prefix = "ë¬¸ì œì íƒì§€ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€ë¹„êµ_ë¬¸ì œì ë¶„ì„"
                                else:
                                    filename_prefix = "ìµœì´ˆì¡°ë¡€_ê¸°ë³¸ë¶„ì„" if is_first_ordinance else "ì¡°ë¡€_ê¸°ë³¸ë¹„êµë¶„ì„"
                                
                                st.download_button(
                                    label="ğŸ“„ ë¶„ì„ ê²°ê³¼ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                                    data=doc_bytes,
                                    file_name=f"{filename_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                                )
                        else:
                            st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()