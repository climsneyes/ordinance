#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë²¡í„°ìŠ¤í† ì–´ ë‚´ìš© ë·°ì–´
PKL íŒŒì¼ì˜ ë‚´ìš©ì„ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë„êµ¬
"""

import pickle
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime
import re

def load_vectorstore_data(pkl_path):
    """ë²¡í„°ìŠ¤í† ì–´ ë°ì´í„° ë¡œë“œ"""
    try:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
        return data
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def clean_text_for_display(text):
    """í‘œì‹œìš© í…ìŠ¤íŠ¸ ì •ë¦¬"""
    # ë„ˆë¬´ ê¸´ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def main():
    st.set_page_config(
        page_title="ë²¡í„°ìŠ¤í† ì–´ ë·°ì–´",
        page_icon="ğŸ“š",
        layout="wide"
    )

    st.title("ğŸ“š ë²¡í„°ìŠ¤í† ì–´ ë‚´ìš© ë·°ì–´")
    st.markdown("---")

    # íŒŒì¼ ê²½ë¡œ
    pkl_path = r"c:\jo(9.11.)\enhanced_vectorstore_20250914_101739.pkl"

    # ë°ì´í„° ë¡œë“œ
    with st.spinner("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘..."):
        data = load_vectorstore_data(pkl_path)

    if data is None:
        st.stop()

    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    st.header("ğŸ“Š ê¸°ë³¸ ì •ë³´")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ì´ ë¬¸ì„œ ìˆ˜", len(data.get('documents', [])))

    with col2:
        st.metric("ì„ë² ë”© ì°¨ì›", data.get('embedding_dimension', 'N/A'))

    with col3:
        st.metric("ëª¨ë¸", data.get('model_name', 'N/A'))

    with col4:
        created_at = data.get('created_at', 'N/A')
        if created_at != 'N/A':
            try:
                dt = datetime.fromisoformat(created_at)
                created_at = dt.strftime('%Y-%m-%d %H:%M')
            except:
                pass
        st.metric("ìƒì„±ì¼", created_at)

    st.markdown("---")

    # íƒ­ìœ¼ë¡œ ê¸°ëŠ¥ ë¶„ë¦¬
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ ë¬¸ì„œ ëª©ë¡", "ğŸ” ê²€ìƒ‰", "ğŸ“Š í†µê³„", "ğŸ”§ ë””ë²„ê·¸"])

    with tab1:
        st.header("ğŸ“„ ë¬¸ì„œ ëª©ë¡")

        documents = data.get('documents', [])
        metadatas = data.get('metadatas', [])
        chunks = data.get('chunks', [])

        if not documents:
            st.warning("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # í˜ì´ì§€ë„¤ì´ì…˜
        items_per_page = st.selectbox("í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜", [10, 25, 50, 100], index=1)
        total_pages = (len(documents) + items_per_page - 1) // items_per_page

        if total_pages > 1:
            page = st.number_input("í˜ì´ì§€", min_value=1, max_value=total_pages, value=1) - 1
        else:
            page = 0

        start_idx = page * items_per_page
        end_idx = min(start_idx + items_per_page, len(documents))

        st.info(f"ì „ì²´ {len(documents)}ê°œ ì¤‘ {start_idx + 1}-{end_idx}ë²ˆì§¸ í‘œì‹œ")

        # ë¬¸ì„œ ëª©ë¡ í‘œì‹œ
        for i in range(start_idx, end_idx):
            with st.expander(f"ğŸ“„ ë¬¸ì„œ {i + 1}", expanded=False):
                doc_text = clean_text_for_display(documents[i])

                # ë©”íƒ€ë°ì´í„° ì •ë³´
                if i < len(metadatas) and metadatas[i]:
                    metadata = metadatas[i]
                    col1, col2 = st.columns([1, 3])

                    with col1:
                        st.markdown("**ğŸ“ ë©”íƒ€ë°ì´í„°:**")
                        for key, value in metadata.items():
                            st.markdown(f"â€¢ {key}: {value}")

                    with col2:
                        st.markdown("**ğŸ“„ ë‚´ìš©:**")
                        st.text_area("", doc_text, height=200, key=f"doc_{i}", disabled=True)
                else:
                    st.markdown("**ğŸ“„ ë‚´ìš©:**")
                    st.text_area("", doc_text, height=200, key=f"doc_{i}", disabled=True)

    with tab2:
        st.header("ğŸ” ë‚´ìš© ê²€ìƒ‰")

        search_query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥")
        search_type = st.selectbox("ê²€ìƒ‰ ë°©ì‹", ["ë‹¨ì–´ í¬í•¨", "ì •ê·œì‹"])

        if search_query:
            documents = data.get('documents', [])
            results = []

            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                for i, doc in enumerate(documents):
                    try:
                        if search_type == "ë‹¨ì–´ í¬í•¨":
                            if search_query.lower() in doc.lower():
                                # ê²€ìƒ‰ì–´ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                idx = doc.lower().find(search_query.lower())
                                start = max(0, idx - 100)
                                end = min(len(doc), idx + len(search_query) + 100)
                                context = doc[start:end]
                                results.append((i, context, doc))

                        elif search_type == "ì •ê·œì‹":
                            if re.search(search_query, doc, re.IGNORECASE):
                                match = re.search(search_query, doc, re.IGNORECASE)
                                start = max(0, match.start() - 100)
                                end = min(len(doc), match.end() + 100)
                                context = doc[start:end]
                                results.append((i, context, doc))

                    except Exception as e:
                        continue

            if results:
                st.success(f"ğŸ¯ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")

                for i, (doc_idx, context, full_doc) in enumerate(results):
                    with st.expander(f"ê²€ìƒ‰ ê²°ê³¼ {i + 1} (ë¬¸ì„œ {doc_idx + 1})", expanded=False):
                        # ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŠ¸
                        if search_type == "ë‹¨ì–´ í¬í•¨":
                            highlighted = context.replace(
                                search_query,
                                f"**ğŸ”{search_query}ğŸ”**"
                            )
                        else:
                            highlighted = context

                        st.markdown("**ğŸ” ê²€ìƒ‰ ê²°ê³¼ (ì£¼ë³€ í…ìŠ¤íŠ¸):**")
                        st.markdown(highlighted)

                        if st.button(f"ì „ì²´ ë‚´ìš© ë³´ê¸°", key=f"full_{i}"):
                            st.markdown("**ğŸ“„ ì „ì²´ ë¬¸ì„œ:**")
                            st.text_area("", full_doc, height=400, key=f"full_doc_{i}", disabled=True)
            else:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab3:
        st.header("ğŸ“Š í†µê³„ ì •ë³´")

        documents = data.get('documents', [])
        embeddings = data.get('embeddings', np.array([]))

        if documents:
            # ë¬¸ì„œ ê¸¸ì´ í†µê³„
            doc_lengths = [len(doc) for doc in documents]

            col1, col2 = st.columns(2)

            with col1:
                st.subheader("ğŸ“ ë¬¸ì„œ ê¸¸ì´ í†µê³„")
                df_lengths = pd.DataFrame({
                    'í†µê³„': ['ìµœì†Œ ê¸¸ì´', 'ìµœëŒ€ ê¸¸ì´', 'í‰ê·  ê¸¸ì´', 'ì¤‘ê°„ê°’'],
                    'ë¬¸ì ìˆ˜': [
                        min(doc_lengths),
                        max(doc_lengths),
                        int(np.mean(doc_lengths)),
                        int(np.median(doc_lengths))
                    ]
                })
                st.dataframe(df_lengths, use_container_width=True)

            with col2:
                st.subheader("ğŸ“Š ê¸¸ì´ ë¶„í¬")
                # íˆìŠ¤í† ê·¸ë¨ ë°ì´í„°
                bins = [0, 100, 300, 500, 1000, max(doc_lengths)]
                bin_labels = ['0-100', '100-300', '300-500', '500-1000', '1000+']
                counts = []

                for i in range(len(bins) - 1):
                    count = sum(1 for length in doc_lengths if bins[i] <= length < bins[i+1])
                    counts.append(count)

                df_dist = pd.DataFrame({
                    'ê¸¸ì´ ë²”ìœ„': bin_labels,
                    'ë¬¸ì„œ ìˆ˜': counts
                })
                st.dataframe(df_dist, use_container_width=True)

        # í‚¤ì›Œë“œ ë¶„ì„
        if documents:
            st.subheader("ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")

            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            all_text = ' '.join(documents[:100])  # ì²˜ìŒ 100ê°œë§Œ ë¶„ì„

            keywords = ['ì¡°ë¡€', 'ë²•ë¥ ', 'ê·œì •', 'ìœ„ë°˜', 'ìœ„ë²•', 'í—ˆê°€', 'ìŠ¹ì¸', 'ì‚¬ë¬´', 'ê¶Œí•œ',
                       'ê¸°ê´€ìœ„ì„', 'ì¬ì˜', 'ì œì†Œ', 'ì˜ê²°', 'ëŒ€ë²•ì›', 'íŒë¡€', 'í—Œë²•', 'ì‹œì¥', 'êµ°ìˆ˜']

            keyword_counts = {}
            for keyword in keywords:
                count = all_text.lower().count(keyword.lower())
                if count > 0:
                    keyword_counts[keyword] = count

            if keyword_counts:
                df_keywords = pd.DataFrame(list(keyword_counts.items()),
                                         columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                df_keywords = df_keywords.sort_values('ë¹ˆë„', ascending=False)
                st.dataframe(df_keywords, use_container_width=True)

    with tab4:
        st.header("ğŸ”§ ë””ë²„ê·¸ ì •ë³´")

        st.subheader("ğŸ“‹ ë°ì´í„° êµ¬ì¡°")
        st.code(f"í‚¤ ëª©ë¡: {list(data.keys())}")

        for key, value in data.items():
            if key in ['documents', 'embeddings', 'chunks']:
                st.code(f"{key}: {type(value)} (ê¸¸ì´: {len(value)})")
            else:
                st.code(f"{key}: {type(value)} = {value}")

        # ì„ë² ë”© ì •ë³´
        embeddings = data.get('embeddings', np.array([]))
        if len(embeddings) > 0:
            st.subheader("ğŸ§® ì„ë² ë”© ì •ë³´")
            st.code(f"Shape: {embeddings.shape}")
            st.code(f"ë°ì´í„° íƒ€ì…: {embeddings.dtype}")
            st.code(f"ì²« ë²ˆì§¸ ì„ë² ë”© ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ ê°’): {embeddings[0][:10]}")

        # ì›ì‹œ ë°ì´í„° ìƒ˜í”Œ
        st.subheader("ğŸ” ì›ì‹œ ë°ì´í„° ìƒ˜í”Œ")

        if st.button("ì²« ë²ˆì§¸ ë¬¸ì„œ ì›ì‹œ ë°ì´í„° ë³´ê¸°"):
            if data.get('documents'):
                st.code(repr(data['documents'][0]))

        if st.button("ì „ì²´ ë°ì´í„° êµ¬ì¡° ë³´ê¸°"):
            st.json({k: str(type(v)) for k, v in data.items()})

if __name__ == "__main__":
    main()